{
    "docs": [
        {
            "location": "/", 
            "text": "Tungsten Fabric for Windows documentation", 
            "title": "Home"
        }, 
        {
            "location": "/ci_admin_guide/Dependencies/", 
            "text": "How to check what is installed on builders\n\n\nPrerequisites:\n\n\n\n\nCheckout \ngithub.com/Juniper/contrail-windows-ci\n repository.\n\n\nEnter \nansible/\n directory.\n\n\nPrepare your ansible runner environment by going through steps described in \nREADME.md\n. \n\n\nLog into Jenkins\n\n\nInspect every builder node and note its IP address\n\n\nCreate inventory file for [builder] group, that contains their IPs:\n\n\n\n\n[windows:children]\nbuilder\n\n[builder]\n10.84.12.A\n10.84.12.B\n10.84.12.C\n...\n\n\n\n\nCheck version of psget package:\n\n\nRun the following command:\n\n\nansible builder -i inventory --vault-password-file ~/ansible-vault-key -m win_shell -a \nGet-Package -Name PSScriptAnalyzer | Select -Expand Version\n\n\n\n\n\nCheck versions of chocolatey packages:\n\n\nRun the following command:\n\n\nansible builder -i inventory --vault-password-file ~/ansible-vault-key -m win_shell -a \nchoco list --local-only", 
            "title": "Dependencies"
        }, 
        {
            "location": "/ci_admin_guide/Dependencies/#how-to-check-what-is-installed-on-builders", 
            "text": "", 
            "title": "How to check what is installed on builders"
        }, 
        {
            "location": "/ci_admin_guide/Dependencies/#prerequisites", 
            "text": "Checkout  github.com/Juniper/contrail-windows-ci  repository.  Enter  ansible/  directory.  Prepare your ansible runner environment by going through steps described in  README.md .   Log into Jenkins  Inspect every builder node and note its IP address  Create inventory file for [builder] group, that contains their IPs:   [windows:children]\nbuilder\n\n[builder]\n10.84.12.A\n10.84.12.B\n10.84.12.C\n...", 
            "title": "Prerequisites:"
        }, 
        {
            "location": "/ci_admin_guide/Dependencies/#check-version-of-psget-package", 
            "text": "Run the following command:  ansible builder -i inventory --vault-password-file ~/ansible-vault-key -m win_shell -a  Get-Package -Name PSScriptAnalyzer | Select -Expand Version", 
            "title": "Check version of psget package:"
        }, 
        {
            "location": "/ci_admin_guide/Dependencies/#check-versions-of-chocolatey-packages", 
            "text": "Run the following command:  ansible builder -i inventory --vault-password-file ~/ansible-vault-key -m win_shell -a  choco list --local-only", 
            "title": "Check versions of chocolatey packages:"
        }, 
        {
            "location": "/ci_admin_guide/Infrastructure_backups/", 
            "text": "Infrastructure backups\n\n\nThis document describes current backup system configuration for Contrail Windows CI.\n\n\nTable of contents\n\n\n\n\nCurrent backup setup\n\n\nVM recovery guide\n\n\nBackup scripts update guide\n\n\n\n\nCurrent backup setup\n\n\nDiagram below presents an overview of current backup setup for Contrail Windows CI.\n\n\n\n\n\n\nContrail Windows CI underlying infrastructure is based VMware vSphere\n\n\nServices support Contrail Windows CI are running on a set of \nInfrastructure VMs\n\n\nInfrastructure VMs reside on \nWinCI-Datastores-Infra\n datastore cluster\n\n\nWinCI-Datastores-Infra datastore cluster consists of an independent set of SSD-based datastores\n\n\nindependent, meaning these datastores are used solely for Infrastructure VMs\n\n\n\n\n\n\nBackups are performed using Veeam Backup \n Replication software\n\n\nVeeam Backup Free Edition is used\n\n\n\n\n\n\nVeeam is installed on \nwinci-veeam\n, Windows Server 2016 virtual machine\n\n\nwinci-veeam\n has a mounted NFS share \nwinci_nfsbackup\n which is used as a backups repository\n\n\nVeeam connects to vSphere cluster using vSphere API\n\n\nVeeam performs full VM backups (using VeeamZIP feature in the Free edition)\n\n\n\n\nBackups are performed regulary using \nWindows Scheduled Tasks\n.\nCurrently scheduled task runs every day, at 10 PM PST.\nScheduled task executes a PowerShell Script \nBackup.ps1\n located in \nC:\\BackupScripts\\\n directory.\n\n\nBackup procedure looks as follows:\n\n\n\n\nBackups are performed by a set of PowerShell scripts\n\n\nScripts are located in \nbackups/\n directory in \ncontrail-windows-ci\n repository\n\n\n\n\n\n\nBackup.ps1\n script performs the following steps:\n\n\nPrunes backups by removing all but last \nN\n backups\n\n\nN\n is configurable in the script\n\n\n\n\n\n\nUses VeeamZIP PowerShell cmdlets to perform a full backups of VMs listed in \nBackup.ps1\n file\n\n\n\n\n\n\nIf an error occurs for any of the VMs listed, script continues its job. After performing backups for other VMs it terminates with an exception\n\n\nEach full backup is stored in a separate directory, located in \n\\\\winci_nfsbackups\\Backups\\\n directory on backup NFS share\n\n\n\n\nVM recovery guide\n\n\nCurrent backup setup supports only full VM recovery. It needs to be performed, e.g. when:\n\n\n\n\nin case of underlying infrastructure hardware failure (e.g. corrupted disk, ESXi host failure)\n\n\nin case of bricking an operating system/virtual machine\n\n\nrecovery of VM state from a certain point in time is desirable\n\n\n\n\nPrerequisites to backup scripts updating:\n\n\n\n\nWindows 10/Windows Server 2016 machine to perform the steps\n\n\nCredentials to \nwinci-veeam\n server\n\n\nPlease contact Windows CI team\n\n\n\n\n\n\n\n\nTo recover a VM from backup perform the following steps:\n\n\n\n\nEstablish a RDP connection \nwinci-veeam\n server\n\n\nOpen \nVeeam Backup \n Replication Console\n\n\nprogram is available through Start menu\n\n\n\n\n\n\nVeeam login window should open up\n\n\nensure that \nlocalhost\n and port \n9392\n are selected and \nUse Windows session authentication\n option is selected\n\n\nclick \nConnect\n\n\n\n\n\n\nIn the upper left corner click an option \nRestore\n\n\nSelect backup file to restore\n windows should open up\n\n\nClick \nbrowse\n\n\nIn the address bar type in \nX:\\Backups\\\n\n\nA list of directories named with date timestamps should show up\n\n\nSelect a directory with a date, matching your desired restore point\n\n\nEnter this directory\n\n\nSelect a backup file (file with \n.vbk\n extension) which has prefix matching VM to be restored\n\n\nClick \nOpen\n\n\n\n\n\n\nProgress bar named \nReading the backup file, please wait...\n should show up\n\n\nWhen a table named \nContained VMs\n appears, select a VM to be restored, click \nRestore\n; clicking \nRestore\n opens up contextual menu; click option \nEntire VM (including registration)\n\n\nAfter a few moments a restore wizard should show up\n\n\nIn \nVirtual machine\n section\n\n\nFrom a table \nVirtual machines to restore\n select a VM to be restored\n\n\nClick \nNext\n\n\n\n\n\n\nIn \nRestore Mode\n section\n\n\nSelect \nRestore to the original location\n option\n\n\nMake sure \nRestore VM tags\n option is checked\n\n\nMake sure \nQuick rollback\n option is unchecked\n\n\nClick \nNext\n\n\n\n\n\n\nIn \nReason\n section\n\n\nEnter a reason for VM restoring into \nRestore reasons\n textbox\n\n\nClick \nNext\n\n\n\n\n\n\nIn \nSummary\n section\n\n\nVerify VM details presented in \nSummary\n textbox\n\n\nCheck \nPower on target virtual machine\n option if you want the restored VM to power on after restoration is completed\n\n\nClick \nFinish\n\n\n\n\n\n\nVM Restore\n window with progress report should appear\n\n\nWindow can be closed by clicking \nClose\n button, this will not stop the restore procedure\n\n\n\n\n\n\nTo open up \nVM Restore\n window again\n\n\nClick \nHistory\n button in the lower left corner of Veeam Console\n\n\nIn the tree (in the left middle part of the Veeam Console) select \nRestore \n Full VM Restore\n branch\n\n\nA list of restore jobs should appear in the middle\n\n\nDouble-click a job corresponding to a VM you are currently restoring\n\n\nVM Restore\n window should open up again\n\n\n\n\n\n\nWhen a restore job finishes, verify if all services provided by the machine are up and working\n\n\n\n\nBackup scripts update guide\n\n\nBackup scripts need to be updated, e.g. when:\n\n\n\n\nA list of VMs requiring backup changes\n\n\nA bug was fixed in the backup scripts\n\n\n\n\nPrerequisites to backup scripts updating:\n\n\n\n\nWindows 10/Windows Server 2016 machine to perform the steps\n\n\nCredentials to \nwinci-veeam\n server\n\n\nPlease contact Windows CI team\n\n\n\n\n\n\n\n\nTo update backup scripts perform the following steps:\n\n\n\n\n\n\nClone \ncontrail-windows-ci\n repository and enter \nbackups/\n directory\n\n\npowershell\nPS C:\\Users\\user\n git clone https://github.com/Juniper/contrail-windows-ci.git\nPS C:\\Users\\user\n cd contrail-windows-ci\\backups\nPS C:\\Users\\user\\contrail-windows-ci\\backups\n\n\n\n\n\n\nEstablish a PowerShell session with \nwinci-veeam\n server\n\n\npowershell\nPS C:\\Users\\user\\contrail-windows-ci\\backups\n $session = New-PSSession -ComputerName 10.84.12.29 -Credentials $(Get-Credential)\n\n\n\n\n\n\nCopy \nbackups\n directory's contents to \nwinci-veeam\n server, to \nC:\\BackupScripts\n directory\n\n\npowershell\nPS C:\\Users\\user\\contrail-windows-ci\\backups\n Copy-Item .\\* C:\\BackupScripts -ToSession $session\n\n\n\n\n\n\nClose PowerShell session\n\n\npowershell\nPS C:\\Users\\user\\contrail-windows-ci\\backups\n Remove-PSSession $session", 
            "title": "Infrastructure backups"
        }, 
        {
            "location": "/ci_admin_guide/Infrastructure_backups/#infrastructure-backups", 
            "text": "This document describes current backup system configuration for Contrail Windows CI.  Table of contents   Current backup setup  VM recovery guide  Backup scripts update guide", 
            "title": "Infrastructure backups"
        }, 
        {
            "location": "/ci_admin_guide/Infrastructure_backups/#current-backup-setup", 
            "text": "Diagram below presents an overview of current backup setup for Contrail Windows CI.    Contrail Windows CI underlying infrastructure is based VMware vSphere  Services support Contrail Windows CI are running on a set of  Infrastructure VMs  Infrastructure VMs reside on  WinCI-Datastores-Infra  datastore cluster  WinCI-Datastores-Infra datastore cluster consists of an independent set of SSD-based datastores  independent, meaning these datastores are used solely for Infrastructure VMs    Backups are performed using Veeam Backup   Replication software  Veeam Backup Free Edition is used    Veeam is installed on  winci-veeam , Windows Server 2016 virtual machine  winci-veeam  has a mounted NFS share  winci_nfsbackup  which is used as a backups repository  Veeam connects to vSphere cluster using vSphere API  Veeam performs full VM backups (using VeeamZIP feature in the Free edition)   Backups are performed regulary using  Windows Scheduled Tasks .\nCurrently scheduled task runs every day, at 10 PM PST.\nScheduled task executes a PowerShell Script  Backup.ps1  located in  C:\\BackupScripts\\  directory.  Backup procedure looks as follows:   Backups are performed by a set of PowerShell scripts  Scripts are located in  backups/  directory in  contrail-windows-ci  repository    Backup.ps1  script performs the following steps:  Prunes backups by removing all but last  N  backups  N  is configurable in the script    Uses VeeamZIP PowerShell cmdlets to perform a full backups of VMs listed in  Backup.ps1  file    If an error occurs for any of the VMs listed, script continues its job. After performing backups for other VMs it terminates with an exception  Each full backup is stored in a separate directory, located in  \\\\winci_nfsbackups\\Backups\\  directory on backup NFS share", 
            "title": "Current backup setup"
        }, 
        {
            "location": "/ci_admin_guide/Infrastructure_backups/#vm-recovery-guide", 
            "text": "Current backup setup supports only full VM recovery. It needs to be performed, e.g. when:   in case of underlying infrastructure hardware failure (e.g. corrupted disk, ESXi host failure)  in case of bricking an operating system/virtual machine  recovery of VM state from a certain point in time is desirable   Prerequisites to backup scripts updating:   Windows 10/Windows Server 2016 machine to perform the steps  Credentials to  winci-veeam  server  Please contact Windows CI team     To recover a VM from backup perform the following steps:   Establish a RDP connection  winci-veeam  server  Open  Veeam Backup   Replication Console  program is available through Start menu    Veeam login window should open up  ensure that  localhost  and port  9392  are selected and  Use Windows session authentication  option is selected  click  Connect    In the upper left corner click an option  Restore  Select backup file to restore  windows should open up  Click  browse  In the address bar type in  X:\\Backups\\  A list of directories named with date timestamps should show up  Select a directory with a date, matching your desired restore point  Enter this directory  Select a backup file (file with  .vbk  extension) which has prefix matching VM to be restored  Click  Open    Progress bar named  Reading the backup file, please wait...  should show up  When a table named  Contained VMs  appears, select a VM to be restored, click  Restore ; clicking  Restore  opens up contextual menu; click option  Entire VM (including registration)  After a few moments a restore wizard should show up  In  Virtual machine  section  From a table  Virtual machines to restore  select a VM to be restored  Click  Next    In  Restore Mode  section  Select  Restore to the original location  option  Make sure  Restore VM tags  option is checked  Make sure  Quick rollback  option is unchecked  Click  Next    In  Reason  section  Enter a reason for VM restoring into  Restore reasons  textbox  Click  Next    In  Summary  section  Verify VM details presented in  Summary  textbox  Check  Power on target virtual machine  option if you want the restored VM to power on after restoration is completed  Click  Finish    VM Restore  window with progress report should appear  Window can be closed by clicking  Close  button, this will not stop the restore procedure    To open up  VM Restore  window again  Click  History  button in the lower left corner of Veeam Console  In the tree (in the left middle part of the Veeam Console) select  Restore   Full VM Restore  branch  A list of restore jobs should appear in the middle  Double-click a job corresponding to a VM you are currently restoring  VM Restore  window should open up again    When a restore job finishes, verify if all services provided by the machine are up and working", 
            "title": "VM recovery guide"
        }, 
        {
            "location": "/ci_admin_guide/Infrastructure_backups/#backup-scripts-update-guide", 
            "text": "Backup scripts need to be updated, e.g. when:   A list of VMs requiring backup changes  A bug was fixed in the backup scripts   Prerequisites to backup scripts updating:   Windows 10/Windows Server 2016 machine to perform the steps  Credentials to  winci-veeam  server  Please contact Windows CI team     To update backup scripts perform the following steps:    Clone  contrail-windows-ci  repository and enter  backups/  directory  powershell\nPS C:\\Users\\user  git clone https://github.com/Juniper/contrail-windows-ci.git\nPS C:\\Users\\user  cd contrail-windows-ci\\backups\nPS C:\\Users\\user\\contrail-windows-ci\\backups    Establish a PowerShell session with  winci-veeam  server  powershell\nPS C:\\Users\\user\\contrail-windows-ci\\backups  $session = New-PSSession -ComputerName 10.84.12.29 -Credentials $(Get-Credential)    Copy  backups  directory's contents to  winci-veeam  server, to  C:\\BackupScripts  directory  powershell\nPS C:\\Users\\user\\contrail-windows-ci\\backups  Copy-Item .\\* C:\\BackupScripts -ToSession $session    Close PowerShell session  powershell\nPS C:\\Users\\user\\contrail-windows-ci\\backups  Remove-PSSession $session", 
            "title": "Backup scripts update guide"
        }, 
        {
            "location": "/ci_admin_guide/List_of_VMs_in_Windows_CI/", 
            "text": "List of all important VMs in Windows CI\n\n\n\n\n\n\nVMware:\n\n\n\n\nci-vc\n\n\nvCenter Server on Windows\n\n\nCritical\n\n\nREQUIRES BACKUP\n\n\n\n\n\n\nci-vc-um\n\n\nvCenter Update Manager\n\n\nREQUIRES BACKUP\n\n\n\n\n\n\n\n\n\n\n\n\nBase OS templates:\n\n\n\n\nWindows\n - Install OS (manually) + instruction (manually)\n\n\nPreferrably: automate\n\n\n\n\n\n\nCentOS\n - Prepared manually, no instruction\n\n\nTODO: Instruction with requirements\n\n\nRequirements:\n\n\nroot access SSH (password-based)\n\n\n2 NICs\n\n\nTODO: Verify if there are no more requirements\n\n\n\n\n\n\n\n\n\n\nUbuntu\n - No template, required to provision Ubuntu-based VMs\n\n\nTODO: Instruction/automation\n\n\nRequirements:\n\n\nciadmin\n user (SSH accessible, password-based)\n\n\npython3\n\n\n1 NIC\n\n\nTODO: Verify if there are no more requirements\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCI templates:\n\n\n\n\nbuilder\n - Ansible, Jenkins job\n\n\nRequires Windows template\n\n\n\n\n\n\ntester\n - Ansible, Jenkins job\n\n\nRequires Windows template \n\n\n\n\n\n\ntestbed\n - Ansible\n\n\nRequires Windows template\n\n\n\n\n\n\n\n\n\n\n\n\nInfra:\n\n\n\n\nbuilders\n - Ansible, Jenkins job\n\n\ntesters\n - Ansible, Jenkins job\n\n\nmgmt-dhcp\n - Created manually, DHCP server + configuration (172.17.0.0/24)\n\n\nimpact if down:\n\n\nwill cause failure in all production CI jobs due to lack of dhcp for testbed machines\n\n\nwill not affect demo-env, dev-env and other machines in public network\n\n\n\n\n\n\nfix cost:\n\n\nAutomate deployment\n\n\nBefore automation, backup\n\n\nVMware HA\n\n\n\n\n\n\n\n\n\n\nwinci-drive\n - Created manually, network drive with (probably) temporary content\n\n\nimpact if down: \n\n\nno impact on production CI\n\n\ndegrades debuggability of CI (cannot upload or use ready artifacts)\n\n\nimpacts ability to create builder templates\n\n\n\n\n\n\nfix cost:\n\n\nwill need to recreate directory structure and copy contents from one of the builders\n\n\nthis process is not documented to this degree of detail (no list of dependencies)\n\n\nBefore automation, backup\n\n\nVMware HA\n\n\n\n\n\n\n\n\n\n\nwinci-graphite\n - Created manually\n\n\nimpact if down:\n\n\nno impact on production CI\n\n\ndegrades CI monitoring\n\n\n\n\n\n\nfix cost:\n\n\nAutomate VM provisioning\n\n\nAutomate configuration and dashboard deployment\n\n\nBackup needed to preserve historical data\n\n\n\n\n\n\n\n\n\n\nwinci-monitoring\n - Created manually\n\n\nimpact if down:\n\n\nTODO: does this affect production job (namely - post stage will fail, right?)\n\n\n\n\n\n\nfix cost:\n\n\nTODO\n\n\n\n\n\n\n\n\n\n\nwinci-jenkins\n - Created manually, plugins + configuration\n\n\nimpact if down:\n\n\nABSOLUTELY CRITICAL\n\n\n\n\n\n\nfix cost:\n\n\nBACKUP\n\n\njob configuration\n\n\njob logs\n\n\nlogs server ssh keys\n\n\njenkins plugin list (initial version on contrail-windows-ci; need to check if works)\n\n\ncredentials\n\n\n\n\n\n\nAutomate configuration\n\n\nVMware HA\n\n\n\n\n\n\n\n\n\n\nwinci-mgmt\n - Created manually, Python requirements.txt have to be installed\n\n\nimpact if down:\n\n\nABSOLUTELY CRITICAL (cannot create testenvs)\n\n\n\n\n\n\nfix cost:\n\n\nAutomate provisioning \n\n\nUbuntu configured for Ansible\n\n\nUbuntu configured for Python tests\n\n\nUbuntu configured for monitoring scripts\n\n\n\n\n\n\nBackup\n\n\nansible-vault-key\n\n\nlogs server ssh keys\n\n\ngithub deploy ssh key\n\n\n\n\n\n\nVMware HA\n\n\n\n\n\n\n\n\n\n\nwinci-vyos-mgmt\n\n\nimpact if down:\n\n\nABSOLUTELY CRITICAL (cannot deploy contrail controllers)\n\n\n\n\n\n\nfix cost:\n\n\nBackup\n\n\nDocument configuration\n\n\nVMware HA\n\n\n\n\n\n\n\n\n\n\nwinci-zuulv2-production\n - Created manually with Ansible script\n\n\nimpact if down:\n\n\nABSOLUTELY CRITICAL (cannot run jobs)\n\n\n\n\n\n\nfix cost:\n\n\nAlready automated\n\n\nCreate prod and dev inventories for Zuul\n\n\n\n\n\n\nBackup:\n\n\ngerrit ssh keys\n\n\njenkins ssh keys\n\n\n\n\n\n\nVMware HA\n\n\n\n\n\n\n\n\n\n\nwinci-purgatory\n - Probably created manually? What is installed there?\n\n\nimpact if down:\n\n\nABSOLUTELY CRITICAL (cannot run jobs)\n\n\n\n\n\n\nfix cost:\n\n\npending PR for automation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHosts:\n\n\n\n\nesxi1-5\n\n\nfix cost:\n\n\nVMware HA on critical virtual machines\n\n\nbackup konfiguracji\n\n\n\n\n\n\n\n\n\n\nSSD/SATA drivers\n\n\nfix cost:\n\n\nTODO: Determine if servers can into RAID\n\n\nMaintenance to configure RAID\n\n\nIf not RAID, then shared storage for critical VMs is needed\n\n\nPreferrably some storage array with RAID configured\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNICs\n\n\nwe lack redundancy in case of main NIC failure\n\n\n\n\n\n\n\n\n\n\n\n\nBackup:\n\n\n\n\nAdditional storage space for backup, independent of VMware storage\n\n\nTODO: Backup method?\n\n\nTODO: Backup tool?\n\n\nTODO: Backup procedure?\n\n\nTODO: DR procedure for critical VMs?\n\n\nTODO: hardware monitoring?", 
            "title": "List of all important VMs in Windows CI"
        }, 
        {
            "location": "/ci_admin_guide/List_of_VMs_in_Windows_CI/#list-of-all-important-vms-in-windows-ci", 
            "text": "VMware:   ci-vc  vCenter Server on Windows  Critical  REQUIRES BACKUP    ci-vc-um  vCenter Update Manager  REQUIRES BACKUP       Base OS templates:   Windows  - Install OS (manually) + instruction (manually)  Preferrably: automate    CentOS  - Prepared manually, no instruction  TODO: Instruction with requirements  Requirements:  root access SSH (password-based)  2 NICs  TODO: Verify if there are no more requirements      Ubuntu  - No template, required to provision Ubuntu-based VMs  TODO: Instruction/automation  Requirements:  ciadmin  user (SSH accessible, password-based)  python3  1 NIC  TODO: Verify if there are no more requirements         CI templates:   builder  - Ansible, Jenkins job  Requires Windows template    tester  - Ansible, Jenkins job  Requires Windows template     testbed  - Ansible  Requires Windows template       Infra:   builders  - Ansible, Jenkins job  testers  - Ansible, Jenkins job  mgmt-dhcp  - Created manually, DHCP server + configuration (172.17.0.0/24)  impact if down:  will cause failure in all production CI jobs due to lack of dhcp for testbed machines  will not affect demo-env, dev-env and other machines in public network    fix cost:  Automate deployment  Before automation, backup  VMware HA      winci-drive  - Created manually, network drive with (probably) temporary content  impact if down:   no impact on production CI  degrades debuggability of CI (cannot upload or use ready artifacts)  impacts ability to create builder templates    fix cost:  will need to recreate directory structure and copy contents from one of the builders  this process is not documented to this degree of detail (no list of dependencies)  Before automation, backup  VMware HA      winci-graphite  - Created manually  impact if down:  no impact on production CI  degrades CI monitoring    fix cost:  Automate VM provisioning  Automate configuration and dashboard deployment  Backup needed to preserve historical data      winci-monitoring  - Created manually  impact if down:  TODO: does this affect production job (namely - post stage will fail, right?)    fix cost:  TODO      winci-jenkins  - Created manually, plugins + configuration  impact if down:  ABSOLUTELY CRITICAL    fix cost:  BACKUP  job configuration  job logs  logs server ssh keys  jenkins plugin list (initial version on contrail-windows-ci; need to check if works)  credentials    Automate configuration  VMware HA      winci-mgmt  - Created manually, Python requirements.txt have to be installed  impact if down:  ABSOLUTELY CRITICAL (cannot create testenvs)    fix cost:  Automate provisioning   Ubuntu configured for Ansible  Ubuntu configured for Python tests  Ubuntu configured for monitoring scripts    Backup  ansible-vault-key  logs server ssh keys  github deploy ssh key    VMware HA      winci-vyos-mgmt  impact if down:  ABSOLUTELY CRITICAL (cannot deploy contrail controllers)    fix cost:  Backup  Document configuration  VMware HA      winci-zuulv2-production  - Created manually with Ansible script  impact if down:  ABSOLUTELY CRITICAL (cannot run jobs)    fix cost:  Already automated  Create prod and dev inventories for Zuul    Backup:  gerrit ssh keys  jenkins ssh keys    VMware HA      winci-purgatory  - Probably created manually? What is installed there?  impact if down:  ABSOLUTELY CRITICAL (cannot run jobs)    fix cost:  pending PR for automation         Hosts:   esxi1-5  fix cost:  VMware HA on critical virtual machines  backup konfiguracji      SSD/SATA drivers  fix cost:  TODO: Determine if servers can into RAID  Maintenance to configure RAID  If not RAID, then shared storage for critical VMs is needed  Preferrably some storage array with RAID configured        NICs  we lack redundancy in case of main NIC failure       Backup:   Additional storage space for backup, independent of VMware storage  TODO: Backup method?  TODO: Backup tool?  TODO: Backup procedure?  TODO: DR procedure for critical VMs?  TODO: hardware monitoring?", 
            "title": "List of all important VMs in Windows CI"
        }, 
        {
            "location": "/ci_admin_guide/Merge_development_to_production/", 
            "text": "Merge development to production\n\n\nThis document describes the steps required to perform a merge from \ndevelopment\n to \nproduction\n in Contrail Windows CI.\n\n\nPre-merge check\n\n\nRun production pipeline on \ndevelopment\n branch\n\n\n\n\nPreferably freeze \n$commitId\n of \ndevelopment\n branch\n\n\nClone \nwinci-server2016-prod\n to \nwinci-prod-test\n\n\nChange \nwinci-prod-test\n configuration\n\n\nProperties content: \nBRANCH_NAME=$commitId\n\n\nBranches to build: \n$commitId\n\n\n\n\n\n\nRun \nwinci-prod-test\n\n\nParameters:\n\n\nZUUL_PROJECT=Juniper/contrail-controller\n\n\nZUUL_BRANCH=master\n\n\nZUUL_REF=None\n (it should literally be \nNone\n)\n\n\nZUUL_URL=http://10.84.12.30/merge-warrior\n\n\nZUUL_UUID=$someRandomUUID\n (e.g. $commitId with the first 8 characters replaced by \ndeadbeef\n)\n\n\nZUUL_CHANGE=\n (can be left empty)\n\n\nZUUL_PATCHSET=\n (can be left empty)\n\n\n\n\n\n\n\n\n\n\n\n\nMerge development to production\n\n\n\n\n\n\nUpdate production branch (following git operations require administrative privileges on GitHub)\n\n\nbash\ngit fetch --all --prune\ngit checkout development\ngit pull\ngit checkout production\ngit merge development\ngit push\n\n\n\n\n\n\nUpdate Zuul configuration\n\n\n\n\nRefer to \nUpdate Windows CI Zuulv2\n documentation\n\n\n\n\n\n\n\n\nZuul update playbook\n\n\n---\n- hosts: zuul\n  remote_user: ciadmin\n  become: yes\n  roles:\n  - zuul\n\n\n\n\nPost checks\n\n\n\n\n\n\nCheck Zuul services:\n\n\n\n\nzuul-merger\n\n\nzuul-server\n\n\n\n\nbash\nlocalhost $ ssh winci-zuulv2-production # with provided credentials; current address 10.84.12.75\nsystemctl status zuul-merger.service # should be active (running)\nsystemctl status zuul-server.service # should be active (running)\n\n\n\n\n\n\nTrigger Zuul job\n\n\n\n\nCreate a dummy PR on Gerrit\n\n\nPost \nrecheck windows\n on some existing PR\n\n\n\n\n\n\n\n\nFuture considerations\n\n\n\n\nSlave VM templates creation and promotion\n\n\nBuilder/tester deployment", 
            "title": "Merge development to production"
        }, 
        {
            "location": "/ci_admin_guide/Merge_development_to_production/#merge-development-to-production", 
            "text": "This document describes the steps required to perform a merge from  development  to  production  in Contrail Windows CI.", 
            "title": "Merge development to production"
        }, 
        {
            "location": "/ci_admin_guide/Merge_development_to_production/#pre-merge-check", 
            "text": "Run production pipeline on  development  branch   Preferably freeze  $commitId  of  development  branch  Clone  winci-server2016-prod  to  winci-prod-test  Change  winci-prod-test  configuration  Properties content:  BRANCH_NAME=$commitId  Branches to build:  $commitId    Run  winci-prod-test  Parameters:  ZUUL_PROJECT=Juniper/contrail-controller  ZUUL_BRANCH=master  ZUUL_REF=None  (it should literally be  None )  ZUUL_URL=http://10.84.12.30/merge-warrior  ZUUL_UUID=$someRandomUUID  (e.g. $commitId with the first 8 characters replaced by  deadbeef )  ZUUL_CHANGE=  (can be left empty)  ZUUL_PATCHSET=  (can be left empty)", 
            "title": "Pre-merge check"
        }, 
        {
            "location": "/ci_admin_guide/Merge_development_to_production/#merge-development-to-production_1", 
            "text": "Update production branch (following git operations require administrative privileges on GitHub)  bash\ngit fetch --all --prune\ngit checkout development\ngit pull\ngit checkout production\ngit merge development\ngit push    Update Zuul configuration   Refer to  Update Windows CI Zuulv2  documentation", 
            "title": "Merge development to production"
        }, 
        {
            "location": "/ci_admin_guide/Merge_development_to_production/#zuul-update-playbook", 
            "text": "---\n- hosts: zuul\n  remote_user: ciadmin\n  become: yes\n  roles:\n  - zuul", 
            "title": "Zuul update playbook"
        }, 
        {
            "location": "/ci_admin_guide/Merge_development_to_production/#post-checks", 
            "text": "Check Zuul services:   zuul-merger  zuul-server   bash\nlocalhost $ ssh winci-zuulv2-production # with provided credentials; current address 10.84.12.75\nsystemctl status zuul-merger.service # should be active (running)\nsystemctl status zuul-server.service # should be active (running)    Trigger Zuul job   Create a dummy PR on Gerrit  Post  recheck windows  on some existing PR", 
            "title": "Post checks"
        }, 
        {
            "location": "/ci_admin_guide/Merge_development_to_production/#future-considerations", 
            "text": "Slave VM templates creation and promotion  Builder/tester deployment", 
            "title": "Future considerations"
        }, 
        {
            "location": "/ci_admin_guide/Run_gerrit_pipeline_on_github_PR/", 
            "text": "Run gerrit pipeline on github PR\n\n\nWhen to do it:\n\n\nYou introduce a change to github/gerrit handling code, e.g. the\ntrigger mechanism - code that depends on Zuul or other Jenkins trigger plugins\n\n\n\n\n\n\nClone Jenkins job \nwinci-server2016-devel\n to some temporary \ntmp-devel-gerrit-check\n\n\nConfigure \ntmp-devel-gerrit-check\n:\n\n\nProperties content: BRANCH_NAME={branch to test}\n\n\nBranches to build: {branch to test}\n\n\nSchedule \ntmp-devel-gerrit-check\n job manually with parameters:\n\n\nZUUL_PROJECT=Juniper/contrail-controller\n\n\nZUUL_BRANCH=master\n\n\nZUUL_REF=None\n\n\nZUUL_URL=http://10.84.12.75/merge-warrior\n\n\nZUUL_UUID= (some random UUID - it will determine path on the logserver)\n\n\nZUUL_CHANGE= (can be left empty)\n\n\nZUUL_PATCHSET= (can be left empty)\n\n\nVerify that it passes\n\n\n(optional, recommended) Paste a link to successful build logs under your PR on github for the reviewers to verify\n\n\nRemove temporary \ntmp-devel-gerrit-check\n job", 
            "title": "Run gerrit pipeline on github PR"
        }, 
        {
            "location": "/ci_admin_guide/Run_gerrit_pipeline_on_github_PR/#run-gerrit-pipeline-on-github-pr", 
            "text": "When to do it:  You introduce a change to github/gerrit handling code, e.g. the\ntrigger mechanism - code that depends on Zuul or other Jenkins trigger plugins", 
            "title": "Run gerrit pipeline on github PR"
        }, 
        {
            "location": "/ci_admin_guide/Update_Jenkins_slaves/", 
            "text": "Updating Jenkins slaves\n\n\nRolling out new slaves\n\n\nTODO\n- update templates\n- create templates\n- spawn new slaves\n- decomission old slaves\n- tags\n- how to test if it worked safely\n\n\nUpdating existing slaves\n\n\nSome minor maintenance work does not require rollout of new slaves. An example might be an\nupgrade of some library or other dependency. In this case, one can update the running cluster\nwithout having to create a new template and rolling out every machine. However, the template must\nbe updated at the end of the procedure, so that future fresh machines will also have the\nrequired changes.\n\n\nThe upgrade procedure consists of:\n1. Manually upgrade a subset of nodes\n1. Test if CI job pass on the new set of nodes\n1. Upgrade the rest of the nodes\n\n\n1. Manually upgrade a subset of nodes\n\n\n\n\nSelect one Jenkins slave to use.\n\n\nMake sure that there are other slaves that perform the same function (they have the same tags and are up).\n\n\nIn Jenkins UI, press \nMark this node as temporarily offline\n, providing the reason.\n\n\nNote the IP address of the slave.\n\n\n\n\n\n\n\n\nManually run ansible against the node.\n\n\n\n\nNOTE: this is a one-off role. It doesn't necessarily need to be commited to\nversion control. Therefore, any \"hacks\" are allowed. For example, to upgrade\na chocolatey package, in addition to updating the \nversion\n parameter of\n\nwin_chocolatey\n role, one must also run an additional task of uninstalling\nthe package first. See examples below.\n\n\n\n\n\n\nCheckout \ngithub.com/Juniper/contrail-windows-ci\n repository.\n\n\nEnter \nansible/\n directory.\n\n\nPrepare your environment by going through steps described in \nREADME.md\n. \n\n\nFind the ansible role that is responsible for the upgrade.\n\n\nModify the role to suit your needs. \n\n\nAdd a tag to every task that needs to be executed.\n\n\n\n\n\n\nPrepare inventory file.\n\n\nEdit file \ninventory.devel/groups\n.\n\n\nAdd IP of your selected slave under the correct group.\n\n\n\n\n\n\nCreate one-off playbook.\n\n\nCreate an yml file in the root of \nansible/\n dir.\n\n\nSpecify hosts and roles to be executed.\n\n\n\n\n\n\nRun the playbook, specifying ansible vault, the inventory and prepared playbook.\n\n\nTest if the change worked.\n\n\nRemote into the machine and verify if change was successful.\n\n\nTest if job passes.\n\n\nIn Jenkins, go into the selected node view. Remove all tags it has. Add some temporary tag (e.g. 'temp-upgrade').\n\n\nOpen a test pull request to \ngithub.com/Juniper/contrail-windows-ci\n\n\nIn its Jenksinfile, replace tags of nodes that you removed on t he node with the temporary tag.\n\n\nOpen the pull request with \"do not merge\" in the title.\n\n\nWait for CI to be triggered normally. Wait for it to pass.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNOTE: the following steps may change depending on case-by-case basic.\n\n\n\n\n\n\n\n\nOpen a normal pull request with cleaned up changes to ansible roles.\n\n\n\n\nGet it merged.\n\n\nCreate a new builder template. (see \nthis\n)\n\n\nRollout the change to other slaves by repeating steps 1-2 but specifying larger subsets of nodes.\n\n\n\n\nExamples.\n\n\n\n\nUpgrade golang version.\n\n\n\n\nansible/roles/builder/tasks/main.yml:\n\n\n...\n- name: Uninstall golang\n  win_chocolatey:\n    name: golang\n    state: absent\n  tags: bump\n\n - name: Install golang\n   win_chocolatey:\n     name: golang\n     version: 1.10.0\n     state: present\n  tags: bump\n...\n\n\n\n\nansible/mysite.yml\n\n\n---\n- name: 'bump golang'\n  hosts: \n    - builder\n  roles: \n    - builder\n\n\n\n\ninventory.devel/groups:\n\n\n...\n[builder]\n10.84.12.87\n...\n\n\n\n\nCommand to run:\n\n\nansible-playbook -i inventory.devel --vault-password-file ~/.ansible-vault --tags \nbump\n mysite.yml", 
            "title": "Updating Jenkins slaves"
        }, 
        {
            "location": "/ci_admin_guide/Update_Jenkins_slaves/#updating-jenkins-slaves", 
            "text": "", 
            "title": "Updating Jenkins slaves"
        }, 
        {
            "location": "/ci_admin_guide/Update_Jenkins_slaves/#rolling-out-new-slaves", 
            "text": "TODO\n- update templates\n- create templates\n- spawn new slaves\n- decomission old slaves\n- tags\n- how to test if it worked safely", 
            "title": "Rolling out new slaves"
        }, 
        {
            "location": "/ci_admin_guide/Update_Jenkins_slaves/#updating-existing-slaves", 
            "text": "Some minor maintenance work does not require rollout of new slaves. An example might be an\nupgrade of some library or other dependency. In this case, one can update the running cluster\nwithout having to create a new template and rolling out every machine. However, the template must\nbe updated at the end of the procedure, so that future fresh machines will also have the\nrequired changes.  The upgrade procedure consists of:\n1. Manually upgrade a subset of nodes\n1. Test if CI job pass on the new set of nodes\n1. Upgrade the rest of the nodes", 
            "title": "Updating existing slaves"
        }, 
        {
            "location": "/ci_admin_guide/Update_Jenkins_slaves/#1-manually-upgrade-a-subset-of-nodes", 
            "text": "Select one Jenkins slave to use.  Make sure that there are other slaves that perform the same function (they have the same tags and are up).  In Jenkins UI, press  Mark this node as temporarily offline , providing the reason.  Note the IP address of the slave.     Manually run ansible against the node.   NOTE: this is a one-off role. It doesn't necessarily need to be commited to\nversion control. Therefore, any \"hacks\" are allowed. For example, to upgrade\na chocolatey package, in addition to updating the  version  parameter of win_chocolatey  role, one must also run an additional task of uninstalling\nthe package first. See examples below.    Checkout  github.com/Juniper/contrail-windows-ci  repository.  Enter  ansible/  directory.  Prepare your environment by going through steps described in  README.md .   Find the ansible role that is responsible for the upgrade.  Modify the role to suit your needs.   Add a tag to every task that needs to be executed.    Prepare inventory file.  Edit file  inventory.devel/groups .  Add IP of your selected slave under the correct group.    Create one-off playbook.  Create an yml file in the root of  ansible/  dir.  Specify hosts and roles to be executed.    Run the playbook, specifying ansible vault, the inventory and prepared playbook.  Test if the change worked.  Remote into the machine and verify if change was successful.  Test if job passes.  In Jenkins, go into the selected node view. Remove all tags it has. Add some temporary tag (e.g. 'temp-upgrade').  Open a test pull request to  github.com/Juniper/contrail-windows-ci  In its Jenksinfile, replace tags of nodes that you removed on t he node with the temporary tag.  Open the pull request with \"do not merge\" in the title.  Wait for CI to be triggered normally. Wait for it to pass.        NOTE: the following steps may change depending on case-by-case basic.     Open a normal pull request with cleaned up changes to ansible roles.   Get it merged.  Create a new builder template. (see  this )  Rollout the change to other slaves by repeating steps 1-2 but specifying larger subsets of nodes.", 
            "title": "1. Manually upgrade a subset of nodes"
        }, 
        {
            "location": "/ci_admin_guide/Update_Jenkins_slaves/#examples", 
            "text": "Upgrade golang version.   ansible/roles/builder/tasks/main.yml:  ...\n- name: Uninstall golang\n  win_chocolatey:\n    name: golang\n    state: absent\n  tags: bump\n\n - name: Install golang\n   win_chocolatey:\n     name: golang\n     version: 1.10.0\n     state: present\n  tags: bump\n...  ansible/mysite.yml  ---\n- name: 'bump golang'\n  hosts: \n    - builder\n  roles: \n    - builder  inventory.devel/groups:  ...\n[builder]\n10.84.12.87\n...  Command to run:  ansible-playbook -i inventory.devel --vault-password-file ~/.ansible-vault --tags  bump  mysite.yml", 
            "title": "Examples."
        }, 
        {
            "location": "/ci_admin_guide/Update_Zuul/", 
            "text": "Update Windows CI Zuulv2\n\n\nThis document describes a procedure required to update Windows CI Zuulv2 configuration.\nThis procedure updates configuration using \ndevelopment\n or \nproduction\n branch from \ncontrail-windows-ci\n repository.\n\n\nPrerequisites\n\n\n\n\nUbuntu 16.04 or Windows Subsystem for Linux with Ubuntu\n\n\nIt will serve as Ansible control machine\n\n\n\n\n\n\nUser's public SSH key installed on Zuulv2 instance\n\n\nPlease contact Windows CI team\n\n\n\n\n\n\nAccess to ansible vault key\n\n\nPlease contact Windows CI team\n\n\n\n\n\n\n\n\nSteps\n\n\n\n\n\n\nInstall required \napt\n dependencies on Ansible control machine\n\n\nbash\napt-get install git python3 python3-pip python3-virtualenv\n\n\n\n\n\n\nClone \ncontrail-windows-ci\n repository\n\n\nbash\ncd ~/\ngit clone https://github.com/Juniper/contrail-windows-ci.git\ncd contrail-windows-ci\n\n\n\n\n\n\nCheckout branch that you want to use for configuring Zuul. For example:\n\n\n\n\n\n\nto checkout \ndevelopment\n branch:\n\n\nbash\ngit checkout development\n\n\n\n\n\n\nto checkout \nproduction\n branch:\n\n\nbash\ngit fetch --all\ngit branch production origin/production\ngit checkout production\n\n\n\n\n\n\n\n\n\n\nVerify that \ndevelopment\n (or \nproduction\n) branch contains PRs with required changes to Zuul configuration\n\n\n\n\nAssuming \nPR#2\n and \nPR#1\n are required PRs, run the following command:\n\n\n\n\nbash\ngit log --oneline\n\n\n\n\nOutput will contain a list of merged PRs, from newest to oldest. Required PRs should be at the top:\n\n\n\n\n```\nabcdabc PR#2\nabcd123 PR#1\ne8691f5 Some other PR\n3af841e Some other PR, part 2\n\n\n... omitted\n\n\n```\n\n\nExample\n\n\n\n\nAssume that \nadd contrail-infra-doc to gerrit (#212)\n is a latest PR that was merged and this change has to be applied on Zuul\n\n\nThen output of \ngit log\n should look like this:\n\n\n\n\n```\n93c783b add contrail-infra-doc to gerrit (#212)\n1f86f26 zuul: setup-zuul-server playbook; variables in prod inventory (#210)\n\n\n... omitted\n\n\n```\n\n\n\n\n\n\nMove to \nansible\n directory\n\n\nbash\ncd ansible\n\n\n\n\n\n\nCreate a virtualenv and install \npip\n dependencies\n\n\nbash\npython3 -m virtualenv -p /usr/bin/python3 venv\nsource venv/bin/activate\npip install -r python-requirements.txt\n\n\n\n\n\n\nCreate a file for Ansible vault key and populate it with the key\n\n\nbash\ntouch ~/ansible-vault-key\nvim ~/ansible-vault-key  # enter ansible vault key into a file\n\n\n\n\n\n\nTest connection to Zuul with Ansible\n\n\nbash\nansible -i inventory.prod --private-key=YOUR_PRIVATE_KEY zuul -m ping\n\n\n\n\nwhere \nYOUR_PRIVATE_KEY\n is a path to your SSH private key\n\n\n\n\n\n\n\n\nRun \nsetup-zuul-server.yml\n playbook\n\n\nbash\nansible-playbook -i inventory.prod --private-key=YOUR_PRIVATE_KEY setup-zuul-server.yml\n\n\nVerify that the run completed successfully.\nThe output should be following and \nfailed\n task count must equal zero.\n\n\n```\n\n\n\n\ntask list omitted for brevity\n\n\n\n\nPLAY RECAP \n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n10.84.12.75                : ok=22   changed=2    unreachable=0    failed=0\n\n\n\n\n... - task run time omitted for brevity\n\n\n\n\n```\n\n\n\n\nfailed\n task count should be equal to zero", 
            "title": "Update Windows CI Zuulv2"
        }, 
        {
            "location": "/ci_admin_guide/Update_Zuul/#update-windows-ci-zuulv2", 
            "text": "This document describes a procedure required to update Windows CI Zuulv2 configuration.\nThis procedure updates configuration using  development  or  production  branch from  contrail-windows-ci  repository.", 
            "title": "Update Windows CI Zuulv2"
        }, 
        {
            "location": "/ci_admin_guide/Update_Zuul/#prerequisites", 
            "text": "Ubuntu 16.04 or Windows Subsystem for Linux with Ubuntu  It will serve as Ansible control machine    User's public SSH key installed on Zuulv2 instance  Please contact Windows CI team    Access to ansible vault key  Please contact Windows CI team", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/ci_admin_guide/Update_Zuul/#steps", 
            "text": "Install required  apt  dependencies on Ansible control machine  bash\napt-get install git python3 python3-pip python3-virtualenv    Clone  contrail-windows-ci  repository  bash\ncd ~/\ngit clone https://github.com/Juniper/contrail-windows-ci.git\ncd contrail-windows-ci    Checkout branch that you want to use for configuring Zuul. For example:    to checkout  development  branch:  bash\ngit checkout development    to checkout  production  branch:  bash\ngit fetch --all\ngit branch production origin/production\ngit checkout production      Verify that  development  (or  production ) branch contains PRs with required changes to Zuul configuration   Assuming  PR#2  and  PR#1  are required PRs, run the following command:   bash\ngit log --oneline   Output will contain a list of merged PRs, from newest to oldest. Required PRs should be at the top:   ```\nabcdabc PR#2\nabcd123 PR#1\ne8691f5 Some other PR\n3af841e Some other PR, part 2", 
            "title": "Steps"
        }, 
        {
            "location": "/ci_admin_guide/Update_Zuul/#omitted", 
            "text": "```  Example   Assume that  add contrail-infra-doc to gerrit (#212)  is a latest PR that was merged and this change has to be applied on Zuul  Then output of  git log  should look like this:   ```\n93c783b add contrail-infra-doc to gerrit (#212)\n1f86f26 zuul: setup-zuul-server playbook; variables in prod inventory (#210)", 
            "title": "... omitted"
        }, 
        {
            "location": "/ci_admin_guide/Update_Zuul/#omitted_1", 
            "text": "```    Move to  ansible  directory  bash\ncd ansible    Create a virtualenv and install  pip  dependencies  bash\npython3 -m virtualenv -p /usr/bin/python3 venv\nsource venv/bin/activate\npip install -r python-requirements.txt    Create a file for Ansible vault key and populate it with the key  bash\ntouch ~/ansible-vault-key\nvim ~/ansible-vault-key  # enter ansible vault key into a file    Test connection to Zuul with Ansible  bash\nansible -i inventory.prod --private-key=YOUR_PRIVATE_KEY zuul -m ping   where  YOUR_PRIVATE_KEY  is a path to your SSH private key     Run  setup-zuul-server.yml  playbook  bash\nansible-playbook -i inventory.prod --private-key=YOUR_PRIVATE_KEY setup-zuul-server.yml  Verify that the run completed successfully.\nThe output should be following and  failed  task count must equal zero.  ```", 
            "title": "... omitted"
        }, 
        {
            "location": "/ci_admin_guide/Update_Zuul/#task-list-omitted-for-brevity", 
            "text": "", 
            "title": "task list omitted for brevity"
        }, 
        {
            "location": "/ci_admin_guide/Update_Zuul/#-task-run-time-omitted-for-brevity", 
            "text": "", 
            "title": "... - task run time omitted for brevity"
        }, 
        {
            "location": "/ci_admin_guide/Update_private_docker_registry/", 
            "text": "How to update private docker registry\n\n\nWhen is the update needed? What need to be updated?\n\n\nThe update of local docker registry is required when the version of Controller used in Windows CI is updated to a newer one. All Contrail images required by Contrail Ansible Deployer running in Windows CI have to be updated to version matching selected Controller version. Some new cached Openstack images may be required by updated Contrail Ansible Deployer.\n\n\nHow to update\n\n\nHow to update Contrail images\n\n\n\n\n\n\nChoose Contrail version tag you want to be cached in private docker registry:\n\n\n\n\nall available tags can be found \nhere\n.\n\n\n\n\n\n\n\n\nLog into \nwinci-registry\n:\n\n\n\n\nplease contact Windows CI team for access.\n\n\n\n\n\n\n\n\nCheck if there is \nupdate-docker-registry.py\n script in root directory.\n\n\n\n\n\n\nIf there is no such script:\n\n\n\n\n\n\ndownload it from \nhere\n:\n\n\ncurl https://raw.githubusercontent.com/Juniper/contrail-windows-ci/development/utility/update_docker_registry/update-docker-registry.py --output update-docker-registry.py\n\n\n\n\n\n\nmake it executable:\n\n\nchmod +x ./update-docker-registry.py\n\n\n\n\n\n\n\n\n\n\nRun the script with Contrail version tag as parameter, for example:\n\n\n./update-docker-registry.py ocata-master-206\n\n\n\n\n\n\nIf new version of Contrail Ansible Deployer uses more images than listed in \nupdate-docker-registry.py\n script:\n\n\n\n\nupdate images list in the script locally,\n\n\nopen up a PR with updated images list to contrail-windows-ci.\n\n\n\n\n\n\n\n\nHow to update Openstack images\n\n\nNew version of Contrail Ansible Deployer may also use some new Openstack images. Currently there is no script that does this automatically. For caching Openstack images manually please do the folowing:\n\n\n\n\n\n\nLog into \nwinci-registry\n:\n\n\n\n\nplease contact Windows CI team for access.\n\n\n\n\n\n\n\n\nSet the name of required image in local variable (replace \nIMAGE-NAME\n with proper value:\n\n\nimage_name=\nIMAGE-NAME\n\n\n\n\n\n\nPull the image:\n\n\ndocker image pull ci-repo.englab.juniper.net:5000/kolla/${image_name}:ocata\n\n\n\n\n\n\nGet the Image ID of previously pulled image:\n\n\nimage_id=$(docker image ls | grep kolla | grep ${image_name} | awk '{ print $3 }' | uniq)\n\n\n\n\n\n\nverify the ID:\n\n\necho \"$image_id\"\n\n\n\n\n\n\n\n\n\n\nTag the image:\n\n\ndocker tag ${image_id} localhost:5000/kolla/${image_name}:ocata\n\n\n\n\n\n\nPush the image to local repository:\n\n\ndocker push localhost:5000/kolla/${image_name}:ocata\n\n\n\n\n\n\nRepeat steps 2-6 for all other required images.", 
            "title": "How to update private docker registry"
        }, 
        {
            "location": "/ci_admin_guide/Update_private_docker_registry/#how-to-update-private-docker-registry", 
            "text": "", 
            "title": "How to update private docker registry"
        }, 
        {
            "location": "/ci_admin_guide/Update_private_docker_registry/#when-is-the-update-needed-what-need-to-be-updated", 
            "text": "The update of local docker registry is required when the version of Controller used in Windows CI is updated to a newer one. All Contrail images required by Contrail Ansible Deployer running in Windows CI have to be updated to version matching selected Controller version. Some new cached Openstack images may be required by updated Contrail Ansible Deployer.", 
            "title": "When is the update needed? What need to be updated?"
        }, 
        {
            "location": "/ci_admin_guide/Update_private_docker_registry/#how-to-update", 
            "text": "", 
            "title": "How to update"
        }, 
        {
            "location": "/ci_admin_guide/Update_private_docker_registry/#how-to-update-contrail-images", 
            "text": "Choose Contrail version tag you want to be cached in private docker registry:   all available tags can be found  here .     Log into  winci-registry :   please contact Windows CI team for access.     Check if there is  update-docker-registry.py  script in root directory.    If there is no such script:    download it from  here :  curl https://raw.githubusercontent.com/Juniper/contrail-windows-ci/development/utility/update_docker_registry/update-docker-registry.py --output update-docker-registry.py    make it executable:  chmod +x ./update-docker-registry.py      Run the script with Contrail version tag as parameter, for example:  ./update-docker-registry.py ocata-master-206    If new version of Contrail Ansible Deployer uses more images than listed in  update-docker-registry.py  script:   update images list in the script locally,  open up a PR with updated images list to contrail-windows-ci.", 
            "title": "How to update Contrail images"
        }, 
        {
            "location": "/ci_admin_guide/Update_private_docker_registry/#how-to-update-openstack-images", 
            "text": "New version of Contrail Ansible Deployer may also use some new Openstack images. Currently there is no script that does this automatically. For caching Openstack images manually please do the folowing:    Log into  winci-registry :   please contact Windows CI team for access.     Set the name of required image in local variable (replace  IMAGE-NAME  with proper value:  image_name= IMAGE-NAME    Pull the image:  docker image pull ci-repo.englab.juniper.net:5000/kolla/${image_name}:ocata    Get the Image ID of previously pulled image:  image_id=$(docker image ls | grep kolla | grep ${image_name} | awk '{ print $3 }' | uniq)    verify the ID:  echo \"$image_id\"      Tag the image:  docker tag ${image_id} localhost:5000/kolla/${image_name}:ocata    Push the image to local repository:  docker push localhost:5000/kolla/${image_name}:ocata    Repeat steps 2-6 for all other required images.", 
            "title": "How to update Openstack images"
        }, 
        {
            "location": "/ci_admin_guide/Windows_CI_logs_layout/", 
            "text": "Windows CI logs layout\n\n\nThis document describes a log structure generated by \nci-contrail-windows-production\n job.\nOne can access these logs by following a link \nci-contrail-windows-production\n in the comment posted by \nContrail Windows CI\n used, after a job has finished.\n\n\nLog structure\n\n\n\n\nlog.txt.gz\n - full Jenkins job log\n\n\nunittests-logs\n - logs for vRouter unit tests ran by Windows CI\n\n\nTestReports/CISelfcheck\n - test report for Windows CI unit tests suite\n\n\nfor information about more detailed logs please refer to \nCI Selfcheck\n\n\n\n\n\n\nTestReports/WindowsCompute\n - test report for Contrail Windows integration tests suite\n\n\nfor information about more detailed logs please refer to \nWindowsCompute\n\n\n\n\n\n\n\n\nCISelfcheck\n\n\nCISelfcheck\n is a set of unit tests for PowerShell scripts used in Windows CI.\n\n\nTestReports/CISelfcheck\n directory holds the following:\n\n\n\n\nTestReports/CISelfcheck/pretty_test_report\n - contains an HTML test report\n\n\nexample test report\n\n\ntests marked \ngreen\n have passed\n\n\ntests marked \nred\n have failed\n\n\ntests marked \norange\n are marked as inconclusive which means they are currently disabled in CI\n\n\n\n\n\n\nTestReports/CISelfcheck/raw_NUnit\n - contains NUnit XML files generated by PowerShell test runner\n\n\n\n\nWindowsCompute\n\n\nWindowsCompute\n is a set of integration tests for Contrail Windows.\nThis test suite is run on a 3-node cluster consisting of 1 Contrail Controller node (all-in-one deployment) and 2 Contrail Windows compute nodes.\nA fresh cluster is provisioned before each test.\nContrail Controller is provisioned using contrail-ansible-deployer.\nIn the future Contrail Windows compute nodes will also be provisioned with contrail-ansible-deployer.\n\n\nTestReports/WindowsCompute\n directory holds the following:\n\n\n\n\nTestReports/WindowsCompute/pretty_test_report\n - contains an HTML test report\n\n\nexample test report\n\n\ntests marked \ngreen\n have passed\n\n\ntests marked \nred\n have failed\n\n\ntests marked \norange\n are marked as inconclusive which means they are currently disabled in CI\n\n\n\n\n\n\nTestReports/WindowsCompute/ddriver_junit_test_logs\n - contains JUnit XML files generated by Contrail Windows Docker Driver test runner\n\n\nTestReports/WindowsCompute/detailed_logs\n - contains logs dumped from test environment for each test; log file can contain logs from:\n\n\nContrail Windows Docker Driver running on compute nodes\n\n\nWindows containers ran on compute nodes\n\n\n\n\n\n\nTestReports/WindowsCompute/raw_NUnit\n - contains NUnit XML files generated by PowerShell test runner", 
            "title": "Windows CI logs layout"
        }, 
        {
            "location": "/ci_admin_guide/Windows_CI_logs_layout/#windows-ci-logs-layout", 
            "text": "This document describes a log structure generated by  ci-contrail-windows-production  job.\nOne can access these logs by following a link  ci-contrail-windows-production  in the comment posted by  Contrail Windows CI  used, after a job has finished.", 
            "title": "Windows CI logs layout"
        }, 
        {
            "location": "/ci_admin_guide/Windows_CI_logs_layout/#log-structure", 
            "text": "log.txt.gz  - full Jenkins job log  unittests-logs  - logs for vRouter unit tests ran by Windows CI  TestReports/CISelfcheck  - test report for Windows CI unit tests suite  for information about more detailed logs please refer to  CI Selfcheck    TestReports/WindowsCompute  - test report for Contrail Windows integration tests suite  for information about more detailed logs please refer to  WindowsCompute", 
            "title": "Log structure"
        }, 
        {
            "location": "/ci_admin_guide/Windows_CI_logs_layout/#ciselfcheck", 
            "text": "CISelfcheck  is a set of unit tests for PowerShell scripts used in Windows CI.  TestReports/CISelfcheck  directory holds the following:   TestReports/CISelfcheck/pretty_test_report  - contains an HTML test report  example test report  tests marked  green  have passed  tests marked  red  have failed  tests marked  orange  are marked as inconclusive which means they are currently disabled in CI    TestReports/CISelfcheck/raw_NUnit  - contains NUnit XML files generated by PowerShell test runner", 
            "title": "CISelfcheck"
        }, 
        {
            "location": "/ci_admin_guide/Windows_CI_logs_layout/#windowscompute", 
            "text": "WindowsCompute  is a set of integration tests for Contrail Windows.\nThis test suite is run on a 3-node cluster consisting of 1 Contrail Controller node (all-in-one deployment) and 2 Contrail Windows compute nodes.\nA fresh cluster is provisioned before each test.\nContrail Controller is provisioned using contrail-ansible-deployer.\nIn the future Contrail Windows compute nodes will also be provisioned with contrail-ansible-deployer.  TestReports/WindowsCompute  directory holds the following:   TestReports/WindowsCompute/pretty_test_report  - contains an HTML test report  example test report  tests marked  green  have passed  tests marked  red  have failed  tests marked  orange  are marked as inconclusive which means they are currently disabled in CI    TestReports/WindowsCompute/ddriver_junit_test_logs  - contains JUnit XML files generated by Contrail Windows Docker Driver test runner  TestReports/WindowsCompute/detailed_logs  - contains logs dumped from test environment for each test; log file can contain logs from:  Contrail Windows Docker Driver running on compute nodes  Windows containers ran on compute nodes    TestReports/WindowsCompute/raw_NUnit  - contains NUnit XML files generated by PowerShell test runner", 
            "title": "WindowsCompute"
        }, 
        {
            "location": "/contrail_deployment/CAD_for_windows_design/", 
            "text": "Contrail-ansible-deployer for Windows design document\n\n\nThis is a document describing initial support of deploying Windows compute nodes with contrail-ansible-deployer.\n\n\nAnsible for Windows\n\n\n\n\nA lot of ansible modules don't work on Windows. Possible workarounds:\n\n\nUse modules with \nwin_\n prefix if they exist, however a lot of modules don't have Windows equivalent,\n\n\nMimic function of not working module with other ways possible (e.g. using \nwin_shell\n).\n\n\n\n\n\n\n\n\nOS-specific roles:\n\n\n\n\nThere are particular roles which have to be done differently for Windows.\n\n\nThus, these roles are divided into \n*_Linux\n and \n*_Win32NT\n for usage in c-a-d for Linux and c-a-d for Windows respectively.\n\n\n\n\nOrchestrators:\n\n\n\n\nOpenStack must be specified as an orchestrator, because Keystone is required by Windows Docker driver for authorization.\n\n\n\n\nPlaybooks:\n\n\n\n\nprovision_instances.yml\n - not supported at this moment.\n\n\nconfigure_instances.yml\n\n\ninstall_openstack.yml\n\n\ninstall_contrail.yml\n\n\n\n\nconfigure_instances:\n\n\n\n\nSome differences between Windows and non-Windows c-a-d workflows exist for this playbook:\n\n\nParticular packages must be present on ansible host machine used for secure communication between the ansible machine and Windows nodes. The packages are installed by this role.\n\n\nWindows compute nodes need Windows-specific dependencies and particular features turned on/off. \ninstall_software_Win32.yml\n is responsible for that. This role makes following changes on Windows nodes:\n\n\nInstalls following software:\n\n\nWindows-Containers\n\n\nNET-Framework-Features\n\n\nHyper-V\n\n\nNSSM\n\n\nDocker\n\n\nMS Visual C++ Redistributable\n\n\n\n\n\n\nSets these Windows features:\n\n\nON:\n\n\nTest-Signing Mode - reason explained below\n\n\nOFF:\n\n\nWinNAT\n\n\nWindows Firewall\n\n\n\n\n\n\n\n\n\n\nIn the future, vRouter kernel module will be digitally signed, but for now, it uses self-signed certificates.\n    To do that, Windows Test-Signing Mode must be enabled and reboot is done to apply the change.\n\n\n\n\ninstall_openstack:\n\n\n\n\nThere are no changes to this playbook.\n\n\nWhole OpenStack is installed for now, but in the future there should have to specify only the role in \nconfig/instances.yaml\n which is responsible for installing Keystone.\n\n\n\n\ninstall_contrail:\n\n\n\n\nSpecific dlls must be present in main Windows system directory (\nC:/Windows/System32\n) for the software to run.\n\n\nBecause on Windows only Contrail compute nodes are supported, there are 2 roles for Windows compute nodes:\n\n\nvrouter\n - installs vRouter kernel module, vRouter agent and utils (\ncreate_vrouter_Win32NT.yml\n):\n\n\nPull \ncontrail-windows-vrouter\n image with artifacts. Artifacts' directory structure:\n\n\nC:/Artifacts/\n\n\nagent/\n\n\ncontrail-vrouter-agent.msi\n - installs vRouter agent and creates \nContrailAgent\n service;\n\n\n\n\n\n\nvrouter/\n\n\nutils.msi\n - installs vRouter utils;\n\n\nvRouter.msi\n - installs vRouter kernel module and creates \nvRouter\n service;\n\n\nvRouter.cer\n - certificate for vRouter kernel module;\n\n\n\n\n\n\nvtest/*\n - vtest utility (needed in Windows CI);\n\n\n\n\n\n\nwin_docker_driver\n - installs Windows Docker driver (\ncreate_win_docker_driver.yml\n):\n\n\nPulls \ncontrail-windows-docker-driver\n image with artifact. Artifact's directory structure:\n\n\nC:/Artifacts/\n\n\ndocker-driver/docker-driver.msi\n - installs Docker driver and creates \nDockerDriver\n service;\n\n\n\n\n\n\nFrom high-level perspective Docker driver is very similiar to OpenStack Nova Agent, however they differ significantly at low-level, because:\n\n\nThe driver needs to have communication with Windows-specific modules,\n\n\nDocker driver communicates directly with Contrail config node.\n\n\n\n\n\n\nThe artifacts have to be built on Windows. Built artifacts are pulled from docker registry specified in \nWINDOWS_CONTAINER_REGISTRY\n in \nconfig/instances.yaml\n.\n\n\n\n\n\n\nOn Windows contrail-ansible-deployer starts components as Windows services,\n    in contrast to Linux where they reside in separate containers.\n\n\nReasons:\n\n\nIt's not possible to run Linux containers on Windows Server 2016,\n\n\nOn other Windows versions, containerization of Windows Compute components wasn't tested and implemented yet.\n\n\nThere are plans to do it in future release.\n\n\nNote: potential challenges:\n\n\nSecurity -  No priviliged containers support in Docker for Windows would result in all containers having access to the shared memory which is implementation of vRouter \nflow0\n interface;\n\n\nCommunication - communication between container and the host through Windows named pipes isn't tested yet. vRouter \npkt0\n and \nksync\n interfaces are implemented with named pipes on Windows.", 
            "title": "Contrail-ansible-deployer for Windows design document"
        }, 
        {
            "location": "/contrail_deployment/CAD_for_windows_design/#contrail-ansible-deployer-for-windows-design-document", 
            "text": "This is a document describing initial support of deploying Windows compute nodes with contrail-ansible-deployer.", 
            "title": "Contrail-ansible-deployer for Windows design document"
        }, 
        {
            "location": "/contrail_deployment/CAD_for_windows_design/#ansible-for-windows", 
            "text": "A lot of ansible modules don't work on Windows. Possible workarounds:  Use modules with  win_  prefix if they exist, however a lot of modules don't have Windows equivalent,  Mimic function of not working module with other ways possible (e.g. using  win_shell ).", 
            "title": "Ansible for Windows"
        }, 
        {
            "location": "/contrail_deployment/CAD_for_windows_design/#os-specific-roles", 
            "text": "There are particular roles which have to be done differently for Windows.  Thus, these roles are divided into  *_Linux  and  *_Win32NT  for usage in c-a-d for Linux and c-a-d for Windows respectively.", 
            "title": "OS-specific roles:"
        }, 
        {
            "location": "/contrail_deployment/CAD_for_windows_design/#orchestrators", 
            "text": "OpenStack must be specified as an orchestrator, because Keystone is required by Windows Docker driver for authorization.", 
            "title": "Orchestrators:"
        }, 
        {
            "location": "/contrail_deployment/CAD_for_windows_design/#playbooks", 
            "text": "provision_instances.yml  - not supported at this moment.  configure_instances.yml  install_openstack.yml  install_contrail.yml", 
            "title": "Playbooks:"
        }, 
        {
            "location": "/contrail_deployment/CAD_for_windows_design/#configure_instances", 
            "text": "Some differences between Windows and non-Windows c-a-d workflows exist for this playbook:  Particular packages must be present on ansible host machine used for secure communication between the ansible machine and Windows nodes. The packages are installed by this role.  Windows compute nodes need Windows-specific dependencies and particular features turned on/off.  install_software_Win32.yml  is responsible for that. This role makes following changes on Windows nodes:  Installs following software:  Windows-Containers  NET-Framework-Features  Hyper-V  NSSM  Docker  MS Visual C++ Redistributable    Sets these Windows features:  ON:  Test-Signing Mode - reason explained below  OFF:  WinNAT  Windows Firewall      In the future, vRouter kernel module will be digitally signed, but for now, it uses self-signed certificates.\n    To do that, Windows Test-Signing Mode must be enabled and reboot is done to apply the change.", 
            "title": "configure_instances:"
        }, 
        {
            "location": "/contrail_deployment/CAD_for_windows_design/#install_openstack", 
            "text": "There are no changes to this playbook.  Whole OpenStack is installed for now, but in the future there should have to specify only the role in  config/instances.yaml  which is responsible for installing Keystone.", 
            "title": "install_openstack:"
        }, 
        {
            "location": "/contrail_deployment/CAD_for_windows_design/#install_contrail", 
            "text": "Specific dlls must be present in main Windows system directory ( C:/Windows/System32 ) for the software to run.  Because on Windows only Contrail compute nodes are supported, there are 2 roles for Windows compute nodes:  vrouter  - installs vRouter kernel module, vRouter agent and utils ( create_vrouter_Win32NT.yml ):  Pull  contrail-windows-vrouter  image with artifacts. Artifacts' directory structure:  C:/Artifacts/  agent/  contrail-vrouter-agent.msi  - installs vRouter agent and creates  ContrailAgent  service;    vrouter/  utils.msi  - installs vRouter utils;  vRouter.msi  - installs vRouter kernel module and creates  vRouter  service;  vRouter.cer  - certificate for vRouter kernel module;    vtest/*  - vtest utility (needed in Windows CI);    win_docker_driver  - installs Windows Docker driver ( create_win_docker_driver.yml ):  Pulls  contrail-windows-docker-driver  image with artifact. Artifact's directory structure:  C:/Artifacts/  docker-driver/docker-driver.msi  - installs Docker driver and creates  DockerDriver  service;    From high-level perspective Docker driver is very similiar to OpenStack Nova Agent, however they differ significantly at low-level, because:  The driver needs to have communication with Windows-specific modules,  Docker driver communicates directly with Contrail config node.    The artifacts have to be built on Windows. Built artifacts are pulled from docker registry specified in  WINDOWS_CONTAINER_REGISTRY  in  config/instances.yaml .    On Windows contrail-ansible-deployer starts components as Windows services,\n    in contrast to Linux where they reside in separate containers.  Reasons:  It's not possible to run Linux containers on Windows Server 2016,  On other Windows versions, containerization of Windows Compute components wasn't tested and implemented yet.  There are plans to do it in future release.  Note: potential challenges:  Security -  No priviliged containers support in Docker for Windows would result in all containers having access to the shared memory which is implementation of vRouter  flow0  interface;  Communication - communication between container and the host through Windows named pipes isn't tested yet. vRouter  pkt0  and  ksync  interfaces are implemented with named pipes on Windows.", 
            "title": "install_contrail:"
        }, 
        {
            "location": "/contrail_deployment/Devenv_in_local_env/", 
            "text": "Devenv in CodiLime VMware lab\n\n\nThis document describes the steps required to provision a development environment in CodiLime VMware lab.\nFrom high-level perspective steps are as follows:\n\n\n\n\nProvision Windows and Linux virtual machines with suitable configuration in CodiLime VMware lab.\n\n\nConfigure Networking on Windows and Linux virtual machines using VMware console.\n\n\nRun Ansible playbook to deploy Windows Contrail compute and Contrail Controller nodes.\n\n\n\n\nPerforming steps described below correctly should result in a configured devenv consiting of:\n\n\n\n\n2 virtual machines configured as Windows Contrail compute, named \n[INITIALS]-tb1\n and \n[INITIALS]-tb2\n\n\nWindows will have vRouter dependencies installed\n\n\nvRouter/vRouter Agent/Docker Driver will not be installed\n\n\n\n\n\n\n1 virtual machine configured as Contrail Controller node named \n[INITIALS]-ctrl\n\n\nContrail Controller will be fully installed and configured\n\n\n\n\n\n\n\n\nPrerequisites\n\n\nSelect private network for devenv\n\n\nBecause of lacking infrastructure and licensing we simulate private networks based on PVLANs, using VMware's standard virtual switches.\nThis limits a communication in this private network to a single ESXi host.\nThus each virtual machine from a single devenv must be placed on a single ESXi host.\nThere are two ESXi hosts at this moment: \n10.5.88.85\n and \n10.5.88.86\n\n\nThese private networks are named \nvS-DevEnvX-Y\n.\n\nX\n is an identifier of the host.\n\nY\n is an identifier of the network.\n\nX\n can be:\n\n\n\n\nIf you chose \n10.5.88.85\n as your host, then \nX = 1\n\n\nIf you chose \n10.5.88.86\n as your host, then \nX = 2\n\n\n\n\nTo find an available network for use do the following:\n\n\n\n\nIn vSphere Web Client, navigate to \nNetworking\n tab in the navigation pane on the left\n\n\nOn the left should be a list of VMware networks\n\n\nLook for an available network from networks named \nvS-DevEnvX-Y\n:\n\n\nClick on the network \nvS-DevEnvX-Y\n \n\n\nSelect \nVMs\n tab in the center pane\n\n\nMake sure \nVirtual Machines\n option is selected\n\n\nIf the table should be empty, there are no virtual machines in this network, thus it is available\n\n\nIf there are some virtual machines, check another network\n\n\n\n\n\n\n\n\nVirtual machines addressing\n\n\nThis document supports the following addressing scheme for the devenv:\n\n\n\n\n\n\n\n\nVirtual machine\n\n\nType\n\n\nNetwork Adapter 1\n\n\nNetwork Adapter 2\n\n\n\n\n\n\n\n\n\n\n[INITIALS]-tb1\n\n\nWindows Compute\n\n\nDHCP, from 10.7.0.0/24 subnet\n\n\nStatic, 172.16.0.1/24\n\n\n\n\n\n\n[INITIALS]-tb2\n\n\nWindows Compute\n\n\nDHCP, from 10.7.0.0/24 subnet\n\n\nStatic, 172.16.0.2/24\n\n\n\n\n\n\n[INITIALS]-ctrl\n\n\nContrail Controller\n\n\nDHCP, from 10.7.0.0/24 subnet\n\n\nStatic, 172.16.0.10/24\n\n\n\n\n\n\n\n\nIn this scenario:\n\n\n\n\nNetwork Adapter 1\n corresponds to management adapter (Contrail's mgmt plane)\n\n\nOn Windows - \nEthernet1\n\n\nOn CentOS - \nens192\n\n\n\n\n\n\nNetwork Adapter 2\n corresponds to control adapter (Contrail's control and data plane)\n\n\nOn Windows - \nEthernet2\n\n\nOn CentOS - \nens224\n\n\n\n\n\n\n\n\nDevenv setup\n\n\nWindows Testbed setup\n\n\n\n\nCreate a folder for virtual machines\n\n\nSelect \nVMs and Templates\n tab in navigation pane on the left\n\n\nTraverse to \nDev\n folder\n\n\nRight clink on \nDev\n directory and select \nNew folder\n\n\nProvide folder name in the text box\n\n\nName it with your initials, e.g. \nDS\n\n\nI'll refer to this folder's name as \n[DESTINATION]\n\n\n\n\n\n\n\n\n\n\nClone virtual machine from \nCodilime/__Templates/windows/win2016core-bare\n template\n\n\nSelect \nVMs and Templates\n tab in navigation pane on the left\n\n\nTraverse to \nCodilime/__Templates/windows/\n folder\n\n\nRight click on \nwin2016core-bare\n template and select \nNew VM from This Template\n\n\nDeploy from Template\n wizard should appear\n\n\nIn \nSelect a name and folder\n step do the following:\n\n\nProvide virtual machine name in \nEnter a name for the virtual machine\n input box\n\n\nName it using your initials, appending testbed number, e.g. \nDS-tb1\n\n\nI'll refer to this named as \n[INITIALS]-tb1\n\n\n\n\n\n\nUsing \nSelect a location for the virtual machine\n window traverse to \nCodilime/Dev/[DESTINATION]\n folder\n\n\nSelect \n[DESTINATION]\n folder in the tree\n\n\nClick \nNext\n\n\n\n\n\n\nIn \nSelect a compute resource\n step do the following:\n\n\nExpand \nMSDN\n item in the resource tree viewer in the middle\n\n\nSelect one of the available hosts from \nMSDN\n cluster\n\n\nIMPORTANT\n Choice is arbitrary, however it is required that all devenv virtual machines are located on the same host\n\n\n\n\n\n\nClick \nNext\n\n\n\n\n\n\nIn \nSelect storage\n step  do the following:\n\n\nSelect \nesxiX-main\n datastore\n\n\nX\n is a host identifier\n\n\nIf you chose \n10.5.88.85\n as your host, then \nX = 1\n\n\nIf you chose \n10.5.88.86\n as your host, then \nX = 2\n\n\n\n\n\n\nSelect \nThin provision\n in \nSelect virtual dist format\n select box\n\n\nClick \nNext\n\n\n\n\n\n\nIn \nSelect clone options\n step do the following:\n\n\nCheck \nCustomize this virtual machine's hardware (Experimental)\n\n\nCheck \nPower on virtual machine after creation\n\n\nClick \nNext\n\n\n\n\n\n\nIn \nCustomize hardware\n step do the following:\n\n\nFor \nNetwork Adapter 1\n choose \nv1800_VM\n from the select menu\n\n\nFor \nNetwork Adapter 1\n check \nConnect At Power On\n\n\nFor \nNetwork Adapter 2\n choose \nvS-DevEnvX-Y\n from the select menu\n\n\nIf no such network is on the list, click \nShow more networks...\n\n\nNetwork should be chosen based on steps from \nSelect private network for devenv\n\n\n\n\n\n\nClick \nNext\n\n\n\n\n\n\nIn \nReady to complete\n step verify your settings and click \nFinish\n\n\n\n\n\n\n\n\n\n\n\n\nConfigure virtual machine\n\n\n\n\nSelect \nVMs and Templates\n tab in navigation pane on the left\n\n\nTraverse to \nCodilime/Dev/[DESTINATION]\n folder\n\n\nLeft click on \n[INITIALS]-tb1\n\n\nOn virtual machine preview click a gear icon and select \nLaunch Remote Console\n\n\nIf you do not have a VMware Remote Console installed, click \nInstall Remote Console\n and install a provided MSI\n\n\n\n\n\n\nClick \nSend Ctrl+Alt+Del to virtual machine\n button\n\n\nClick on the interior to passthrough mouse and keyboard to the virtual machine\n\n\nLogin using provided credentials\n\n\nStart PowerShell console\n\n\n\n\nChange hostname\n\n\nRename-Computer \"[INITIALS]-tb1\"\n\n\n\n\n\n\nDisable DHCP on Ethernet1 and preserve address on restart\n\n\nSet-NetIPInterface -InterfaceAlias Ethernet1 -Dhcp Disabled -PolicyStore PersistentStore\n\n\n\n\n\n\nRestart Ethernet1\n\n\nRestart-NetAdapter -InterfaceAlias Ethernet1\n\n\n\n\n\n\nSet static IP address on dataplane adater Ethernet1 (based on addressing scheme from \nVirtual machines addressing\n)\n\n\nNew-NetIPAddress -InterfaceAlias Ethernet1 -IPAddress \"172.16.0.1\" -PrefixLength 24\n\n\n\n\n\n\nVerify if IP address is configured correctly\n\n\nGet-NetIPAddress -InterfaceAlias Ethernet1\n\n\n\n\n\n\nTurn off the firewall\n\n\nSet-NetFirewallProfile -Enabled false\n\n\n\n\n\n\nRestart virtual machine\n\n\nRestart-Computer\n\n\n\n\n\n\nCheck IP address of \nEthernet0\n adapter\n\n\nGet-NetIPAddress -InterfaceAlias Ethernet0 -AddressFamily IPv4\n\n\n\n\nNote down shown address; It will be required to connect to this virtual machine using Ansible\n\n\n\n\n\n\n\n\nPress \nCtrl+Alt\n to escape VMware Remote Console\n\n\n\n\nClose VMware Remote Console\n\n\n\n\n\n\n\n\nNow repeat these steps to create a second testbed \n[INITIALS]-tb2\n\n\nController setup\n\n\n\n\nClone virtual machine from \nCodilime/__Templates/windows/centos7.4-bare\n template\n\n\nSteps are the same as in \nClone virtual machine from \nCodilime/__Templates/windows/win2016core-bare\n template\n\n\nThis time use \n[INITIALS]-ctrl\n\n\n\n\n\n\n\n\nConfigure virtual machine\n\n\n\n\nSelect \nVMs and Templates\n tab in navigation pane on the left\n\n\nTraverse to \nCodilime/Dev/[DESTINATION]\n folder\n\n\nLeft click on \n[INITIALS]-ctrl\n\n\nOn virtual machine preview click a gear icon and select \nLaunch Remote Console\n\n\nClick on the interior to passthrough mouse and keyboard to the virtual machine\n\n\nIf nothing is shown, start typing\n\n\n\n\n\n\nLogin using provided credentials\n\n\n\n\nChange hostname\n\n\n```\n\n\nhostnamectl set-hostname \"[INITIALS]-ctrl\"\n\n\n```\n\n\n\n\n\n\nConfigure static addressing on \nens224\n adapter (\nNetwork Adapter 2\n from addressing scheme)\n\n\n\n\n\n\nEdit file \n/etc/sysconfig/network-scripts/ifcfg-ens224\n to include following entries\n\n\nBOOTPROTO=static\nDEFROUTE=no\nIPADDR=172.16.0.10\nNETMASK=255.255.255.0\n\n\n\n\n\n\nRestart networking\n\n\nsystemctl restart network\n\n\n\n\n\n\nVerify address configuration\n\n\nip addr show\n\n\n\n\n\n\nCheck \nens192\n address\n\n\nip addr show ens192\n\n\n\n\nNote down shown address; It will be required to connect to this virtual machine using Ansible\n\n\n\n\n\n\n\n\n\n\n\n\nReboot virtual machine\n\n\nsystemctl reboot\n\n\n\n\n\n\n\n\n\n\nRun Ansible playbook\n\n\nThis section assumes the reader has Windows Subsystem for Linux (WSL) installed and configured.\nIf not, please refer to \nthis\n document for install and configuration details.\n\n\nAll steps below should be done from WSL.\n\n\n\n\n\n\nInstall required packages\n\n\nsudo apt install python3 python3-pip python3-virtualenv sshpass\n\n\n\n\n\n\nClone \ncontrail-windows-ci\n git repository from \nhttps://github.com/Juniper/contrail-windows-ci\n\n\nmkdir ~/dev\ncd ~/dev\ngit clone https://github.com/Juniper/contrail-windows-ci.git\n\n\n\n\n\n\nCheckout \nlocal-testenv\n branch\n\n\ncd ./contrail-windows-ci\ngit branch local-testenv origin/local-testenv\ngit checkout local-testenv\n\n\n\n\n\n\nTraverse to \ncontrail-windows-ci/ansible\n directory\n\n\ncd ./ansible\n\n\n\n\n\n\nCreate virtualenv for Python\n\n\npython3 -m virtualenv -p /usr/bin/python3 venv\n\n\n\n\n\n\nActive virtualenv\n\n\n\n\nshell prompt should change to indicate that virtualenv is activated\n\n\n\n\n$ . venv/bin/activate\n(venv) $\n\n\n\n\n\n\nInstall Python dependencies\n\n\npip install -r python-requirements.txt\n\n\n\n\n\n\nCreate a file for a provided Ansible vault key\n\n\n```\ntouch ~/ansible-vault-key\nvi ~/ansible-vault-key\n\n\nEnter vault key\n\n\n```\n\n\n\n\n\n\nCopy \ninventory.testenv\n file to \ninventory\n\n\ncp inventory.testenv inventory\n\n\n\n\n\n\nAdd Windows virtual machines to \ntestbed\n group by editing the \ninventory\n file\n\n\n[testbed]\n[INITIALS]-tb1 ansible_host=MGMT_IP_TB1\n[INITIALS]-tb2 ansible_host=MGMT_IP_TB2\n\n\n\n\nMGMT_IP_TB1\n, \nMGMT_IP_TB2\n are addresses of \nEthernet0\n adapters we checked earlier\n\n\n\n\n\n\n\n\nAdd Controller virtual machine to \ncontroller\n group\n\n\n[controller]\n[INITIALS]-ctrl ansible_host=MGMT_IP_CTRL ansible_user=PROVIDED_USER ansible_ssh_pass=PROVIDED_PASSWORD\n\n\n\n\nMGMT_IP_CTRL\n is an address of \nens192\n adapter we checked earlier\n\n\nPROVIDED_USER\n and \nPROVIDED_PASSWORD\n are provided credentials for CentOS\n\n\n\n\n\n\n\n\nTest network connection to devenv virtual machines\n\n\nping MGMT_IP_TB1\nping MGMT_IP_TB2\nping MGMT_IP_CTRL\n\n\n\n\n\n\nTest Ansible connectivity to Windows virtual machines\n\n\nansible -i inventory testbed -m win_ping\n\n\n\n\n\n\nTest Ansible connectivity to Controller virtual machine\n\n\nansible -i inventory controller -m ping\n\n\n\n\n\n\nRun \nconfigure-local-testenv.yml\n\n\nansible-playbook -i inventory configure-local-testenv.yml --skip-tags yum-repos\n\n\n\n\nPlaybook should take at least an hour to complete\n\n\n\n\n\n\n\n\nVerify if Contrail Controller is accessible\n\n\n\n\nAccess \nhttps://MGMT_IP_CTRL:8143\n\n\n\n\n\n\n\n\nTo do\n\n\n\n\nChanges in \ncontroller\n role required to provision Linux compute node.\n\n\nMerge \nlocal-testenv\n to main branch in \ncontrail-windows-ci\n repository", 
            "title": "Devenv in CodiLime VMware lab"
        }, 
        {
            "location": "/contrail_deployment/Devenv_in_local_env/#devenv-in-codilime-vmware-lab", 
            "text": "This document describes the steps required to provision a development environment in CodiLime VMware lab.\nFrom high-level perspective steps are as follows:   Provision Windows and Linux virtual machines with suitable configuration in CodiLime VMware lab.  Configure Networking on Windows and Linux virtual machines using VMware console.  Run Ansible playbook to deploy Windows Contrail compute and Contrail Controller nodes.   Performing steps described below correctly should result in a configured devenv consiting of:   2 virtual machines configured as Windows Contrail compute, named  [INITIALS]-tb1  and  [INITIALS]-tb2  Windows will have vRouter dependencies installed  vRouter/vRouter Agent/Docker Driver will not be installed    1 virtual machine configured as Contrail Controller node named  [INITIALS]-ctrl  Contrail Controller will be fully installed and configured", 
            "title": "Devenv in CodiLime VMware lab"
        }, 
        {
            "location": "/contrail_deployment/Devenv_in_local_env/#prerequisites", 
            "text": "", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/contrail_deployment/Devenv_in_local_env/#select-private-network-for-devenv", 
            "text": "Because of lacking infrastructure and licensing we simulate private networks based on PVLANs, using VMware's standard virtual switches.\nThis limits a communication in this private network to a single ESXi host.\nThus each virtual machine from a single devenv must be placed on a single ESXi host.\nThere are two ESXi hosts at this moment:  10.5.88.85  and  10.5.88.86  These private networks are named  vS-DevEnvX-Y . X  is an identifier of the host. Y  is an identifier of the network. X  can be:   If you chose  10.5.88.85  as your host, then  X = 1  If you chose  10.5.88.86  as your host, then  X = 2   To find an available network for use do the following:   In vSphere Web Client, navigate to  Networking  tab in the navigation pane on the left  On the left should be a list of VMware networks  Look for an available network from networks named  vS-DevEnvX-Y :  Click on the network  vS-DevEnvX-Y    Select  VMs  tab in the center pane  Make sure  Virtual Machines  option is selected  If the table should be empty, there are no virtual machines in this network, thus it is available  If there are some virtual machines, check another network", 
            "title": "Select private network for devenv"
        }, 
        {
            "location": "/contrail_deployment/Devenv_in_local_env/#virtual-machines-addressing", 
            "text": "This document supports the following addressing scheme for the devenv:     Virtual machine  Type  Network Adapter 1  Network Adapter 2      [INITIALS]-tb1  Windows Compute  DHCP, from 10.7.0.0/24 subnet  Static, 172.16.0.1/24    [INITIALS]-tb2  Windows Compute  DHCP, from 10.7.0.0/24 subnet  Static, 172.16.0.2/24    [INITIALS]-ctrl  Contrail Controller  DHCP, from 10.7.0.0/24 subnet  Static, 172.16.0.10/24     In this scenario:   Network Adapter 1  corresponds to management adapter (Contrail's mgmt plane)  On Windows -  Ethernet1  On CentOS -  ens192    Network Adapter 2  corresponds to control adapter (Contrail's control and data plane)  On Windows -  Ethernet2  On CentOS -  ens224", 
            "title": "Virtual machines addressing"
        }, 
        {
            "location": "/contrail_deployment/Devenv_in_local_env/#devenv-setup", 
            "text": "", 
            "title": "Devenv setup"
        }, 
        {
            "location": "/contrail_deployment/Devenv_in_local_env/#windows-testbed-setup", 
            "text": "Create a folder for virtual machines  Select  VMs and Templates  tab in navigation pane on the left  Traverse to  Dev  folder  Right clink on  Dev  directory and select  New folder  Provide folder name in the text box  Name it with your initials, e.g.  DS  I'll refer to this folder's name as  [DESTINATION]      Clone virtual machine from  Codilime/__Templates/windows/win2016core-bare  template  Select  VMs and Templates  tab in navigation pane on the left  Traverse to  Codilime/__Templates/windows/  folder  Right click on  win2016core-bare  template and select  New VM from This Template  Deploy from Template  wizard should appear  In  Select a name and folder  step do the following:  Provide virtual machine name in  Enter a name for the virtual machine  input box  Name it using your initials, appending testbed number, e.g.  DS-tb1  I'll refer to this named as  [INITIALS]-tb1    Using  Select a location for the virtual machine  window traverse to  Codilime/Dev/[DESTINATION]  folder  Select  [DESTINATION]  folder in the tree  Click  Next    In  Select a compute resource  step do the following:  Expand  MSDN  item in the resource tree viewer in the middle  Select one of the available hosts from  MSDN  cluster  IMPORTANT  Choice is arbitrary, however it is required that all devenv virtual machines are located on the same host    Click  Next    In  Select storage  step  do the following:  Select  esxiX-main  datastore  X  is a host identifier  If you chose  10.5.88.85  as your host, then  X = 1  If you chose  10.5.88.86  as your host, then  X = 2    Select  Thin provision  in  Select virtual dist format  select box  Click  Next    In  Select clone options  step do the following:  Check  Customize this virtual machine's hardware (Experimental)  Check  Power on virtual machine after creation  Click  Next    In  Customize hardware  step do the following:  For  Network Adapter 1  choose  v1800_VM  from the select menu  For  Network Adapter 1  check  Connect At Power On  For  Network Adapter 2  choose  vS-DevEnvX-Y  from the select menu  If no such network is on the list, click  Show more networks...  Network should be chosen based on steps from  Select private network for devenv    Click  Next    In  Ready to complete  step verify your settings and click  Finish       Configure virtual machine   Select  VMs and Templates  tab in navigation pane on the left  Traverse to  Codilime/Dev/[DESTINATION]  folder  Left click on  [INITIALS]-tb1  On virtual machine preview click a gear icon and select  Launch Remote Console  If you do not have a VMware Remote Console installed, click  Install Remote Console  and install a provided MSI    Click  Send Ctrl+Alt+Del to virtual machine  button  Click on the interior to passthrough mouse and keyboard to the virtual machine  Login using provided credentials  Start PowerShell console   Change hostname  Rename-Computer \"[INITIALS]-tb1\"    Disable DHCP on Ethernet1 and preserve address on restart  Set-NetIPInterface -InterfaceAlias Ethernet1 -Dhcp Disabled -PolicyStore PersistentStore    Restart Ethernet1  Restart-NetAdapter -InterfaceAlias Ethernet1    Set static IP address on dataplane adater Ethernet1 (based on addressing scheme from  Virtual machines addressing )  New-NetIPAddress -InterfaceAlias Ethernet1 -IPAddress \"172.16.0.1\" -PrefixLength 24    Verify if IP address is configured correctly  Get-NetIPAddress -InterfaceAlias Ethernet1    Turn off the firewall  Set-NetFirewallProfile -Enabled false    Restart virtual machine  Restart-Computer    Check IP address of  Ethernet0  adapter  Get-NetIPAddress -InterfaceAlias Ethernet0 -AddressFamily IPv4   Note down shown address; It will be required to connect to this virtual machine using Ansible     Press  Ctrl+Alt  to escape VMware Remote Console   Close VMware Remote Console     Now repeat these steps to create a second testbed  [INITIALS]-tb2", 
            "title": "Windows Testbed setup"
        }, 
        {
            "location": "/contrail_deployment/Devenv_in_local_env/#controller-setup", 
            "text": "Clone virtual machine from  Codilime/__Templates/windows/centos7.4-bare  template  Steps are the same as in  Clone virtual machine from  Codilime/__Templates/windows/win2016core-bare  template  This time use  [INITIALS]-ctrl     Configure virtual machine   Select  VMs and Templates  tab in navigation pane on the left  Traverse to  Codilime/Dev/[DESTINATION]  folder  Left click on  [INITIALS]-ctrl  On virtual machine preview click a gear icon and select  Launch Remote Console  Click on the interior to passthrough mouse and keyboard to the virtual machine  If nothing is shown, start typing    Login using provided credentials   Change hostname  ```", 
            "title": "Controller setup"
        }, 
        {
            "location": "/contrail_deployment/Devenv_in_local_env/#hostnamectl-set-hostname-initials-ctrl", 
            "text": "```    Configure static addressing on  ens224  adapter ( Network Adapter 2  from addressing scheme)    Edit file  /etc/sysconfig/network-scripts/ifcfg-ens224  to include following entries  BOOTPROTO=static\nDEFROUTE=no\nIPADDR=172.16.0.10\nNETMASK=255.255.255.0    Restart networking  systemctl restart network    Verify address configuration  ip addr show    Check  ens192  address  ip addr show ens192   Note down shown address; It will be required to connect to this virtual machine using Ansible       Reboot virtual machine  systemctl reboot", 
            "title": "hostnamectl set-hostname \"[INITIALS]-ctrl\""
        }, 
        {
            "location": "/contrail_deployment/Devenv_in_local_env/#run-ansible-playbook", 
            "text": "This section assumes the reader has Windows Subsystem for Linux (WSL) installed and configured.\nIf not, please refer to  this  document for install and configuration details.  All steps below should be done from WSL.    Install required packages  sudo apt install python3 python3-pip python3-virtualenv sshpass    Clone  contrail-windows-ci  git repository from  https://github.com/Juniper/contrail-windows-ci  mkdir ~/dev\ncd ~/dev\ngit clone https://github.com/Juniper/contrail-windows-ci.git    Checkout  local-testenv  branch  cd ./contrail-windows-ci\ngit branch local-testenv origin/local-testenv\ngit checkout local-testenv    Traverse to  contrail-windows-ci/ansible  directory  cd ./ansible    Create virtualenv for Python  python3 -m virtualenv -p /usr/bin/python3 venv    Active virtualenv   shell prompt should change to indicate that virtualenv is activated   $ . venv/bin/activate\n(venv) $    Install Python dependencies  pip install -r python-requirements.txt    Create a file for a provided Ansible vault key  ```\ntouch ~/ansible-vault-key\nvi ~/ansible-vault-key", 
            "title": "Run Ansible playbook"
        }, 
        {
            "location": "/contrail_deployment/Devenv_in_local_env/#enter-vault-key", 
            "text": "```    Copy  inventory.testenv  file to  inventory  cp inventory.testenv inventory    Add Windows virtual machines to  testbed  group by editing the  inventory  file  [testbed]\n[INITIALS]-tb1 ansible_host=MGMT_IP_TB1\n[INITIALS]-tb2 ansible_host=MGMT_IP_TB2   MGMT_IP_TB1 ,  MGMT_IP_TB2  are addresses of  Ethernet0  adapters we checked earlier     Add Controller virtual machine to  controller  group  [controller]\n[INITIALS]-ctrl ansible_host=MGMT_IP_CTRL ansible_user=PROVIDED_USER ansible_ssh_pass=PROVIDED_PASSWORD   MGMT_IP_CTRL  is an address of  ens192  adapter we checked earlier  PROVIDED_USER  and  PROVIDED_PASSWORD  are provided credentials for CentOS     Test network connection to devenv virtual machines  ping MGMT_IP_TB1\nping MGMT_IP_TB2\nping MGMT_IP_CTRL    Test Ansible connectivity to Windows virtual machines  ansible -i inventory testbed -m win_ping    Test Ansible connectivity to Controller virtual machine  ansible -i inventory controller -m ping    Run  configure-local-testenv.yml  ansible-playbook -i inventory configure-local-testenv.yml --skip-tags yum-repos   Playbook should take at least an hour to complete     Verify if Contrail Controller is accessible   Access  https://MGMT_IP_CTRL:8143", 
            "title": "Enter vault key"
        }, 
        {
            "location": "/contrail_deployment/Devenv_in_local_env/#to-do", 
            "text": "Changes in  controller  role required to provision Linux compute node.  Merge  local-testenv  to main branch in  contrail-windows-ci  repository", 
            "title": "To do"
        }, 
        {
            "location": "/contrail_deployment/contrail_deployment_in_ci/", 
            "text": "A few words about contrail deployment in Windows CI\n\n\nEnvironment\n\n\n\n\nPlaybooks\n\n\n\n\nprovision_instances.yml - creates server instances (e.g. on AWS) not used in Contrail Windows CI\n\n\nconfigure_instances.yml - installs dependencies installs and starts docker\n\n\ninstall_contrail.yml - installs Contrail and installs orchestrator (optional)\n\n\n\n\nConfiguration\n\n\n\n\nOrchestrators:\n\n\nNone\n\n\nOpenStack\n\n\nVCenter\n\n\nContrail version and kolla version:\n\n\nocata-master-91 and ocata\n\n\nDocker registry\n\n\nci-repo.englab.juniper.net or \nopencontrailnigthly\n\n\nInterfaces\n\n\nExample \nconfig\n\n\nWindows-CI \nconfig\n\n\n\n\nKolla\n\n\n\n\nKolla == production-ready containers and deployment tools for operating OpenStack\n\n\ncontrail-ansible-deployer internally clones Juniper/contrail-kolla-ansible (branch: contrail/ocata)\n\n\nBugs which may affect us can be introduced in contrail-ansible-deployer as well as in contrail-kolla-ansible\n\n\n\n\nBase image\n\n\n\n\nCentOS 7.4 (kernel \n= 3.10.0-693.17.1)\n\n\nTwo interfaces:\n\n\nControl Plane\n\n\nData Plane\n\n\nIP address is required on every interface - can be static\n\n\nMore information:\n\n\nhttps://github.com/Juniper/contrail-ansible-deployer#prerequisites\n\n\n\n\nWindows-CI\n\n\n\n\nTroubleshooting\n\n\n\n\nTry to run playbook locally from laptop\n\n\nTry to look for similar issue on contra-cloud.slack channel: contrail-ansible\n\n\nCheck recent commits in contrail-ansible-deployer and contrail-kolla-deployer\n\n\nAsk our devops team\n\n\nAsk on slack channel\n\n\n\n\nZuul V3\n\n\n\n\nController image from pull request\n\n\nCorrect version of contrail-ansible-deployer\n\n\nCorrect version of contrail-kolla-deployer\n\n\nEnable voting on contrail-ansible-deployer and contrail-kolla-deployer", 
            "title": "A few words about contrail deployment in Windows CI"
        }, 
        {
            "location": "/contrail_deployment/contrail_deployment_in_ci/#a-few-words-about-contrail-deployment-in-windows-ci", 
            "text": "", 
            "title": "A few words about contrail deployment in Windows CI"
        }, 
        {
            "location": "/contrail_deployment/contrail_deployment_in_ci/#environment", 
            "text": "", 
            "title": "Environment"
        }, 
        {
            "location": "/contrail_deployment/contrail_deployment_in_ci/#playbooks", 
            "text": "provision_instances.yml - creates server instances (e.g. on AWS) not used in Contrail Windows CI  configure_instances.yml - installs dependencies installs and starts docker  install_contrail.yml - installs Contrail and installs orchestrator (optional)", 
            "title": "Playbooks"
        }, 
        {
            "location": "/contrail_deployment/contrail_deployment_in_ci/#configuration", 
            "text": "Orchestrators:  None  OpenStack  VCenter  Contrail version and kolla version:  ocata-master-91 and ocata  Docker registry  ci-repo.englab.juniper.net or  opencontrailnigthly  Interfaces  Example  config  Windows-CI  config", 
            "title": "Configuration"
        }, 
        {
            "location": "/contrail_deployment/contrail_deployment_in_ci/#kolla", 
            "text": "Kolla == production-ready containers and deployment tools for operating OpenStack  contrail-ansible-deployer internally clones Juniper/contrail-kolla-ansible (branch: contrail/ocata)  Bugs which may affect us can be introduced in contrail-ansible-deployer as well as in contrail-kolla-ansible", 
            "title": "Kolla"
        }, 
        {
            "location": "/contrail_deployment/contrail_deployment_in_ci/#base-image", 
            "text": "CentOS 7.4 (kernel  = 3.10.0-693.17.1)  Two interfaces:  Control Plane  Data Plane  IP address is required on every interface - can be static  More information:  https://github.com/Juniper/contrail-ansible-deployer#prerequisites", 
            "title": "Base image"
        }, 
        {
            "location": "/contrail_deployment/contrail_deployment_in_ci/#windows-ci", 
            "text": "", 
            "title": "Windows-CI"
        }, 
        {
            "location": "/contrail_deployment/contrail_deployment_in_ci/#troubleshooting", 
            "text": "Try to run playbook locally from laptop  Try to look for similar issue on contra-cloud.slack channel: contrail-ansible  Check recent commits in contrail-ansible-deployer and contrail-kolla-deployer  Ask our devops team  Ask on slack channel", 
            "title": "Troubleshooting"
        }, 
        {
            "location": "/contrail_deployment/contrail_deployment_in_ci/#zuul-v3", 
            "text": "Controller image from pull request  Correct version of contrail-ansible-deployer  Correct version of contrail-kolla-deployer  Enable voting on contrail-ansible-deployer and contrail-kolla-deployer", 
            "title": "Zuul V3"
        }, 
        {
            "location": "/contrail_deployment/contrail_deployment_manual/", 
            "text": "How to deploy Contrail with Windows compute nodes\n\n\nPrerequisites\n\n\n\n\nMachine with CentOS 7.4 installed\n\n\nMachine(s) with Windows Server 2016 and updates installed\n\n\nMachine for running Ansible playbooks (Linux or Windows with WSL)\n\n\n\n\nSteps\n\n\n\n\nOn each of the Windows hosts:\n    \npowershell\n    Invoke-WebRequest https://raw.githubusercontent.com/ansible/ansible/devel/examples/scripts/ConfigureRemotingForAnsible.ps1 -OutFile ConfigureRemotingForAnsible.ps1\n    .\\ConfigureRemotingForAnsible.ps1 -DisableBasicAuth -EnableCredSSP -ForceNewSSLCert -SkipNetworkProfileCheck\n\n\nOn the Ansible machine:\n    \nbash\n    git clone git@github.com:Juniper/contrail-ansible-deployer.git\n    cd contrail-ansible-deployer\n    vim config/instances.yaml\n\n\nRefer to examples:\n\n\nconfig/instances.yaml.bms_win_example\n if you have already deployed controller and you only want windows compute nodes\n\n\nconfig/instances.yaml.bms_win_full_example\n if you want to deploy controller and windows compute nodes together\n\n\n\n\n\n\nMore precise explanation of the required fields:\n\n\nFor Windows computes use \nbms_win\n dict instead of regular \nbms\n\n\nRoles supported on Windows are \nvrouter\n and \nwin_docker_driver\n - specify them\n\n\nSet \nWINDOWS_PHYSICAL_INTERFACE\n to dataplane interface alias\n\n\nSpecify \nWINDOWS_ENABLE_TEST_SIGNING\n if you want to install untrusted artifacts (this is needed at this moment)\n\n\nSet \nWINDOWS_DEBUG_DLLS_PATH\n to path on Ansible machine containing MSVC 2015 debug dlls, specifically: \nmsvcp140d.dll\n, \nucrtbased.dll\n and \nvcruntime140d.dll\n\n\n\n\n\n\nTo deploy a controller for windows compute nodes:\n\n\nConfigure as it would be a controller node for a linux ecosystem\n\n\nFor now docker driver on windows needs keystone set up on controller:\n\n\nAdd openstack-* roles to the controller node and set \nCLOUD_ORCHESTRATOR\n to \nopenstack\n\n\nFill keystone credentials and kolla config. Check \nconfig/instances.yaml.openstack_example\n\n\n\n\n\n\n\n\n\n\nProceed with running Ansible playbooks:\n    \nbash\n    sudo -H ansible-playbook -e orchestrator=openstack -i inventory/ playbooks/configure_instances.yml\n    sudo -H ansible-playbook -i inventory playbooks/install_openstack.yml\n    sudo -H ansible-playbook -e orchestrator=openstack -i inventory/ playbooks/install_contrail.yml\n\n\n\n\n\n\n\n\nTesting the new setup\n\n\nRefer to \nthis document", 
            "title": "How to deploy Contrail with Windows compute nodes"
        }, 
        {
            "location": "/contrail_deployment/contrail_deployment_manual/#how-to-deploy-contrail-with-windows-compute-nodes", 
            "text": "", 
            "title": "How to deploy Contrail with Windows compute nodes"
        }, 
        {
            "location": "/contrail_deployment/contrail_deployment_manual/#prerequisites", 
            "text": "Machine with CentOS 7.4 installed  Machine(s) with Windows Server 2016 and updates installed  Machine for running Ansible playbooks (Linux or Windows with WSL)", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/contrail_deployment/contrail_deployment_manual/#steps", 
            "text": "On each of the Windows hosts:\n     powershell\n    Invoke-WebRequest https://raw.githubusercontent.com/ansible/ansible/devel/examples/scripts/ConfigureRemotingForAnsible.ps1 -OutFile ConfigureRemotingForAnsible.ps1\n    .\\ConfigureRemotingForAnsible.ps1 -DisableBasicAuth -EnableCredSSP -ForceNewSSLCert -SkipNetworkProfileCheck  On the Ansible machine:\n     bash\n    git clone git@github.com:Juniper/contrail-ansible-deployer.git\n    cd contrail-ansible-deployer\n    vim config/instances.yaml  Refer to examples:  config/instances.yaml.bms_win_example  if you have already deployed controller and you only want windows compute nodes  config/instances.yaml.bms_win_full_example  if you want to deploy controller and windows compute nodes together    More precise explanation of the required fields:  For Windows computes use  bms_win  dict instead of regular  bms  Roles supported on Windows are  vrouter  and  win_docker_driver  - specify them  Set  WINDOWS_PHYSICAL_INTERFACE  to dataplane interface alias  Specify  WINDOWS_ENABLE_TEST_SIGNING  if you want to install untrusted artifacts (this is needed at this moment)  Set  WINDOWS_DEBUG_DLLS_PATH  to path on Ansible machine containing MSVC 2015 debug dlls, specifically:  msvcp140d.dll ,  ucrtbased.dll  and  vcruntime140d.dll    To deploy a controller for windows compute nodes:  Configure as it would be a controller node for a linux ecosystem  For now docker driver on windows needs keystone set up on controller:  Add openstack-* roles to the controller node and set  CLOUD_ORCHESTRATOR  to  openstack  Fill keystone credentials and kolla config. Check  config/instances.yaml.openstack_example      Proceed with running Ansible playbooks:\n     bash\n    sudo -H ansible-playbook -e orchestrator=openstack -i inventory/ playbooks/configure_instances.yml\n    sudo -H ansible-playbook -i inventory playbooks/install_openstack.yml\n    sudo -H ansible-playbook -e orchestrator=openstack -i inventory/ playbooks/install_contrail.yml", 
            "title": "Steps"
        }, 
        {
            "location": "/contrail_deployment/contrail_deployment_manual/#testing-the-new-setup", 
            "text": "Refer to  this document", 
            "title": "Testing the new setup"
        }, 
        {
            "location": "/developer_guide/HNS_error_bestiary/", 
            "text": "HNS errors bestiary\n\n\nHNS error messages ofter aren't descriptive enough, and the cleanup is quite hard.\nThe following document describes common cleanup techniques and error interpretations.\n\n\nHNS Decontamination procedures\n\n\nBefore working with HNS, one must realize the risks.\nThere are many ways in which HNS can stop working and affect other parts of the system.\nFor a developer, it is important to be able to recognize those, which can be recovered \nfrom. Below are some techniques which may help.\n\n\nAlways try cleaning using the lowest level procedure. The higher the level, the more \npotential damage the cleanup can cause.\n\n\nA word of caution\n\n\nNever\n develop HNS on a baremetal or own laptop.\n\n\nLevel Alpha decontamination procedure\n\n\n\n\nFirst, stop the docker service.\n\n\n\n\nStop-Service docker\n\n\n\n\n\n\nRemove all virtual switches, container networks and NAT\n\n\n\n\nGet-ContainerNetwork | Remove-ContainerNetwork\nGet-VMSwitch | Remove-VMSwitch\nGet-NetNAT | Remove-NetNAT\n\n\n\n\n\n\nRestart HNS and docker services.\n\n\n\n\nRestart-Service hns\nRestart-Service docker\n\n\n\n\nLevel Beta decontamination procedure\n\n\n\n\nRemove all container networks. Always do this because some vmswitches may not be removed properly if you just execute the next command.\n\n\n\n\nGet-ContainerNetwork | Remove-ContainerNetwork\n\n\n\n\n\n\nManually remove \nHNS.data\n file. \nnet\n command sometimes works better at restarting HNS than PowerShell's \nRestart-Service\n.\n\n\n\n\n\n\nWarning: this is not recommended by Microsoft, but they do it in their official cleanup script, so I guess it's not that bad.\n\n\n\n\nnet stop hns; \ndel C:\\programdata\\Microsoft\\Windows\\HNS\\HNS.data; \nnet start hns;\n\n\n\n\nLevel Gamma decontamination procedure\n\n\nUse official Microsoft script to cleanup. It will also cleanup some registry entries and do much more.\n\n\nhttps://github.com/MicrosoftDocs/Virtualization-Documentation/tree/live/windows-server-container-tools/CleanupContainerHostNetworking\n\n\nLevel [REDACTED] decontamination procedure\n\n\nEverything is lost. All we can do wipe all devices and hope that the contamination won't spread to other hosts.\n\n\n\n\nWarning: this might BSOD.\n\n\n\n\n// perform as single command because you will lose connectivity during netcfg -D\nnetcfg -D; Restart-Computer -force\n\n\n\n\nHNS error specimen list\n\n\nThe following chapter contains a list of HNS errors encountered throughout development, along with reproduction methods and natural habitat.\n\n\n1. HNS Unspecified Error\n\n\nNatural habitat:\n\n\n\n\n\n\nWhen attempting to create a transparent HNS network\n\n\n\n\nwhen creation of another network or VMSwitch is already in progress\n  (eg. we've just started Docker service and it tries to create the NAT network).\n\n\n\n\nWe can work around this bug by retrying after a few seconds.\n\n\n\n\n\n\nReproduction:\n\n\nTry to create multiple HNS networks in a loop simultaneously with multiple processes.\nWe suspect that this error occurs during a high load.\n\n\n2. HNS Invalid Parameter\n\n\nNatural habitat:\n\n\nTODO\n\n\nReproduction:\n\n\nTODO\n\n\n3. HNS Element not found\n\n\nNatural habitat:\n\n\n\n\n[Hypothesis] When attempting to create a transparent docker network\n\n\nwhen no other transparent docker network exists and\n\n\nEthernet adapter to be used by the transparent networks has no IP address or it's invalid.\n\n\n\n\n\n\n\n\nReproduction:\n\n\nSee https://github.com/Microsoft/hcsshim/issues/95\n\n\n4. HNS failed with error : {Object Exists} An attempt was made to create an object and the object name already exists\n\n\nNatural habitat:\n\n\nThis error probably happens when docker tries to create NAT network, but HNS left over some trash after last NAT network.\n\n\nCleanup everything as explained in the Decontamination Procedures chapter. If the problem still persists, just create a random NAT network:\n\n\nNew-ContainerNetwork foo\n\n\n\n\nReproduction:\n\n\nTODO\n\n\n5. Container creation fails with error: CreateContainer: failure in a Windows system call\n\n\nNatural habitat:\n\n\nThis error happens occasionally when Docker tries to create a container.\n\n\nThe container is actually created (it enters CREATED state), but can not be run (Docker doesn't start it automatically and manual start fails). Such a faulty container can be removed. Then one may try to create container again - this is expected to succeed (no case has been observed, when second attempt failed).\n\n\nReproduction:\n\n\nThere's no obvious correlation with any special circumstances. On a VM that's not heavily loaded, it is expected that hundreds of tries might be needed to reproduce this error. Creating and removing containers in a loop is enough.", 
            "title": "HNS errors bestiary"
        }, 
        {
            "location": "/developer_guide/HNS_error_bestiary/#hns-errors-bestiary", 
            "text": "HNS error messages ofter aren't descriptive enough, and the cleanup is quite hard.\nThe following document describes common cleanup techniques and error interpretations.", 
            "title": "HNS errors bestiary"
        }, 
        {
            "location": "/developer_guide/HNS_error_bestiary/#hns-decontamination-procedures", 
            "text": "Before working with HNS, one must realize the risks.\nThere are many ways in which HNS can stop working and affect other parts of the system.\nFor a developer, it is important to be able to recognize those, which can be recovered \nfrom. Below are some techniques which may help.  Always try cleaning using the lowest level procedure. The higher the level, the more \npotential damage the cleanup can cause.", 
            "title": "HNS Decontamination procedures"
        }, 
        {
            "location": "/developer_guide/HNS_error_bestiary/#a-word-of-caution", 
            "text": "Never  develop HNS on a baremetal or own laptop.", 
            "title": "A word of caution"
        }, 
        {
            "location": "/developer_guide/HNS_error_bestiary/#level-alpha-decontamination-procedure", 
            "text": "First, stop the docker service.   Stop-Service docker   Remove all virtual switches, container networks and NAT   Get-ContainerNetwork | Remove-ContainerNetwork\nGet-VMSwitch | Remove-VMSwitch\nGet-NetNAT | Remove-NetNAT   Restart HNS and docker services.   Restart-Service hns\nRestart-Service docker", 
            "title": "Level Alpha decontamination procedure"
        }, 
        {
            "location": "/developer_guide/HNS_error_bestiary/#level-beta-decontamination-procedure", 
            "text": "Remove all container networks. Always do this because some vmswitches may not be removed properly if you just execute the next command.   Get-ContainerNetwork | Remove-ContainerNetwork   Manually remove  HNS.data  file.  net  command sometimes works better at restarting HNS than PowerShell's  Restart-Service .    Warning: this is not recommended by Microsoft, but they do it in their official cleanup script, so I guess it's not that bad.   net stop hns; \ndel C:\\programdata\\Microsoft\\Windows\\HNS\\HNS.data; \nnet start hns;", 
            "title": "Level Beta decontamination procedure"
        }, 
        {
            "location": "/developer_guide/HNS_error_bestiary/#level-gamma-decontamination-procedure", 
            "text": "Use official Microsoft script to cleanup. It will also cleanup some registry entries and do much more.  https://github.com/MicrosoftDocs/Virtualization-Documentation/tree/live/windows-server-container-tools/CleanupContainerHostNetworking", 
            "title": "Level Gamma decontamination procedure"
        }, 
        {
            "location": "/developer_guide/HNS_error_bestiary/#level-redacted-decontamination-procedure", 
            "text": "Everything is lost. All we can do wipe all devices and hope that the contamination won't spread to other hosts.   Warning: this might BSOD.   // perform as single command because you will lose connectivity during netcfg -D\nnetcfg -D; Restart-Computer -force", 
            "title": "Level [REDACTED] decontamination procedure"
        }, 
        {
            "location": "/developer_guide/HNS_error_bestiary/#hns-error-specimen-list", 
            "text": "The following chapter contains a list of HNS errors encountered throughout development, along with reproduction methods and natural habitat.", 
            "title": "HNS error specimen list"
        }, 
        {
            "location": "/developer_guide/HNS_error_bestiary/#1-hns-unspecified-error", 
            "text": "", 
            "title": "1. HNS Unspecified Error"
        }, 
        {
            "location": "/developer_guide/HNS_error_bestiary/#natural-habitat", 
            "text": "When attempting to create a transparent HNS network   when creation of another network or VMSwitch is already in progress\n  (eg. we've just started Docker service and it tries to create the NAT network).   We can work around this bug by retrying after a few seconds.", 
            "title": "Natural habitat:"
        }, 
        {
            "location": "/developer_guide/HNS_error_bestiary/#reproduction", 
            "text": "Try to create multiple HNS networks in a loop simultaneously with multiple processes.\nWe suspect that this error occurs during a high load.", 
            "title": "Reproduction:"
        }, 
        {
            "location": "/developer_guide/HNS_error_bestiary/#2-hns-invalid-parameter", 
            "text": "", 
            "title": "2. HNS Invalid Parameter"
        }, 
        {
            "location": "/developer_guide/HNS_error_bestiary/#natural-habitat_1", 
            "text": "TODO", 
            "title": "Natural habitat:"
        }, 
        {
            "location": "/developer_guide/HNS_error_bestiary/#reproduction_1", 
            "text": "TODO", 
            "title": "Reproduction:"
        }, 
        {
            "location": "/developer_guide/HNS_error_bestiary/#3-hns-element-not-found", 
            "text": "", 
            "title": "3. HNS Element not found"
        }, 
        {
            "location": "/developer_guide/HNS_error_bestiary/#natural-habitat_2", 
            "text": "[Hypothesis] When attempting to create a transparent docker network  when no other transparent docker network exists and  Ethernet adapter to be used by the transparent networks has no IP address or it's invalid.", 
            "title": "Natural habitat:"
        }, 
        {
            "location": "/developer_guide/HNS_error_bestiary/#reproduction_2", 
            "text": "See https://github.com/Microsoft/hcsshim/issues/95", 
            "title": "Reproduction:"
        }, 
        {
            "location": "/developer_guide/HNS_error_bestiary/#4-hns-failed-with-error-object-exists-an-attempt-was-made-to-create-an-object-and-the-object-name-already-exists", 
            "text": "", 
            "title": "4. HNS failed with error : {Object Exists} An attempt was made to create an object and the object name already exists"
        }, 
        {
            "location": "/developer_guide/HNS_error_bestiary/#natural-habitat_3", 
            "text": "This error probably happens when docker tries to create NAT network, but HNS left over some trash after last NAT network.  Cleanup everything as explained in the Decontamination Procedures chapter. If the problem still persists, just create a random NAT network:  New-ContainerNetwork foo", 
            "title": "Natural habitat:"
        }, 
        {
            "location": "/developer_guide/HNS_error_bestiary/#reproduction_3", 
            "text": "TODO", 
            "title": "Reproduction:"
        }, 
        {
            "location": "/developer_guide/HNS_error_bestiary/#5-container-creation-fails-with-error-createcontainer-failure-in-a-windows-system-call", 
            "text": "", 
            "title": "5. Container creation fails with error: CreateContainer: failure in a Windows system call"
        }, 
        {
            "location": "/developer_guide/HNS_error_bestiary/#natural-habitat_4", 
            "text": "This error happens occasionally when Docker tries to create a container.  The container is actually created (it enters CREATED state), but can not be run (Docker doesn't start it automatically and manual start fails). Such a faulty container can be removed. Then one may try to create container again - this is expected to succeed (no case has been observed, when second attempt failed).", 
            "title": "Natural habitat:"
        }, 
        {
            "location": "/developer_guide/HNS_error_bestiary/#reproduction_4", 
            "text": "There's no obvious correlation with any special circumstances. On a VM that's not heavily loaded, it is expected that hundreds of tries might be needed to reproduce this error. Creating and removing containers in a loop is enough.", 
            "title": "Reproduction:"
        }, 
        {
            "location": "/developer_guide/building/Dockerized_builder_using_ansible/", 
            "text": "Dockerized builder using ansible\n\n\nThis document describes what \nwould\n be needed create\na builder docker image (for building vrouter agent and extension)\nby reusing \nansible\n scripts that are already used in\nthe \ncontrail-windows-ci\n repository to deploy builder machines.\n\n\nThis method of building is not yet supported, but this document\ndescribes the steps needed to make it work. It also tries to\nlist the potential obstacles.\n\n\nThis document describes the process from the developer's point\nof view, but it could be adapted to running in CI too.\n\n\nWindows\n\n\nCurrently, this process is supported only for Windows machines.\n\n\nDocker\n\n\nInstall \nDocker Community Edition for Windows\n. Note that:\n\n\n\n\nYou need to sign in / create to accout to download the installer.\n\n\nYou need to choose windows containers (not linux) when installing.\n\n\n\n\nTODO: Is there another installation method?\n\n\nAnsible\n\n\nTo execute ansible on Windows Docker containers you need to have:\n\n\n\n\nA container image prepared for connecting via ansible to.\n\n\nA (virtual) linux machine to run ansible.\n\n\n(Optionally) To troubleshoot remote connection to container,\n   you need to have the container IP in \ntrusted hosts lists\n.\n   Depending on the configuration of your system, this may require\n   changing the policy in your domain.\n\n\n\n\nPreparing container for ansible\n\n\nNotes:\n\n\n\n\nPerhaps there exists a ready container prepared for ansible?\n\n\nThere exists an example script \nConfigureRemotingForAnsible.ps1\n.\n  Unfortunately, the \nmicrosoft/windowsservercore\n image does not have the firewall\n  service used by the script, so steps in this scripts need to be run manually.\n  This wasn't finished, and it needs some additional investigation.\n\n\n\n\nMachine for ansible\n\n\nThe preferred way to use ansible on windows machine is to use\nthe Linux Subsystem on Windows. You can install \nUbuntu 18.04\nfrom the Windows Store\n. (18.04 version is\nneeded for the ansible 2.4 to be available. On the older\nversion you can probably install ansible using pip)\n\n\n\n\nNote: Alternatively, you can install ansible and its dependencies also\nusing \nthis python-requirements.txt file\n\nfrom \ncontrail-windows-ci\n repository with\n\npip install -r python-requirements.txt\n\n\n\n\nInstall ansible \n= 2.4 using \napt\n:\n\n\nsudo apt install ansible\n\n\n\n\nThere are additional python packages that need to be installed\nto let ansible connect to Windows machine:\n\n\nsudo apt install python-requests\nsudo apt install python-pip\npip install pywinrm\npip install requests-credssp\n\n\n\n\nThe \ngroup_vars/windows\n also need to be configured for windows:\nNOTE: \ngroup_vars\n, inventory and other scripts would be\nalready prepared, so this step won't be needed for the end user.\n\n\nansible_port: 5986\nansible_connection: winrm\nansible_winrm_transport: credssp\nansible_winrm_server_cert_validation: ignore\nansible_winrm_operation_timeout_sec: 60\nansible_winrm_read_timeout_sec: 120\nansible_user: Administrator@localhost\nansible_password: 'hunter2'\n\n\n\n\nContainer components\n\n\nBuild tools and .NET\n\n\nInstallation of .NET on \nmicrosoft/windowsservercore\n is not supported,\nbut microsoft provides the \nmicrosoft/dotnet-framework\n image.\n\n\nTODO: Do we need to install multiple .NET versions? If not,\nthis image would be sufficient.\n\n\nThe build tools can be installed on the container following\nthe instruction on \nthis page in microsoft docs\n.\nUnfortunately, I haven't managed to finish it, due to some\nHCS errors. Perhaps it was related to insufficient disk\nspace (before running the Dockerfile I had 50GB free space).\nPerhaps there is some other way of installing build tools,\nwhich I haven't tested.\n\n\nTroubleshooting\n\n\nWhen encountering weird errors in \ndocker build\n\n(the errors may come from HCS), it's probable that they're caused by:\n\n\n\n\nInsufficient available RAM/swap on the host machine.\n\n\nInsufficient RAM assigned to container.\n\n\nInsufficient disk space on the host machine.\n\n\nInsufficient disk space assigned to container.", 
            "title": "Dockerized builder using ansible"
        }, 
        {
            "location": "/developer_guide/building/Dockerized_builder_using_ansible/#dockerized-builder-using-ansible", 
            "text": "This document describes what  would  be needed create\na builder docker image (for building vrouter agent and extension)\nby reusing  ansible  scripts that are already used in\nthe  contrail-windows-ci  repository to deploy builder machines.  This method of building is not yet supported, but this document\ndescribes the steps needed to make it work. It also tries to\nlist the potential obstacles.  This document describes the process from the developer's point\nof view, but it could be adapted to running in CI too.", 
            "title": "Dockerized builder using ansible"
        }, 
        {
            "location": "/developer_guide/building/Dockerized_builder_using_ansible/#windows", 
            "text": "Currently, this process is supported only for Windows machines.", 
            "title": "Windows"
        }, 
        {
            "location": "/developer_guide/building/Dockerized_builder_using_ansible/#docker", 
            "text": "Install  Docker Community Edition for Windows . Note that:   You need to sign in / create to accout to download the installer.  You need to choose windows containers (not linux) when installing.   TODO: Is there another installation method?", 
            "title": "Docker"
        }, 
        {
            "location": "/developer_guide/building/Dockerized_builder_using_ansible/#ansible", 
            "text": "To execute ansible on Windows Docker containers you need to have:   A container image prepared for connecting via ansible to.  A (virtual) linux machine to run ansible.  (Optionally) To troubleshoot remote connection to container,\n   you need to have the container IP in  trusted hosts lists .\n   Depending on the configuration of your system, this may require\n   changing the policy in your domain.", 
            "title": "Ansible"
        }, 
        {
            "location": "/developer_guide/building/Dockerized_builder_using_ansible/#preparing-container-for-ansible", 
            "text": "Notes:   Perhaps there exists a ready container prepared for ansible?  There exists an example script  ConfigureRemotingForAnsible.ps1 .\n  Unfortunately, the  microsoft/windowsservercore  image does not have the firewall\n  service used by the script, so steps in this scripts need to be run manually.\n  This wasn't finished, and it needs some additional investigation.", 
            "title": "Preparing container for ansible"
        }, 
        {
            "location": "/developer_guide/building/Dockerized_builder_using_ansible/#machine-for-ansible", 
            "text": "The preferred way to use ansible on windows machine is to use\nthe Linux Subsystem on Windows. You can install  Ubuntu 18.04\nfrom the Windows Store . (18.04 version is\nneeded for the ansible 2.4 to be available. On the older\nversion you can probably install ansible using pip)   Note: Alternatively, you can install ansible and its dependencies also\nusing  this python-requirements.txt file \nfrom  contrail-windows-ci  repository with pip install -r python-requirements.txt   Install ansible  = 2.4 using  apt :  sudo apt install ansible  There are additional python packages that need to be installed\nto let ansible connect to Windows machine:  sudo apt install python-requests\nsudo apt install python-pip\npip install pywinrm\npip install requests-credssp  The  group_vars/windows  also need to be configured for windows:\nNOTE:  group_vars , inventory and other scripts would be\nalready prepared, so this step won't be needed for the end user.  ansible_port: 5986\nansible_connection: winrm\nansible_winrm_transport: credssp\nansible_winrm_server_cert_validation: ignore\nansible_winrm_operation_timeout_sec: 60\nansible_winrm_read_timeout_sec: 120\nansible_user: Administrator@localhost\nansible_password: 'hunter2'", 
            "title": "Machine for ansible"
        }, 
        {
            "location": "/developer_guide/building/Dockerized_builder_using_ansible/#container-components", 
            "text": "", 
            "title": "Container components"
        }, 
        {
            "location": "/developer_guide/building/Dockerized_builder_using_ansible/#build-tools-and-net", 
            "text": "Installation of .NET on  microsoft/windowsservercore  is not supported,\nbut microsoft provides the  microsoft/dotnet-framework  image.  TODO: Do we need to install multiple .NET versions? If not,\nthis image would be sufficient.  The build tools can be installed on the container following\nthe instruction on  this page in microsoft docs .\nUnfortunately, I haven't managed to finish it, due to some\nHCS errors. Perhaps it was related to insufficient disk\nspace (before running the Dockerfile I had 50GB free space).\nPerhaps there is some other way of installing build tools,\nwhich I haven't tested.", 
            "title": "Build tools and .NET"
        }, 
        {
            "location": "/developer_guide/building/Dockerized_builder_using_ansible/#troubleshooting", 
            "text": "When encountering weird errors in  docker build \n(the errors may come from HCS), it's probable that they're caused by:   Insufficient available RAM/swap on the host machine.  Insufficient RAM assigned to container.  Insufficient disk space on the host machine.  Insufficient disk space assigned to container.", 
            "title": "Troubleshooting"
        }, 
        {
            "location": "/style_guides/C/", 
            "text": "We try to maintain the same coding style as the rest of vRouter wherever we can, but still use conventions used by Windows Driver Kit wherever applicable (regarding variable names, P convention instead of * for specifying pointers to native Windows structures).\n\n\nBelow every rule, there is an example illustrating it with a code sample.\n\n\nGeneral\n\n\nStyling\n\n\n\n\nUse spaces, not tabs.\n\n\nOne indentation level is 4 spaces\n\n\nEvery line should end with a newline character ('\\n')\n\n\nNewlines commited into the remote repository should be Unix-like (newline character), avoid Windows-like newlines (carriage return; newline)\n\n\nAvoid magic numbers, unless there is a very good reason.\n\n\nUse define directives instead.\n\n\nStatements with blocks (if, while, for, do...) should have the opening bracket in the same line\n\n\nStatements with only one line instead of a whole block can skip brackets if it improves readability\n\n\nStatements if/else should not skip brackets unless it can be done for both if, else and all eventual else ifs\n\n\n\n\nif (condition)\n    do_something();\nelse\n    do_nothing();\n\n\nif (condition) {\n    do_something();\n} else {\n    i+= 1;\n    do_nothing();\n}\n\n\ndo {\n    ASSERT(FALSE);\n    nobody_uses_this_feature();\n} while(condition);\n\n\n\n\n\n\nIf, for, while and similar statements should not be too long. This means, for example creating variables for parts of statement being checked\n\n\n\n\nBOOLEAN usable = (value \n= MIN_VALUE \n value \n= MAX_VALUE);\nBOOLEAN safe = (state == STABLE);\nif (usable \n safe)\n    do_something();\n\n\n\n\nFiles\n\n\n\n\nFiles in 'windows' directory are free to modify and do not have to be platform portable (they have to be Windows-compatible)\n\n\nOther files need to be platform independant. This may be done in #ifdef manner, but more elegant solutions are preferable\n\n\nFor example callbacks\n\n\nFunction names should be:\nvr_filename.c or vr_filename.h\n** Please note vrouter_mod.c is an exception to this\n\n\nFiles should be in directory:\n\n windows for Windows source files (.c extension)\n\n include for header files (.h extension)\n*\n dp-core for multiplatform source files (we don't forsee creating any)\n\nFor header files, use guards to make sure there is no problem if they are included two or more times.\n\n\n\n\n#ifndef __VR_DEFS_H__\n#define __VR_DEFS_H__\n// Some code\n#endif\n\n\n\n\n\n\nUse #ifdef, not #pragma once for guards\n\n\n\n\nNaming\n\n\nFunctions\n\n\n\n\nName functions in snake case, names starting with vr_ or win_. Please note functions in vrouter_mod.c ATM don't follow this rule because of SxLibrary requirements. This will be changed when we get rid of it.\nvr_reinject_packet\n\n\nvr_ variant should be used for multiplatform functions\n\n\nwin_ variant should be used for Windows-specific functions.\n\n\nWhen defining function, place the type in one line and the rest of it in another line.\n\n\n\n\nunsigned int\nvr_reinject_packet(struct vr_packet *pkt, struct vr_forwarding_md *fmd)\n\n\n\n\n\n\nWhen declaring functions, use the extern keyword and write everything in one line.\n\n\n\n\nextern unsigned int vr_some_function();\n\n\n\n\n\n\nIt is preferential but not required to declare which parameters of a function are constant.\n\n\n\n\nint\nvr_function(const char* address)\n\n\n\n\nVariables\n\n\n\n\nWe avoid constant literal variables. Instead, one should avoid magic numbers using #define directive. Const structs are OK.\n\n\n\n\n#define MAX_SIZE 32\n\n\n\n\n\n\nGlobal variables can be used whenever necessary, especially when the function being implemented will be called by some pre-existing dp-core code without supplying required arguments. However, it is preferential to avoid them. Static variables are lesser evil. Please check if a global variable can be replaced with a static one\n\n\nVariables should be named using snake case.\n\n\n\n\nLARGE_INTEGER current_gmt_time;\n\n\n\n\n\n\nIt is preferential but not required to declare constant variables as such\n\n\n\n\nconst unsigned int num_of_tries = input + 5;\n\n\n\n\n\n\nIf a value is known to be always positive, one should use unsigned variable. Same for bitfields.\n\n\n\n\nunsigned int i = 5;\n\n\n\n\n\n\nVariables should be initialized before reading them.\n\n\n\n\nTypes\n\n\n\n\nExisting types should be used as-is\n\n\n\n\nPNET_BUFFER_LIST nbl;\n\n\n\n\n\n\nNew types and types declared by us should use snake case and begin with vr_.\n\n\n\n\nstruct vr_struct {};\n\n\n\n\n\n\nDo not typedef function, use struct as a part of the name\n\n\n\n\nstruct vr_router* router = vrouter_get(0);\n\n\n\n\nComments\n\n\n\n\nBoth block and line comments are allowed\n\n\n\n\nint i = pointer-\nfield; // Pointer cannot be null\n\n/* TODO: Fix this */\n\n\n\n\n\n\nNon-obvious logic should be commented\n\n\nDo not leave commented out code unless there is a very good reason.\n\n\nPlatform-specific code\n\n\nUse #ifdefs to specify code that should be compiled on only selected platforms\n\n\n\n\n#ifdef __KERNEL__\n\n\n\n\n\n\nPrefer #ifdef to #if defined unless more specific statement checking if required.\n\n\n\n\n#if defined(_WINDOWS) || defined(__FreeBSD__)\n\n\n\n\n\n\nDo not use compiler or linker specific features in code if they would break compatibility with other tools. For example, most #pragma statements.\n\n\n\n\n#ifdef _WINDOWS\n#pragma warning(disable : 4018) // Disable warning about signed/unsigned mismatch\n#endif\n\n\n\n\n\n\nTry not to modify existing dp-core code if possible, otherwise modify it:\n\n\nPreferential: Creating additional callbacks (can be NULL or empty functions on Linux/FreeBSD)\n\n\n\n\nUsing #ifdefs\n\n\n\n\nTry not to use os, compiler or linker specific features if possible\n\n\nIf there is already some code implemented using them, consider reimplementing a native Linux function using Windows libraries. This function should only be compiled on Windows as Linux-like systems will provide their own implementation. Either of these two options will work\n\n\nWrite it in a file that will only be included on Windows\n\n\n\n\n#ifdef the implementation\nIf the body of a function will be completely different on Linux and Windows, write one function header and footer and #ifdef the rest of the body\nvoid print(const char* a)\n{\n#ifdef _WINDOWS\nDbgPrint(a);\n#else\nprintf(a);\n#endif\n}\n\n\n\n\n\n\nWindows-specific code should be properely ASSERTed. Use this function or ASSERTMSG to check your assumptions\n\n\n\n\nOther points\n\n\n\n\nIf a possible data-race condition arises, find the smallest critical section and wrap it into a mutex, semaphore or a RW lock.\n\n\nFunctions should return:\n\n In Windows-specific code: NDIS_STATUS and all effects via pointers unless the function cannot fail\n\n In multiplatform code: regular return value if functions succeeds and -errno if the function fails", 
            "title": "C"
        }, 
        {
            "location": "/style_guides/C/#general", 
            "text": "", 
            "title": "General"
        }, 
        {
            "location": "/style_guides/C/#styling", 
            "text": "Use spaces, not tabs.  One indentation level is 4 spaces  Every line should end with a newline character ('\\n')  Newlines commited into the remote repository should be Unix-like (newline character), avoid Windows-like newlines (carriage return; newline)  Avoid magic numbers, unless there is a very good reason.  Use define directives instead.  Statements with blocks (if, while, for, do...) should have the opening bracket in the same line  Statements with only one line instead of a whole block can skip brackets if it improves readability  Statements if/else should not skip brackets unless it can be done for both if, else and all eventual else ifs   if (condition)\n    do_something();\nelse\n    do_nothing();\n\n\nif (condition) {\n    do_something();\n} else {\n    i+= 1;\n    do_nothing();\n}\n\n\ndo {\n    ASSERT(FALSE);\n    nobody_uses_this_feature();\n} while(condition);   If, for, while and similar statements should not be too long. This means, for example creating variables for parts of statement being checked   BOOLEAN usable = (value  = MIN_VALUE   value  = MAX_VALUE);\nBOOLEAN safe = (state == STABLE);\nif (usable   safe)\n    do_something();", 
            "title": "Styling"
        }, 
        {
            "location": "/style_guides/C/#files", 
            "text": "Files in 'windows' directory are free to modify and do not have to be platform portable (they have to be Windows-compatible)  Other files need to be platform independant. This may be done in #ifdef manner, but more elegant solutions are preferable  For example callbacks  Function names should be:\nvr_filename.c or vr_filename.h\n** Please note vrouter_mod.c is an exception to this  Files should be in directory:  windows for Windows source files (.c extension)  include for header files (.h extension)\n*  dp-core for multiplatform source files (we don't forsee creating any) For header files, use guards to make sure there is no problem if they are included two or more times.   #ifndef __VR_DEFS_H__\n#define __VR_DEFS_H__\n// Some code\n#endif   Use #ifdef, not #pragma once for guards", 
            "title": "Files"
        }, 
        {
            "location": "/style_guides/C/#naming", 
            "text": "", 
            "title": "Naming"
        }, 
        {
            "location": "/style_guides/C/#functions", 
            "text": "Name functions in snake case, names starting with vr_ or win_. Please note functions in vrouter_mod.c ATM don't follow this rule because of SxLibrary requirements. This will be changed when we get rid of it.\nvr_reinject_packet  vr_ variant should be used for multiplatform functions  win_ variant should be used for Windows-specific functions.  When defining function, place the type in one line and the rest of it in another line.   unsigned int\nvr_reinject_packet(struct vr_packet *pkt, struct vr_forwarding_md *fmd)   When declaring functions, use the extern keyword and write everything in one line.   extern unsigned int vr_some_function();   It is preferential but not required to declare which parameters of a function are constant.   int\nvr_function(const char* address)", 
            "title": "Functions"
        }, 
        {
            "location": "/style_guides/C/#variables", 
            "text": "We avoid constant literal variables. Instead, one should avoid magic numbers using #define directive. Const structs are OK.   #define MAX_SIZE 32   Global variables can be used whenever necessary, especially when the function being implemented will be called by some pre-existing dp-core code without supplying required arguments. However, it is preferential to avoid them. Static variables are lesser evil. Please check if a global variable can be replaced with a static one  Variables should be named using snake case.   LARGE_INTEGER current_gmt_time;   It is preferential but not required to declare constant variables as such   const unsigned int num_of_tries = input + 5;   If a value is known to be always positive, one should use unsigned variable. Same for bitfields.   unsigned int i = 5;   Variables should be initialized before reading them.", 
            "title": "Variables"
        }, 
        {
            "location": "/style_guides/C/#types", 
            "text": "Existing types should be used as-is   PNET_BUFFER_LIST nbl;   New types and types declared by us should use snake case and begin with vr_.   struct vr_struct {};   Do not typedef function, use struct as a part of the name   struct vr_router* router = vrouter_get(0);", 
            "title": "Types"
        }, 
        {
            "location": "/style_guides/C/#comments", 
            "text": "Both block and line comments are allowed   int i = pointer- field; // Pointer cannot be null\n\n/* TODO: Fix this */   Non-obvious logic should be commented  Do not leave commented out code unless there is a very good reason.  Platform-specific code  Use #ifdefs to specify code that should be compiled on only selected platforms   #ifdef __KERNEL__   Prefer #ifdef to #if defined unless more specific statement checking if required.   #if defined(_WINDOWS) || defined(__FreeBSD__)   Do not use compiler or linker specific features in code if they would break compatibility with other tools. For example, most #pragma statements.   #ifdef _WINDOWS\n#pragma warning(disable : 4018) // Disable warning about signed/unsigned mismatch\n#endif   Try not to modify existing dp-core code if possible, otherwise modify it:  Preferential: Creating additional callbacks (can be NULL or empty functions on Linux/FreeBSD)", 
            "title": "Comments"
        }, 
        {
            "location": "/style_guides/C/#using-ifdefs", 
            "text": "Try not to use os, compiler or linker specific features if possible  If there is already some code implemented using them, consider reimplementing a native Linux function using Windows libraries. This function should only be compiled on Windows as Linux-like systems will provide their own implementation. Either of these two options will work  Write it in a file that will only be included on Windows   #ifdef the implementation\nIf the body of a function will be completely different on Linux and Windows, write one function header and footer and #ifdef the rest of the body\nvoid print(const char* a)\n{\n#ifdef _WINDOWS\nDbgPrint(a);\n#else\nprintf(a);\n#endif\n}   Windows-specific code should be properely ASSERTed. Use this function or ASSERTMSG to check your assumptions", 
            "title": "Using #ifdefs"
        }, 
        {
            "location": "/style_guides/C/#other-points", 
            "text": "If a possible data-race condition arises, find the smallest critical section and wrap it into a mutex, semaphore or a RW lock.  Functions should return:  In Windows-specific code: NDIS_STATUS and all effects via pointers unless the function cannot fail  In multiplatform code: regular return value if functions succeeds and -errno if the function fails", 
            "title": "Other points"
        }, 
        {
            "location": "/style_guides/Cpp/", 
            "text": "We aim to use a subset of all C++ features. This includes most features present in C++ up to C++11.\n\n\nGeneral\n\n\nPlatform-specific code\n\n\n\n\nUse #ifdef to differentiate Windows and non-Windows code\n\n\nTry to implement Windows-specific code on the lowest possible abstraction layer so that the higher ones can be platform agnostic.\n\n\nWhen providing alternate body of a function, use one function header/signature but two implementations in its body, encased in #ifdef guards.\n\n\n\n\nPractices\n\n\n\n\nUse class declarations in header files if the definition is not necessary.\n\n\n\n\nclass BgpAttr;\n\n\n\n\n\n\nIn header files, use #ifdef guards to prevent any issues arising from including the same header twice\n\n\n\n\n#ifndef ctrlplane_ksync_sock_h\n#define ctrlplane_ksync_sock_h\n// Some code\n#endif\nUse of many for-each variants instead of iterating over an index when accessing an array\nfor(auto \nelement : array) {\n  //do something...\n}\nauto itr = strvec.begin();\nwhile(itr != strvec.end()) {\n  //do something...\n  ++itr;\n}\n\n\n\n\nStyle\n\n\nFiles\n\n\n\n\nUse .cc files for C++ source code\n\n\nUse .h files for C++ header code\n\n\n\n\nNaming\n\n\nFiles\n\n\n\n\nFiles should be named in snake case.\n\n\n\n\niroute_aggregator.h\nClasses\nClasses should be named in camel case with the first letter capital\nclass IRouteAggregator {\n};\n\n\n\n\nFunctions/Methods\n\n\n\n\nMethods and functions should be named in upper camel case\n\n\n\n\nclass IRouteAggregator {\n    void Initialize();\n};\n\n\n\n\nVariables/Fields\n\n\n\n\nVariables and fields should be named according to the snake case convention\n** Note: Private fields often have a single underscore appended, as this makes creating a getter/setter more intuitive\n\n\n\n\nDesign\n\n\nParadigm\n\n\n\n\nAgent is designed with traditional object-oriented model of C++ application, therefore we should try to maintain that style.\n\n\n\n\nAgent does not use exceptions, instead relying on old-school return code error handling.\nint func()\n{\n    if (error)\n        return -ENOMEM;\n}\n\n\n\n\n\n\nAgent does use virtual and abstract (pure virtual) classes.\n\n\n\n\nclass Class {\npublic:\nvirtual void Func() = 0;\nvirtual void Func2();\nvoid Func3();\nprivate:\n};\n\n\n\n\n\n\nAgent uses proper visibility settings, ie. uses public, private and protected specifiers in class definitions.\n\n\nTemplating is used and encouraged.", 
            "title": "Cpp"
        }, 
        {
            "location": "/style_guides/Cpp/#general", 
            "text": "", 
            "title": "General"
        }, 
        {
            "location": "/style_guides/Cpp/#platform-specific-code", 
            "text": "Use #ifdef to differentiate Windows and non-Windows code  Try to implement Windows-specific code on the lowest possible abstraction layer so that the higher ones can be platform agnostic.  When providing alternate body of a function, use one function header/signature but two implementations in its body, encased in #ifdef guards.", 
            "title": "Platform-specific code"
        }, 
        {
            "location": "/style_guides/Cpp/#practices", 
            "text": "Use class declarations in header files if the definition is not necessary.   class BgpAttr;   In header files, use #ifdef guards to prevent any issues arising from including the same header twice   #ifndef ctrlplane_ksync_sock_h\n#define ctrlplane_ksync_sock_h\n// Some code\n#endif\nUse of many for-each variants instead of iterating over an index when accessing an array\nfor(auto  element : array) {\n  //do something...\n}\nauto itr = strvec.begin();\nwhile(itr != strvec.end()) {\n  //do something...\n  ++itr;\n}", 
            "title": "Practices"
        }, 
        {
            "location": "/style_guides/Cpp/#style", 
            "text": "", 
            "title": "Style"
        }, 
        {
            "location": "/style_guides/Cpp/#files", 
            "text": "Use .cc files for C++ source code  Use .h files for C++ header code", 
            "title": "Files"
        }, 
        {
            "location": "/style_guides/Cpp/#naming", 
            "text": "", 
            "title": "Naming"
        }, 
        {
            "location": "/style_guides/Cpp/#files_1", 
            "text": "Files should be named in snake case.   iroute_aggregator.h\nClasses\nClasses should be named in camel case with the first letter capital\nclass IRouteAggregator {\n};", 
            "title": "Files"
        }, 
        {
            "location": "/style_guides/Cpp/#functionsmethods", 
            "text": "Methods and functions should be named in upper camel case   class IRouteAggregator {\n    void Initialize();\n};", 
            "title": "Functions/Methods"
        }, 
        {
            "location": "/style_guides/Cpp/#variablesfields", 
            "text": "Variables and fields should be named according to the snake case convention\n** Note: Private fields often have a single underscore appended, as this makes creating a getter/setter more intuitive", 
            "title": "Variables/Fields"
        }, 
        {
            "location": "/style_guides/Cpp/#design", 
            "text": "", 
            "title": "Design"
        }, 
        {
            "location": "/style_guides/Cpp/#paradigm", 
            "text": "Agent is designed with traditional object-oriented model of C++ application, therefore we should try to maintain that style.   Agent does not use exceptions, instead relying on old-school return code error handling.\nint func()\n{\n    if (error)\n        return -ENOMEM;\n}   Agent does use virtual and abstract (pure virtual) classes.   class Class {\npublic:\nvirtual void Func() = 0;\nvirtual void Func2();\nvoid Func3();\nprivate:\n};   Agent uses proper visibility settings, ie. uses public, private and protected specifiers in class definitions.  Templating is used and encouraged.", 
            "title": "Paradigm"
        }, 
        {
            "location": "/style_guides/Go/", 
            "text": "Since docker driver written in Go is the only module of OpenContrail we've written completely by ourselves, we can introduce the best programming practices here, without regard for compatibility with legacy code.\n\n\nFor issues not discussed here, look at Official Code Review Comments.\n\n\nGeneral\n\n\nCoding style\n\n\n\n\nAvoid nesting if possible\n\n\nTry to exit from the function early in a case of an error\n\n\nUse tabs for indentations.\n\n\nOne tab is one indentation level.\n\n\nCode should be as compliant with gometalinter.\n\n\nIf a line would be longer than 100 characters (cound tab as 4 spaces for this), divide it into a few lines, so every line is shorter than said length.\n\n\n\n\nErrors\n\n\n\n\nHandle errors as soon as possible\n\n\nFunctions that are not sure to be successful should return error as one of returned values\n\n\n\n\nfunc SomeFunc(param string) (string, error) {\n\n\n\n\nOther guidelines\n\n\n\n\nTry to use libraries to do required functionality, instead of calling PowerShell commands or otherwise using external features if possible\n\n\nOften it's not possible\n\n\nUse log module to provide logging capability\n\n\nRemember to use different message levels correctly, ie. error for errors, info for regular info, debug for info which should usually be hidden, etc.\n\n\n\n\nlog.Infoln(\nDeleting HNS network\n, hnsID)\n\n\n\n\nDesign\n\n\nModules\n\n\n\n\nKeep modules independent if possible\n\n\nOtherwise, make the dependency explicit\n\n\nDependencies can often be avoided by using interfaces\n\n\nWhen importing not inbuilt modules (eg. from github), keep a copy of them in vendor directory. Use govendor to upkeep it.\n\n\n\n\nFunctions\n\n\n\n\nFunctions should only accept as arguments what they really need, ie. io.Writer instead of *os.File\n\n\nDependencies can be avoided by using interfaces\n\n\nAvoid functions concurrent by default. By exposing synchronous API one can easily choose how to call the function, the other way around doesn't work.\n\n\nUse chans to communicate with goroutines\n\n\nUnexported functions should be named in camel case with first letter lowercase\n\n\nFunctions should be named in camel case with first letter capital for exported functions\n\n\n\n\nfunc CreateHNSNetwork(configuration *hcsshim.HNSNetwork) (string, error) {\n\n\n\n\nTesting\n\n\n\n\nAll features should be tested if possible\n\n\nFeatures implemented in file.go should be tested in file_test.go\n\n\nUsed frameworks should be ginkgo and gomega", 
            "title": "Go"
        }, 
        {
            "location": "/style_guides/Go/#general", 
            "text": "", 
            "title": "General"
        }, 
        {
            "location": "/style_guides/Go/#coding-style", 
            "text": "Avoid nesting if possible  Try to exit from the function early in a case of an error  Use tabs for indentations.  One tab is one indentation level.  Code should be as compliant with gometalinter.  If a line would be longer than 100 characters (cound tab as 4 spaces for this), divide it into a few lines, so every line is shorter than said length.", 
            "title": "Coding style"
        }, 
        {
            "location": "/style_guides/Go/#errors", 
            "text": "Handle errors as soon as possible  Functions that are not sure to be successful should return error as one of returned values   func SomeFunc(param string) (string, error) {", 
            "title": "Errors"
        }, 
        {
            "location": "/style_guides/Go/#other-guidelines", 
            "text": "Try to use libraries to do required functionality, instead of calling PowerShell commands or otherwise using external features if possible  Often it's not possible  Use log module to provide logging capability  Remember to use different message levels correctly, ie. error for errors, info for regular info, debug for info which should usually be hidden, etc.   log.Infoln( Deleting HNS network , hnsID)", 
            "title": "Other guidelines"
        }, 
        {
            "location": "/style_guides/Go/#design", 
            "text": "", 
            "title": "Design"
        }, 
        {
            "location": "/style_guides/Go/#modules", 
            "text": "Keep modules independent if possible  Otherwise, make the dependency explicit  Dependencies can often be avoided by using interfaces  When importing not inbuilt modules (eg. from github), keep a copy of them in vendor directory. Use govendor to upkeep it.", 
            "title": "Modules"
        }, 
        {
            "location": "/style_guides/Go/#functions", 
            "text": "Functions should only accept as arguments what they really need, ie. io.Writer instead of *os.File  Dependencies can be avoided by using interfaces  Avoid functions concurrent by default. By exposing synchronous API one can easily choose how to call the function, the other way around doesn't work.  Use chans to communicate with goroutines  Unexported functions should be named in camel case with first letter lowercase  Functions should be named in camel case with first letter capital for exported functions   func CreateHNSNetwork(configuration *hcsshim.HNSNetwork) (string, error) {", 
            "title": "Functions"
        }, 
        {
            "location": "/style_guides/Go/#testing", 
            "text": "All features should be tested if possible  Features implemented in file.go should be tested in file_test.go  Used frameworks should be ginkgo and gomega", 
            "title": "Testing"
        }, 
        {
            "location": "/updating_documentation/How_to_update_this_documentation/", 
            "text": "How to update this documentation\n\n\nHow does it work\n\n\nDocumentation is stored on master branch of Juniper/contrail-windows-docs repository. It is stored in Markdown (.md) format. Documentation is served as HTML on github.io. HTML pages are generated by MkDocs tool. HTML pages need to be redeployed manually after modification.\n\n\nDocumentation update procedure (procedure for developer)\n\n\n\n\nOpen a pull request to master branch in repository Juniper/contrail-windows-docs with your changes.\n\n\nOnce PR is accepted, an administrator is going to deploy new modifications to gh-pages branch.\n\n\n\n\nHow to preview modifications (procedure for developer)\n\n\n\n\nInstall MkDocs (see below).\n\n\nApply required modifications to the documentation.\n\n\nInvoke \nmkdocs serve\n in main repository directory. It's going to start local HTTP server with documentation preview. Follow instructions displayed by mkdocs to see the outcome.\n\n\n\n\nHow to install MkDocs (procedure for administrator and developer)\n\n\nMkDocs requires Python (both Python 2 and Python 3 are supported) and PIP. Follow instructions on \nhttp://www.mkdocs.org/#installation\n. Basically \npip install mkdocs\n should work. It may also be useful to use virtualenv to keep mkdocs and its dependencies in one isolated place.\n\n\nHow to deploy modifications (procedure for administrator)\n\n\nAfter master branch of Juniper/contrail-windows-docs repository is updated, github.io pages need to be redeployed manually. To do this, follow these steps:\n\n\n\n\nInstall MkDocs.\n\n\nCheckout master branch of repository Juniper/contrail-windows-docs.\n\n\nInvoke \nmkdocs gh-deploy\n in main repository directory and follow displayed instructions. Additional details are described in \nhttp://www.mkdocs.org/user-guide/deploying-your-docs/\n.", 
            "title": "How to update this documentation"
        }, 
        {
            "location": "/updating_documentation/How_to_update_this_documentation/#how-to-update-this-documentation", 
            "text": "", 
            "title": "How to update this documentation"
        }, 
        {
            "location": "/updating_documentation/How_to_update_this_documentation/#how-does-it-work", 
            "text": "Documentation is stored on master branch of Juniper/contrail-windows-docs repository. It is stored in Markdown (.md) format. Documentation is served as HTML on github.io. HTML pages are generated by MkDocs tool. HTML pages need to be redeployed manually after modification.", 
            "title": "How does it work"
        }, 
        {
            "location": "/updating_documentation/How_to_update_this_documentation/#documentation-update-procedure-procedure-for-developer", 
            "text": "Open a pull request to master branch in repository Juniper/contrail-windows-docs with your changes.  Once PR is accepted, an administrator is going to deploy new modifications to gh-pages branch.", 
            "title": "Documentation update procedure (procedure for developer)"
        }, 
        {
            "location": "/updating_documentation/How_to_update_this_documentation/#how-to-preview-modifications-procedure-for-developer", 
            "text": "Install MkDocs (see below).  Apply required modifications to the documentation.  Invoke  mkdocs serve  in main repository directory. It's going to start local HTTP server with documentation preview. Follow instructions displayed by mkdocs to see the outcome.", 
            "title": "How to preview modifications (procedure for developer)"
        }, 
        {
            "location": "/updating_documentation/How_to_update_this_documentation/#how-to-install-mkdocs-procedure-for-administrator-and-developer", 
            "text": "MkDocs requires Python (both Python 2 and Python 3 are supported) and PIP. Follow instructions on  http://www.mkdocs.org/#installation . Basically  pip install mkdocs  should work. It may also be useful to use virtualenv to keep mkdocs and its dependencies in one isolated place.", 
            "title": "How to install MkDocs (procedure for administrator and developer)"
        }, 
        {
            "location": "/updating_documentation/How_to_update_this_documentation/#how-to-deploy-modifications-procedure-for-administrator", 
            "text": "After master branch of Juniper/contrail-windows-docs repository is updated, github.io pages need to be redeployed manually. To do this, follow these steps:   Install MkDocs.  Checkout master branch of repository Juniper/contrail-windows-docs.  Invoke  mkdocs gh-deploy  in main repository directory and follow displayed instructions. Additional details are described in  http://www.mkdocs.org/user-guide/deploying-your-docs/ .", 
            "title": "How to deploy modifications (procedure for administrator)"
        }, 
        {
            "location": "/user_guide/caveats/", 
            "text": "Known issues\n\n\n\n\nOn Windows Server 2016, NAT network on compute node must be disabled in order for IP packet\nfragmentation to work.\n\n\nMultiple endpoints per container are not supported.\n\n\nMultiple IPs/networks per container endpoint are not supported.", 
            "title": "Known issues"
        }, 
        {
            "location": "/user_guide/caveats/#known-issues", 
            "text": "On Windows Server 2016, NAT network on compute node must be disabled in order for IP packet\nfragmentation to work.  Multiple endpoints per container are not supported.  Multiple IPs/networks per container endpoint are not supported.", 
            "title": "Known issues"
        }, 
        {
            "location": "/user_guide/connection_scenarios/", 
            "text": "Connection scenarios\n\n\nCreating virtual network(s), containers and interactive sessions\n\n\n\n\n\n\nVia Contrail WebUI:\n\n\n\n\nCreate a policy with the following rule:\n    \npass protocol any network any ports any \n network any ports any\n\n\nCreate virtual network called \ntestnet1\n with single subnet: \n10.0.1.0/24\n. Leave every field as default except IP pool (set it to be from \n10.0.1.10\n to \n10.0.1.100\n) and network policy (set it to policy created in first step).\n\n\nCreate virtual network called \ntestnet2\n with single subnet: \n10.0.2.0/24\n. Leave every field as default except IP pool (set it to be from \n10.0.2.10\n to \n10.0.2.100\n) and network policy (set it to policy created in first step).\n\n\n\n\n\n\n\n\nOn both Windows VMs:\n\n\n\n\nVerify if Agent service is running:\n    \npowershell\n    Get-Service ContrailAgent\n\n    State should be Running\n\n\nVerify if VMSwitch Extension is running and enabled:\n    \npowershell\n    Get-VMSwitch -Name \"Layered*\" | Get-VMSwitchExtension\n\n    Find block of data describing to \nvRouter\n\n    Both Enabled and Running fields should be True\n\n\nVerify if Docker Driver service is running:\n    \npowershell\n    Get-Service contrail-docker-driver\n\n    State should be Running\n\n\nCreate docker network \ntestnet1\n on the 1st Windows compute code:\n    \npowershell\n    docker network create --ipam-driver windows -d Contrail --opt tenant=admin --opt network=testnet1 testnet1\n\n\nRun a docker container in the prepared network on the 1st Windows compute node:\n    \npowershell\n    docker run -id --network testnet1 --name container1 microsoft/windowsservercore powershell\n\n\nInspect container1 on the 1st Windows compute node. Find the part that describes endpoints and verify that it has a correct IP address assigned (from the previously configured IP pool).\n    \npowershell\n    docker inspect container1\n\n\nRun interactive shell inside the container1 on the 1st Windows compute node:\n    \npowershell\n    docker exec -it container1 powershell\n\n\nCreate docker network \u2018testnet2\u2019 on the 2nd Windows compute node:\n    \npowershell\n    docker network create --ipam-driver windows -d Contrail --opt tenant=admin --opt network=testnet2 testnet2\n\n\nRun a docker container in the prepared network (\u2018testnet2\u2019) on the 2nd Windows compute node:\n    \npowershell\n    docker run -id --network testnet2 --name container2 microsoft/windowsservercore powershell\n\n\nInspect container2 on 2nd Windows compute node. Find the part that describes endpoints and verify that it has a correct IP address assigned (from the previously configured IP pool).\n    \npowershell\n    docker inspect container2\n\n\nRun a 2nd docker container in the prepared network (\u2018testnet2\u2019) on the 2nd Windows compute node:\n    \npowershell\n    docker run -id --network testnet2 --name container3 microsoft/iis powershell\n\n\nInspect container3 on the 2nd Windows compute node. Find the part that describes endpoints and verify that it has a correct IP address assigned (from the previously configured IP pool).\n    \npowershell\n    docker inspect container3\n\n\nRun interactive shell inside the container2 on the 2nd Windows compute node:\n    \npowershell\n    docker exec -it container2 powershell\n\n\nRun interactive shell inside the container3 on the 2nd Windows compute node. Remember that Powershell console that has been used in previous step is now running a session in container2. To operate on a host system, open new Powershell console.\n    \npowershell\n    docker exec -it container3 powershell\n\n\n\n\n\n\n\n\nICMP\n\n\n\n\nVerify that ICMP protocol works within single virtual network.\n\n    To do that invoke the following Powershell command in interactive session of container2 (on the 2nd Windows compute Node):\n    \npowershell\n    ping \nIP address of container3\n\n\nVerify that ICMP protocol works across virtual network boundaries.\n\n    To do that invoke the following Powershell command in interactive session of container1 (on the 1st Windows Compute Node):\n    \npowershell\n    ping \nIP address of container2\n\n\n\n\nTCP\n\n\n\n\nInvoke the following commands in container3\u2019s interactive session (on the 2nd Windows Compute Node):\n    \npowershell\n    cd \\inetpub\\wwwroot\n    echo \"We come in peace.\" \n index.html\n\n\nInvoke the following command in container1\u2019s interactive session (on the 1st Windows Compute Node). Verify that you receive server response with webpage prepared in previous step.\n    \npowershell\n    Invoke-WebRequest http://\ncontainer3's IP address\n -UseBasicParsing\n\n\n\n\nUDP\n\n\n\n\nSet up UDP listener on container3 (on the 2nd Windows Compute Node).\n\n    To do that invoke the following commands in container3\u2019s interactive console. Last instruction is a blocking call waiting for incoming packet.\n    \npowershell\n    $RemoteIPEndpoint = New-Object System.Net.IPEndPoint([IPAddress]::Any, 0)\n    $UDPRcvSocket = New-Object System.Net.Sockets.UdpClient 3000\n    $Payload = $UDPRcvSocket.Receive([ref]$RemoteIPEndpoint)\n\n\nSend UDP packet from container1 (on the 1st Windows Compute Node).\n\n    To do that invoke the following commands in container1\u2019s interactive console.\n    \npowershell\n    $ListenerAddress = New-Object System.Net.IPEndPoint([IPAddress]::Parse(\"10.0.2.4\"), 3000)\n    $UDPSenderSocket = New-Object System.Net.Sockets.UdpClient 0\n    $Payload = [Text.Encoding]::UTF8.GetBytes(\"We come in peace.\")\n    $UDPSenderSocket.Send($Payload, $Payload.Length, $ListenerAddress)\n\n\nWhen packet is received by container3, blocking call to \nSystem.Net.Sockets.UdpClient.Receive\n (in container3 on the 2nd Windows Compute Node) is going to return. If that doesn\u2019t happen immediately, repeat invocation of \n$UDPSenderSocket.Send\n (in container1 on the 1st Windows Compute Node).\n\n\nInvoke the following command in container1\u2019s interactive session to display payload of received UDP packet to verify that it\u2019s consisted with sent message:\n    \npowershell\n    [Text.Encoding]::UTF8.GetString($Payload)\n\n\n\n\nNetwork policies and security groups\n\n\n\n\nCreate additional policies and security groups (through WebUI) with various settings and demonstrate that they affect communication between networks. One may follow ICMP/TCP/UDP test procedures with various policies and security groups.\n\n\nRevert network configuration (policies and security groups) to its original state not to affect other experiments.\n\n\n\n\nECMP\n\n\n\n\nCreate Floating IP Pool through Contrail WebUI assigned to \ntestnet2\n.\n\n\nCreate Floating IP through WebUI. Assign it to previously created pool and set specific IP that is a correct address in \ntestnet2\n (e.g. 10.0.2.101).\n\n\nAssign Floating IP created in second step to port (\nPorts\n tab) that belongs to container2.\n\n\nAssign Floating IP created in second step to port (\nPorts\n tab) that belongs to container3.\n\n\nSet up an UDP listener on container2. Enter the following Powershell instructions in interactive console of container2 on the 2nd Windows Compute Node:\n    \npowershell\n    $Sock = [System.Net.Sockets.UdpClient]::new(5000)\n    $Endpoint = [System.Net.IPEndPoint]::new([IPAddress]::Any, 5000)\n    while ($True) {\n        $Data = $Sock.Receive([ref]$Endpoint)\n        Write-Host \"Received packet from $Endpoint\"\n    }\n\n\nSet up an UDP listener on container3. Enter the following Powershell instructions in interactive console of container3 on the 2nd Windows Compute Node:\n    \npowershell\n    $Sock = [System.Net.Sockets.UdpClient]::new(5000)\n    $Endpoint = [System.Net.IPEndPoint]::new([IPAddress]::Any, 5000)\n    while ($True) {\n        $Data = $Sock.Receive([ref]$Endpoint)\n        Write-Host \"Received packet from $Endpoint\"\n    }\n\n\nSend multiple UDP packets from container1 on the 1st Windows Compute Node. Use different source ports. Set the floating IP (created in step 2) as destination address. Use 5000 (as used in step 6) as destination port.\n\n    Then verify that some packets were received by container2 while some other packets were received by container3. Receiving container is going to display a message: \nReceived packet from \nsender address\n.\n\n    Use the following Powershell command to send 10 UDP packets from various source ports:\n    \npowershell\n    1..100 | % {\n        $s = [System.Net.Sockets.UdpClient]::new(10000+$_)\n        $s.Send(\"C\".ToCharArray(), 1, \"10.0.2.101\", 5000)\n    }", 
            "title": "Connection scenarios"
        }, 
        {
            "location": "/user_guide/connection_scenarios/#connection-scenarios", 
            "text": "", 
            "title": "Connection scenarios"
        }, 
        {
            "location": "/user_guide/connection_scenarios/#creating-virtual-networks-containers-and-interactive-sessions", 
            "text": "Via Contrail WebUI:   Create a policy with the following rule:\n     pass protocol any network any ports any   network any ports any  Create virtual network called  testnet1  with single subnet:  10.0.1.0/24 . Leave every field as default except IP pool (set it to be from  10.0.1.10  to  10.0.1.100 ) and network policy (set it to policy created in first step).  Create virtual network called  testnet2  with single subnet:  10.0.2.0/24 . Leave every field as default except IP pool (set it to be from  10.0.2.10  to  10.0.2.100 ) and network policy (set it to policy created in first step).     On both Windows VMs:   Verify if Agent service is running:\n     powershell\n    Get-Service ContrailAgent \n    State should be Running  Verify if VMSwitch Extension is running and enabled:\n     powershell\n    Get-VMSwitch -Name \"Layered*\" | Get-VMSwitchExtension \n    Find block of data describing to  vRouter \n    Both Enabled and Running fields should be True  Verify if Docker Driver service is running:\n     powershell\n    Get-Service contrail-docker-driver \n    State should be Running  Create docker network  testnet1  on the 1st Windows compute code:\n     powershell\n    docker network create --ipam-driver windows -d Contrail --opt tenant=admin --opt network=testnet1 testnet1  Run a docker container in the prepared network on the 1st Windows compute node:\n     powershell\n    docker run -id --network testnet1 --name container1 microsoft/windowsservercore powershell  Inspect container1 on the 1st Windows compute node. Find the part that describes endpoints and verify that it has a correct IP address assigned (from the previously configured IP pool).\n     powershell\n    docker inspect container1  Run interactive shell inside the container1 on the 1st Windows compute node:\n     powershell\n    docker exec -it container1 powershell  Create docker network \u2018testnet2\u2019 on the 2nd Windows compute node:\n     powershell\n    docker network create --ipam-driver windows -d Contrail --opt tenant=admin --opt network=testnet2 testnet2  Run a docker container in the prepared network (\u2018testnet2\u2019) on the 2nd Windows compute node:\n     powershell\n    docker run -id --network testnet2 --name container2 microsoft/windowsservercore powershell  Inspect container2 on 2nd Windows compute node. Find the part that describes endpoints and verify that it has a correct IP address assigned (from the previously configured IP pool).\n     powershell\n    docker inspect container2  Run a 2nd docker container in the prepared network (\u2018testnet2\u2019) on the 2nd Windows compute node:\n     powershell\n    docker run -id --network testnet2 --name container3 microsoft/iis powershell  Inspect container3 on the 2nd Windows compute node. Find the part that describes endpoints and verify that it has a correct IP address assigned (from the previously configured IP pool).\n     powershell\n    docker inspect container3  Run interactive shell inside the container2 on the 2nd Windows compute node:\n     powershell\n    docker exec -it container2 powershell  Run interactive shell inside the container3 on the 2nd Windows compute node. Remember that Powershell console that has been used in previous step is now running a session in container2. To operate on a host system, open new Powershell console.\n     powershell\n    docker exec -it container3 powershell", 
            "title": "Creating virtual network(s), containers and interactive sessions"
        }, 
        {
            "location": "/user_guide/connection_scenarios/#icmp", 
            "text": "Verify that ICMP protocol works within single virtual network. \n    To do that invoke the following Powershell command in interactive session of container2 (on the 2nd Windows compute Node):\n     powershell\n    ping  IP address of container3  Verify that ICMP protocol works across virtual network boundaries. \n    To do that invoke the following Powershell command in interactive session of container1 (on the 1st Windows Compute Node):\n     powershell\n    ping  IP address of container2", 
            "title": "ICMP"
        }, 
        {
            "location": "/user_guide/connection_scenarios/#tcp", 
            "text": "Invoke the following commands in container3\u2019s interactive session (on the 2nd Windows Compute Node):\n     powershell\n    cd \\inetpub\\wwwroot\n    echo \"We come in peace.\"   index.html  Invoke the following command in container1\u2019s interactive session (on the 1st Windows Compute Node). Verify that you receive server response with webpage prepared in previous step.\n     powershell\n    Invoke-WebRequest http:// container3's IP address  -UseBasicParsing", 
            "title": "TCP"
        }, 
        {
            "location": "/user_guide/connection_scenarios/#udp", 
            "text": "Set up UDP listener on container3 (on the 2nd Windows Compute Node). \n    To do that invoke the following commands in container3\u2019s interactive console. Last instruction is a blocking call waiting for incoming packet.\n     powershell\n    $RemoteIPEndpoint = New-Object System.Net.IPEndPoint([IPAddress]::Any, 0)\n    $UDPRcvSocket = New-Object System.Net.Sockets.UdpClient 3000\n    $Payload = $UDPRcvSocket.Receive([ref]$RemoteIPEndpoint)  Send UDP packet from container1 (on the 1st Windows Compute Node). \n    To do that invoke the following commands in container1\u2019s interactive console.\n     powershell\n    $ListenerAddress = New-Object System.Net.IPEndPoint([IPAddress]::Parse(\"10.0.2.4\"), 3000)\n    $UDPSenderSocket = New-Object System.Net.Sockets.UdpClient 0\n    $Payload = [Text.Encoding]::UTF8.GetBytes(\"We come in peace.\")\n    $UDPSenderSocket.Send($Payload, $Payload.Length, $ListenerAddress)  When packet is received by container3, blocking call to  System.Net.Sockets.UdpClient.Receive  (in container3 on the 2nd Windows Compute Node) is going to return. If that doesn\u2019t happen immediately, repeat invocation of  $UDPSenderSocket.Send  (in container1 on the 1st Windows Compute Node).  Invoke the following command in container1\u2019s interactive session to display payload of received UDP packet to verify that it\u2019s consisted with sent message:\n     powershell\n    [Text.Encoding]::UTF8.GetString($Payload)", 
            "title": "UDP"
        }, 
        {
            "location": "/user_guide/connection_scenarios/#network-policies-and-security-groups", 
            "text": "Create additional policies and security groups (through WebUI) with various settings and demonstrate that they affect communication between networks. One may follow ICMP/TCP/UDP test procedures with various policies and security groups.  Revert network configuration (policies and security groups) to its original state not to affect other experiments.", 
            "title": "Network policies and security groups"
        }, 
        {
            "location": "/user_guide/connection_scenarios/#ecmp", 
            "text": "Create Floating IP Pool through Contrail WebUI assigned to  testnet2 .  Create Floating IP through WebUI. Assign it to previously created pool and set specific IP that is a correct address in  testnet2  (e.g. 10.0.2.101).  Assign Floating IP created in second step to port ( Ports  tab) that belongs to container2.  Assign Floating IP created in second step to port ( Ports  tab) that belongs to container3.  Set up an UDP listener on container2. Enter the following Powershell instructions in interactive console of container2 on the 2nd Windows Compute Node:\n     powershell\n    $Sock = [System.Net.Sockets.UdpClient]::new(5000)\n    $Endpoint = [System.Net.IPEndPoint]::new([IPAddress]::Any, 5000)\n    while ($True) {\n        $Data = $Sock.Receive([ref]$Endpoint)\n        Write-Host \"Received packet from $Endpoint\"\n    }  Set up an UDP listener on container3. Enter the following Powershell instructions in interactive console of container3 on the 2nd Windows Compute Node:\n     powershell\n    $Sock = [System.Net.Sockets.UdpClient]::new(5000)\n    $Endpoint = [System.Net.IPEndPoint]::new([IPAddress]::Any, 5000)\n    while ($True) {\n        $Data = $Sock.Receive([ref]$Endpoint)\n        Write-Host \"Received packet from $Endpoint\"\n    }  Send multiple UDP packets from container1 on the 1st Windows Compute Node. Use different source ports. Set the floating IP (created in step 2) as destination address. Use 5000 (as used in step 6) as destination port. \n    Then verify that some packets were received by container2 while some other packets were received by container3. Receiving container is going to display a message:  Received packet from  sender address . \n    Use the following Powershell command to send 10 UDP packets from various source ports:\n     powershell\n    1..100 | % {\n        $s = [System.Net.Sockets.UdpClient]::new(10000+$_)\n        $s.Send(\"C\".ToCharArray(), 1, \"10.0.2.101\", 5000)\n    }", 
            "title": "ECMP"
        }, 
        {
            "location": "/windows_compute/Communication_Agent_Extension/", 
            "text": "There are three ways in which vRouter Agent exchanges information with the Forwarding Extension, which mimics their Linux behaviour.\n\n\nKsync\n\n\nThis communication channel is used for inserting forwarding rules into the Forwarding Extension, like routes and next hops.\n\n\nOn Linux, Netlink sockets are used for this. Equivalent Windows mechanism is a Named Pipe.\n\n\nOnly vRouter Agent and other userspace programs can initialize the communiation. Howerver, different programs (for example, Agent and utils) may talk via ksync at the same time, so process separation (session layer) is required.\n\n\nThis communication is performed over a binary protocol called Sandesh (the same one which is used by Contrail Analytics Nodes).\n\n\nFlow\n\n\nAll flows are inserted into the Forwarding Extension using shared memory mechanism. Shared memory is allocated inside Forwarding Extension, and is written to by Agent. Only Agent may allocate flows inside the shared memory. Forwarding Extension may modify the shared memory contents, but it never directly communicates with vRouter Agent using it or creates new flows - it mostly updates flow statistics.\n\n\nOnly Agent and \"flow\" util need access to shared memory from userspace.\n\n\nSince kernel memory is mapped into userspace, so the team ensured that the implementation is secure.\n\n\npkt0\n\n\nThis channel is asynchronous - both vRouter Agent and Forwarding Extension can send packets through it at arbitrary times.\n\n\nWhen a \"new\" packet arrives at Forwarding Extension (for example, ARP or DHCP request), it will transfer it using pkt0 to vRouter Agent. vRouter Agent will then analyze the packet (possibly sending it to Contrail Controller for even more analysis), and insert ksync rules or flows into the Forwarding Extension. Then, it re-injects the packet into the Forwarding Extension. \n\n\nPkt0 is implemented on Linux using a traditional networking interface, of AF_PACKET family. On Windows, this is done using a named pipe. The reason for this is that Windows implementation of Berkeley sockets does not allow any manipulation of any layer below IP.\n\n\nNamed pipe server is created by the Forwarding Extension.\n\n\nSandesh\n\n\nSandesh is a binary protocol based on Apache Thrift. It's a set of header files to be included in projects that use it, but also a code generation tool. Sandesh uses own Interface Definition Language to generate code in many different languages, like Java, C, Python and Golang. Generated code consists of definitions of data structures and getter/setter functions.", 
            "title": "Communication Agent Extension"
        }, 
        {
            "location": "/windows_compute/Communication_Agent_Extension/#ksync", 
            "text": "This communication channel is used for inserting forwarding rules into the Forwarding Extension, like routes and next hops.  On Linux, Netlink sockets are used for this. Equivalent Windows mechanism is a Named Pipe.  Only vRouter Agent and other userspace programs can initialize the communiation. Howerver, different programs (for example, Agent and utils) may talk via ksync at the same time, so process separation (session layer) is required.  This communication is performed over a binary protocol called Sandesh (the same one which is used by Contrail Analytics Nodes).", 
            "title": "Ksync"
        }, 
        {
            "location": "/windows_compute/Communication_Agent_Extension/#flow", 
            "text": "All flows are inserted into the Forwarding Extension using shared memory mechanism. Shared memory is allocated inside Forwarding Extension, and is written to by Agent. Only Agent may allocate flows inside the shared memory. Forwarding Extension may modify the shared memory contents, but it never directly communicates with vRouter Agent using it or creates new flows - it mostly updates flow statistics.  Only Agent and \"flow\" util need access to shared memory from userspace.  Since kernel memory is mapped into userspace, so the team ensured that the implementation is secure.", 
            "title": "Flow"
        }, 
        {
            "location": "/windows_compute/Communication_Agent_Extension/#pkt0", 
            "text": "This channel is asynchronous - both vRouter Agent and Forwarding Extension can send packets through it at arbitrary times.  When a \"new\" packet arrives at Forwarding Extension (for example, ARP or DHCP request), it will transfer it using pkt0 to vRouter Agent. vRouter Agent will then analyze the packet (possibly sending it to Contrail Controller for even more analysis), and insert ksync rules or flows into the Forwarding Extension. Then, it re-injects the packet into the Forwarding Extension.   Pkt0 is implemented on Linux using a traditional networking interface, of AF_PACKET family. On Windows, this is done using a named pipe. The reason for this is that Windows implementation of Berkeley sockets does not allow any manipulation of any layer below IP.  Named pipe server is created by the Forwarding Extension.", 
            "title": "pkt0"
        }, 
        {
            "location": "/windows_compute/Communication_Agent_Extension/#sandesh", 
            "text": "Sandesh is a binary protocol based on Apache Thrift. It's a set of header files to be included in projects that use it, but also a code generation tool. Sandesh uses own Interface Definition Language to generate code in many different languages, like Java, C, Python and Golang. Generated code consists of definitions of data structures and getter/setter functions.", 
            "title": "Sandesh"
        }, 
        {
            "location": "/windows_compute/Container_creation_lifecycle/", 
            "text": "Docker client tells docker daemon to run a container and attach it to a Docker network (that corresponds to a chunk of a Contrail subnet).\n\n\nDocker deamon delegates the network configuration of the container that is being created to the remote docker driver.\n\n\nDocker driver queries docker daemon about metadata associated with docker's network resource and receives Contrail tenant name and subnet CIDR.\n\n\nDocker driver performs a series of queries against Contrail Controller that create various Contrail resources, most notably: virtual-machine, virtual-machine-interface, instance-ip.\n\n\nDocker driver receives Instance IP, MAC and default gateway to configure the newly created container's interface with. It also receives UUID of virtual-machine-interface resource\n\n\nDocker driver knows all the information necessary to recreate HNS Network's name. It uses this name to identify which HNS network to attach the container to. Docker driver sends requests to HNS to configure endpoint with newly received IP, MAC and Default gateway.\n\n\nIn the background, HNS creates the network compartment for the container and configures its interface.\n\n\nThe moment the interface is created, the Forwarding Extension receives an event about new adapter being connected. \n\n\nForwarding Extension doesn't know what to do with it yet, so it stores it in a hash map, where the key is adapters FriendlyName. Then it waits, dropping all packets except the ones sent from native Hyper-V driver.\n\n\nMeanwhile, if container creation was successful, HNS returns ID of the endpoint to docker driver.\n\n\nDocker driver sends \"add port\" request to vRouter Agent, with all the necessary information to identify the port both in Contrail terms (UUID of virtual-machine-interface) as well as in Windows terms (using Endpoint ID, which is a part of FriendlyName).\n\n\nvRouter Agent communicates with Contrail Controller to let it know about newly added port.\n\n\nvRouter Agent inserts basic rules and flows into the Forwarding Extension. \n\n\nForwarding Extension uses the FriendlyName to determine which port seen in userspace corresponds to port waiting in Forwarding Extension's hash map.", 
            "title": "Container creation lifecycle"
        }, 
        {
            "location": "/windows_compute/Docker_on_Windows/", 
            "text": "Docker\n\n\nDocker is a popular tools for managing containers on Linux.\n\n\nSince Windows Server 2016, docker is the main tool used for managing Windows Containers.\n\n\nDocker client\n\n\nDocker command line tool, which communicates with Docker Daemon over a named pipe.\n\n\nDocker daemon\n\n\nA program that runs as a service. It exposes an API over a named pipe. It's responsible for managing containers, from parsing Dockerfiles and pulling layers to basic networking.\n\n\nDocker is cross-platform and is integrated into many different network orchestration systems. This is achieved thanks to its plugin architecture. One can implement all kinds of plugins for docker, for example for volume or network management. These plugins can either be integrated into docker's code directly, or can be implemented as Remote drivers. The advantage of Remote drivers is that one doesn't have to build the whole Docker if she wants to implement a driver. Remote drivers on Windows can use tcp sockets or named pipes for communication. In either case, docker daemon will look for a \"spec file\" in a specific path: $Env:ProgramData\\docker\\plugins. The name of the spec file must be the same as driver's name. The file itself contains a protocol and address on which docker daemon will try to initiate communication.\n\n\nSince docker is written in Go, Microsoft engineers had to expose an API for managing Windows Containers written in this language. \n\n\nHost compute service (HCS) \n host network service (HNS)\n\n\nGolang wrapper for HCS and HNS is implemented in a public hcsshim repository: www.github.com/Microsoft/hcsshim. HCS and HNS are Windows Services responsible for light container virtualization as well as setting up virtual networking for them. They are implemented in some system dynamic libraries, which interact with the kernel directly. How it works isn't documented.\n\n\nHost compute service\n\n\nDeals with volumes, filesystem layers etc. It's related to HNS, but it's not inside scope of the project directly.\n\n\nHost network service\n\n\nHNS is a wrapper for syscalling functions in vmcompute.dll. Only one file in the whole hcsshim repository is used by HNS - 'hnsfuncs.go'.\n\n\nWindows Containers\n\n\nWindows Containers are a feature shipped with Windows Server 2016, Windows 10 and Windows Nano Server. These act similarly to lxc containers. On Windows, they are implemented using various namespaces, as well as network namespace equivalent (called network compartment).\n\n\nNetwork compartments are not documented, but vmcompute.dll contains some functions related to how they work.\nUnlike on Linux, Windows supports two types/levels of isolation for its containers:\n\n Windows Server containers, which work just like normal Linux containers - they all share the kernel\n\n Hyper-V containers. These are essentialy small VMs (running Nano Server), that run Windows Containers which then run the container we wanted. This additional level of virtualization is useful mostly for security reasons. It provides some pros and cons of both containers and VMs. To run a container in this mode, one needs to add \"--isolation-level=hyperv\" to \"docker run\" command.\n\n\nThere are no differences on how networking is implemented for both of these types of containers.\n\n\nThere are two base images for docker container creation:\n\n\n\n\nmicrosoft/windowsservercore which is headless version of normal Windows Server Core\n\n\nmicrosoft/nanoserver which is very lightweight, stripped of most functionality version of Windows Server Core", 
            "title": "Docker"
        }, 
        {
            "location": "/windows_compute/Docker_on_Windows/#docker", 
            "text": "Docker is a popular tools for managing containers on Linux.  Since Windows Server 2016, docker is the main tool used for managing Windows Containers.", 
            "title": "Docker"
        }, 
        {
            "location": "/windows_compute/Docker_on_Windows/#docker-client", 
            "text": "Docker command line tool, which communicates with Docker Daemon over a named pipe.", 
            "title": "Docker client"
        }, 
        {
            "location": "/windows_compute/Docker_on_Windows/#docker-daemon", 
            "text": "A program that runs as a service. It exposes an API over a named pipe. It's responsible for managing containers, from parsing Dockerfiles and pulling layers to basic networking.  Docker is cross-platform and is integrated into many different network orchestration systems. This is achieved thanks to its plugin architecture. One can implement all kinds of plugins for docker, for example for volume or network management. These plugins can either be integrated into docker's code directly, or can be implemented as Remote drivers. The advantage of Remote drivers is that one doesn't have to build the whole Docker if she wants to implement a driver. Remote drivers on Windows can use tcp sockets or named pipes for communication. In either case, docker daemon will look for a \"spec file\" in a specific path: $Env:ProgramData\\docker\\plugins. The name of the spec file must be the same as driver's name. The file itself contains a protocol and address on which docker daemon will try to initiate communication.  Since docker is written in Go, Microsoft engineers had to expose an API for managing Windows Containers written in this language.", 
            "title": "Docker daemon"
        }, 
        {
            "location": "/windows_compute/Docker_on_Windows/#host-compute-service-hcs-host-network-service-hns", 
            "text": "Golang wrapper for HCS and HNS is implemented in a public hcsshim repository: www.github.com/Microsoft/hcsshim. HCS and HNS are Windows Services responsible for light container virtualization as well as setting up virtual networking for them. They are implemented in some system dynamic libraries, which interact with the kernel directly. How it works isn't documented.", 
            "title": "Host compute service (HCS) &amp; host network service (HNS)"
        }, 
        {
            "location": "/windows_compute/Docker_on_Windows/#host-compute-service", 
            "text": "Deals with volumes, filesystem layers etc. It's related to HNS, but it's not inside scope of the project directly.", 
            "title": "Host compute service"
        }, 
        {
            "location": "/windows_compute/Docker_on_Windows/#host-network-service", 
            "text": "HNS is a wrapper for syscalling functions in vmcompute.dll. Only one file in the whole hcsshim repository is used by HNS - 'hnsfuncs.go'.", 
            "title": "Host network service"
        }, 
        {
            "location": "/windows_compute/Docker_on_Windows/#windows-containers", 
            "text": "Windows Containers are a feature shipped with Windows Server 2016, Windows 10 and Windows Nano Server. These act similarly to lxc containers. On Windows, they are implemented using various namespaces, as well as network namespace equivalent (called network compartment).  Network compartments are not documented, but vmcompute.dll contains some functions related to how they work.\nUnlike on Linux, Windows supports two types/levels of isolation for its containers:  Windows Server containers, which work just like normal Linux containers - they all share the kernel  Hyper-V containers. These are essentialy small VMs (running Nano Server), that run Windows Containers which then run the container we wanted. This additional level of virtualization is useful mostly for security reasons. It provides some pros and cons of both containers and VMs. To run a container in this mode, one needs to add \"--isolation-level=hyperv\" to \"docker run\" command.  There are no differences on how networking is implemented for both of these types of containers.  There are two base images for docker container creation:   microsoft/windowsservercore which is headless version of normal Windows Server Core  microsoft/nanoserver which is very lightweight, stripped of most functionality version of Windows Server Core", 
            "title": "Windows Containers"
        }, 
        {
            "location": "/windows_compute/Forwarding_extension/", 
            "text": "On Linux, a special vRouter Kernel Module is enabled to perform the heavy-duty, high-performance datapath functionality of Contrail. The Forwarding Extension is a Windows equivalent. It's implemented as a forwarding extension of Window's Hyper-V Extensible Switch.\n\n\nNote: Even though the name \"forwarding extension\" refers to the type of extension of Hyper-V Switch, the team calls the equivalent of Linux kernel module as \"the Forwarding Extension\".\n\n\nDP-core is a cross platform module that is responsible for datapath logic. Even though its design is cross platform (DP-core exposes a set of function pointers, that need to be implemented differently on different operating systems, for example memory allocation, packet inspection procedures, timer functions etc.), its implementation uses many gcc-specific built ins, which require porting over to Windows.\n\n\nNote: In theory, kernel extension compiled with GCC might work on Windows, but Visual Studio compiler is recommended.\n\n\nThe Extension implements an NDIS (Network Driver Interface Specification) API. The NDIS API is event driven, and is realized in terms of callback functions.\n\n\nThere is two-way interaction between DP-core and NDIS:\n\n\n\n\nSince the Extension is event driven, whenever an event occurs, a specific NDIS callback is ran - for example, if a packet is received, or a new port is connected. In those cases, DP-core's functions will need to be used inside the callback to enact vRouter logic and reach a forwarding decision.\n\n\nHowever, DP-core sometimes needs additional information to reach those decisions. It uses the \"callbacks\" in form of aforementioned function pointers to OS-specific implementations of those functionalities.\n\n\n\n\nNote: In other words, there are two \"kinds\" of callbacks: \"NDIS callbacks\" and \"DP-core callbacks\". NDIS callbacks are a consequence of event-driven architecture of Hyper-V Extensions, while DP-core callbacks are actually just platform specific \"dependency injections\" or \"C interfaces\". \nThe team usually refers to those \"DP-core callbacks\" as just callbacks. NDIS callbacks are usually referred to as \"Extension/NDIS API implementation\".\n\n\nNote: There is a convenience library provided by Microsoft around the raw NDIS API. The library is referred to as SxBase, but it's just 4 source files. They can be found here: https://github.com/Microsoft/Windows-driver-samples/tree/master/network/ndis/extension/base. SxBase is used in the current implementation, but it will HAVE TO be replaced in the future, due to licensing issues.\n\n\nThe Forwarding Extension does not directly communicate with Contrail Controller. Instead, vRouter Agent is responsible for creating all the objects and flows inside the Extension.", 
            "title": "Forwarding extension"
        }, 
        {
            "location": "/windows_compute/HyperV_extensible_switch/", 
            "text": "Windows Server 2012 introduced Hyper-V Extensible Switch. It\u2019s a virtual switch that runs in OS running in Hyper-V\u2019s parent partition. It\u2019s mainly used for forwarding packets flowing in and out of child partitions (VMs and containers).\n\n\nThree types of switches can be instantiated:\n\n\n\n\nprivate - only interfaces connected to the vSwitch can communicate\n\n\ninternal - like private, but the host (hypervisor) is also connected\n\n\nexternal - like internal, but external network adapter is also connected. Usually, this is the physical adapter. Multiple external switches may exist at any given time.\n\n\n\n\nHyper-V Extensible Switch is \"Extensible\", which means it can be extended using third party extensions (plugins).\n\n\nMultiple extensions can be enabled at the same time. Hyper-V Switch's plugin architecture resembles a stack. The order in which ingress and egress packets will be processed by a specific extension depends on their position in the filter stack.\n\n\nThere are three types of extensions:\n\n\n\n\ncapture - which can inspect packets, and, for example, log them somewhere\n\n\nfilter - like capture, but has the ability to drop packets\n\n\nforwarding - like filter, but has the ability to forward and modify the packets.\n\n\n\n\nMultiple instances of capture and filter extensions can be enabled in arbitrary locations in the extensions stack, but only one forwarding extension can be enabled at a time. It must be located at the bottom of the stack as well, which means that it is the last to process ingress packets, but the first to process egress packets.", 
            "title": "HyperV extensible switch"
        }, 
        {
            "location": "/windows_compute/Linux_Overview/", 
            "text": "Contrail Controller\n\n\nContrail Controller is a logical component. In fact, it's a distributed system consisting of many nodes, most importantly:\n\n\n\n\nConfig nodes. These nodes expose a REST API used by various applications to configure Contrail resources. Contrail's web client communicates with it, and so does Docker Network Driver. Config information is eventually propagated to Control nodes.\n\n\nControl nodes, responsible for interacting with vRouter Agent via XMPP protocol. These interactions include inserting forwarding rules and flows and sending packets for further analysis.\n\n\nAnalytics nodes, which collect high volume information using Sandesh binary protocol.\n\n\n\n\nvRouter Agent mainly interacts with Control and Analytics nodes, while Docker network driver interacts with Config nodes. \n\n\nCompute Node\n\n\nCompute Node is a Contrail component. It's a hypervisor that is able to spawn VMs or containers. Every compute node must have an instance of vRouter Agent running.\n\n\nWindows Server 2016 and Nano Server are new versions of Microsoft's OS, that can act both as Hyper-V hypervisor as well as Windows Containers host.\n\n\nNano Server running as Compute Node is currently not supported, due to lacking the capability of installing Hyper-V networking extensions.\n\n\nvRouter Agent\n\n\nNote: The team sometimes refers to vRouter Agent simply as \"vRouter\", \"Agent\" or \"vRA\".\n\n\nvRouter Agent is a Contrail agent that runs in userspace on a compute node. It has many functions. Most importantly:\n\n\n\n\nit communicates with Contrail Controller via XMPP protocol. Controller will inject rules and flows into the agent so that it enacts it's routing policies and implements network virtualization,\nexposes an API for the hypervisor, that is used for registering newly created virtual machines or containers. This * is most notably used in Contrail-OpenStack integration, where Nova-Agent informs vRouter Agent whenever a new VM is spawned. On Windows, the API is exposed over a named pipe.\n\n\ncommunicates with the Forwarding Extension (three communication channels in total). Since vRouter Agent is a control plane component, it must inject all low level information like next hops, virtual interface information, routes and flows into the datapath component (the Forwarding Extension).\n\n\nsometimes forwards packets from Forwarding Extension to Contrail Controller\n\n\nsometimes injects packets sent from Contrail Controller into the Forwarding Extension", 
            "title": "Contrail Controller"
        }, 
        {
            "location": "/windows_compute/Linux_Overview/#contrail-controller", 
            "text": "Contrail Controller is a logical component. In fact, it's a distributed system consisting of many nodes, most importantly:   Config nodes. These nodes expose a REST API used by various applications to configure Contrail resources. Contrail's web client communicates with it, and so does Docker Network Driver. Config information is eventually propagated to Control nodes.  Control nodes, responsible for interacting with vRouter Agent via XMPP protocol. These interactions include inserting forwarding rules and flows and sending packets for further analysis.  Analytics nodes, which collect high volume information using Sandesh binary protocol.   vRouter Agent mainly interacts with Control and Analytics nodes, while Docker network driver interacts with Config nodes.", 
            "title": "Contrail Controller"
        }, 
        {
            "location": "/windows_compute/Linux_Overview/#compute-node", 
            "text": "Compute Node is a Contrail component. It's a hypervisor that is able to spawn VMs or containers. Every compute node must have an instance of vRouter Agent running.  Windows Server 2016 and Nano Server are new versions of Microsoft's OS, that can act both as Hyper-V hypervisor as well as Windows Containers host.  Nano Server running as Compute Node is currently not supported, due to lacking the capability of installing Hyper-V networking extensions.", 
            "title": "Compute Node"
        }, 
        {
            "location": "/windows_compute/Linux_Overview/#vrouter-agent", 
            "text": "Note: The team sometimes refers to vRouter Agent simply as \"vRouter\", \"Agent\" or \"vRA\".  vRouter Agent is a Contrail agent that runs in userspace on a compute node. It has many functions. Most importantly:   it communicates with Contrail Controller via XMPP protocol. Controller will inject rules and flows into the agent so that it enacts it's routing policies and implements network virtualization,\nexposes an API for the hypervisor, that is used for registering newly created virtual machines or containers. This * is most notably used in Contrail-OpenStack integration, where Nova-Agent informs vRouter Agent whenever a new VM is spawned. On Windows, the API is exposed over a named pipe.  communicates with the Forwarding Extension (three communication channels in total). Since vRouter Agent is a control plane component, it must inject all low level information like next hops, virtual interface information, routes and flows into the datapath component (the Forwarding Extension).  sometimes forwards packets from Forwarding Extension to Contrail Controller  sometimes injects packets sent from Contrail Controller into the Forwarding Extension", 
            "title": "vRouter Agent"
        }, 
        {
            "location": "/windows_compute/Network_creation_lifecycle/", 
            "text": "There is a slight discrepancy between Docker's and Contrail's networking model. Contrail can implement a logical, overlay network for containers. However, docker can only create a network locally, on the hypervisor.\n\n\nThis means that \"docker network create\" command needs to be ran on each hypervisor that will contain any container that could be connected to a specific network. In other words, \"local\" networks must be prepared on each host.\n\n\nFurthermore, Docker's \"network\" is actually equivalent to Contrail's \"subnet\". This means, that during docker network creation, specific Contrail subnet (using CIDR notation) must be specified as a parameter.\n\n\n\n\nDocker client tells docker daemon to create a docker network that will represent a chunk of a Contrail subnet. Tenant name, network name and subnet's CIDR are passed as parameters.\n\n\nDocker daemon creates a network resource in its own, local database, and then delegates handling of network configuration to the docker driver.\n\n\nDocker driver queries Contrail Controller whether specified tenant, network and subnet combination exists. It also asks for their details.\n\n\nContrail Controller returns subnet's default gateway address as well as subnet's UUID\n\n\nDocker driver calls HNS function to create a HNS network with subnet's CIDR and default gateway address. It also sets the HNS network's name as a concatenation of \"Contrail\" with tenant name, network name and subnet ID.\n\n\nDocker driver tells docker daemon to store tenant and network names in docker network's metadata. Docker daemon stores this information in its own, local database.", 
            "title": "Network creation lifecycle"
        }, 
        {
            "location": "/windows_compute/Root_HNS_network/", 
            "text": "HNS operates by using two virtual switches:\n\n\n\n\nan external vswitch, which is connected directly to physical adapter. This adapter is called \"vEthernet (HNSTransparent)\", and the switch is called \"Layered Etherenet0\" depending on which physicial adapter it's connected to. If there is physical NIC teaming, then its name contains all the physical adapters separated by a coma, like \"Layered Ethernet0,Ethernet1\"\n\n\nan internal vswitch, for the purpose of NATing. The adapter that connects to the host is named \"vEthernet (HNS Internal NIC)\", and the switch is called \"nat\".\n\n\n\n\nThose virtual switches are not created unless there is at least one HNS network using corresponding Mode: \"transparent\" or \"nat\". If the last network of corresponding Mode is removed, the vswitch is also removed. Creation/deletion of vswitch takes a couple seconds and disrupts network connectivity.\n\n\nThis dynamism of creating/deleting vswitches poses a problem, because we want vRouter Forwarding Extension to be persistent, no matter if there are many or no virtual networks. That's why, when Docker Driver initializes, it creates a \"dummy\" Root HNS Network, which makes HNS create an external vswitch \"Layered Ethernet*\". Forwarding Extension is then enabled for that vswitch.", 
            "title": "Root HNS network"
        }, 
        {
            "location": "/windows_compute/Utility_tools/", 
            "text": "A set of utility tools (sometimes referred to simply as \"Utils\") useful for manually injecting rules into the datapath or for introspect and debugging. They communicate directly with the Forwarding Extension. There are split into two categories:\n\n\nBasic utils\n\n\nThese are used for basic vRouter introspection, but also for manual insertion of own rules and objects into the Forwarding Extension.\n\n\n\n\nvif - used for showing and creating virtual interfaces\n\n\nnh - used for showing and creating next hops\n\n\nrt - used for showing and creating routes\n\n\nflow - used for showing flows\n\n\n\n\nThe fifth \"basic util\" is a test util. Since kernel drivers are hard to test without moving them into userspace, Juniper decided to implement a tool that simulates normal usage.\n\n\n\n\nvtest - runs playback tests against the Forwarding Extension\n\n\n\n\nExtended utils\n\n\nThese are mainly used for statistics and advanced debugging.\n\n\n\n\nvxlan\n\n\nvrmemstats\n\n\nvrfstats\n\n\nqosmap\n\n\nmpls\n\n\ndropstats\n\n\nmirror", 
            "title": "Utility tools"
        }, 
        {
            "location": "/windows_compute/Utility_tools/#basic-utils", 
            "text": "These are used for basic vRouter introspection, but also for manual insertion of own rules and objects into the Forwarding Extension.   vif - used for showing and creating virtual interfaces  nh - used for showing and creating next hops  rt - used for showing and creating routes  flow - used for showing flows   The fifth \"basic util\" is a test util. Since kernel drivers are hard to test without moving them into userspace, Juniper decided to implement a tool that simulates normal usage.   vtest - runs playback tests against the Forwarding Extension", 
            "title": "Basic utils"
        }, 
        {
            "location": "/windows_compute/Utility_tools/#extended-utils", 
            "text": "These are mainly used for statistics and advanced debugging.   vxlan  vrmemstats  vrfstats  qosmap  mpls  dropstats  mirror", 
            "title": "Extended utils"
        }, 
        {
            "location": "/windows_compute/_Blueprint/", 
            "text": "Rationale and Scope\n\n\nWindows Server 2016 comes with support for containers and docker. Pros and cons of using containers are well known. This also sets up Contrail for possible Hyper-V support in the future.\n\n\nThe scope is to port Compute node to Windows (Agent and vRouter) and integrate with Windows docker and Windows Containers.\n\n\nImplementors\n\n\nRajagopalan Sivaramakrishnan (raja@juniper.net)\nSagar Chitnis (sagarc@juniper.net)\nCodiLime dev team (windows-contrail@codilime.com)\n\n\nTarget Release\n\n\nFor release 1: 5.0\nFor release 2+: tbd\n\n\nUser-visible changes\n\n\nOn Linux, none.\nOn Windows, one can use Windows docker command line tools to spawn Windows containers and connect them to Contrail networks. (orchestration is not in scope of release 1)\nFor deployment, MSI installers are provided.\n\n\nInternal changes\n\n\n3 components are affected:\n\n\n\n\n\n\nWindows docker driver is added. It is roughly equivalent to Nova Agent, but runs as a Windows service and implements docker driver APIs. It communicates with config, Agent and HNS (Host Network Service - Windows container management service). Written in Golang.\n\n\n\n\n\n\nvRouter Agent. Parts of the codebase are not cross platform. Changes involve rewriting those and fixing related bugs.\n\n\n\n\n\n\nvRouter Forwarding Extension. Implements vRouter kernel module functionality in terms of a Hyper-V Forwarding Extension, which is basically a kernel mode \"plugin\" for Windows virtual switch.\n\n\n\n\n\n\nLinux communication channels between (2) and (3) are also ported using Named Pipes (for ksync and pkt0) and Windows shared memory (for flow).\n\n\nMore info is available under spec url.", 
            "title": " Blueprint"
        }, 
        {
            "location": "/windows_compute/_Blueprint/#rationale-and-scope", 
            "text": "Windows Server 2016 comes with support for containers and docker. Pros and cons of using containers are well known. This also sets up Contrail for possible Hyper-V support in the future.  The scope is to port Compute node to Windows (Agent and vRouter) and integrate with Windows docker and Windows Containers.", 
            "title": "Rationale and Scope"
        }, 
        {
            "location": "/windows_compute/_Blueprint/#implementors", 
            "text": "Rajagopalan Sivaramakrishnan (raja@juniper.net)\nSagar Chitnis (sagarc@juniper.net)\nCodiLime dev team (windows-contrail@codilime.com)", 
            "title": "Implementors"
        }, 
        {
            "location": "/windows_compute/_Blueprint/#target-release", 
            "text": "For release 1: 5.0\nFor release 2+: tbd", 
            "title": "Target Release"
        }, 
        {
            "location": "/windows_compute/_Blueprint/#user-visible-changes", 
            "text": "On Linux, none.\nOn Windows, one can use Windows docker command line tools to spawn Windows containers and connect them to Contrail networks. (orchestration is not in scope of release 1)\nFor deployment, MSI installers are provided.", 
            "title": "User-visible changes"
        }, 
        {
            "location": "/windows_compute/_Blueprint/#internal-changes", 
            "text": "3 components are affected:    Windows docker driver is added. It is roughly equivalent to Nova Agent, but runs as a Windows service and implements docker driver APIs. It communicates with config, Agent and HNS (Host Network Service - Windows container management service). Written in Golang.    vRouter Agent. Parts of the codebase are not cross platform. Changes involve rewriting those and fixing related bugs.    vRouter Forwarding Extension. Implements vRouter kernel module functionality in terms of a Hyper-V Forwarding Extension, which is basically a kernel mode \"plugin\" for Windows virtual switch.    Linux communication channels between (2) and (3) are also ported using Named Pipes (for ksync and pkt0) and Windows shared memory (for flow).  More info is available under spec url.", 
            "title": "Internal changes"
        }, 
        {
            "location": "/windows_compute/vRouter_Linux_implementation/", 
            "text": "On Linux, vRouter implementation consists of two parts: a userspace agent and a kernel module, along with all the userspace utility command line tools. The agent communicates with OpenContrail\u2019s control nodes and receives configuration state, like forwarding information. Its main responsibility is installation of forwarding state into the kernel module.\n\n\nFunctionality of the data plane, like flow tables, is contained in dp-core component. Translation between the agent\u2019s object model and low-level data model used by datapath is realized by the KSync module, which is utilized by both the agent and the kernel module.\n\n\nOn Linux, communication between the forwarding plane, which runs in kernel space, and the user space agent, is done via use of Netlink sockets and shared memory. However, neither BSD nor Windows support Netlink sockets. That\u2019s why, on FreeBSD, they are emulated over raw sockets.", 
            "title": "vRouter Linux implementation"
        }
    ]
}