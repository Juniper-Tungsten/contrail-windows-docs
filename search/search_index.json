{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Home Windows Contrail provides support for Windows Containers to Contrail. Please use the panel on the left to select topic. Quick links: Deployment Usage Troubleshooting Test scenarios Platforms Docker: Docker EE 17.06 Compute node OS: Windows Server 2016 Not supported Nano server Not supported Windows 10 Instance types: Windows Server containers Not supported Hyper-V isolation level containers Not supported Linux on Windows containers Not supported Hyper-V virtual machines Please refer to unsupported features for more information.","title":"Home"},{"location":"#home","text":"Windows Contrail provides support for Windows Containers to Contrail. Please use the panel on the left to select topic. Quick links: Deployment Usage Troubleshooting Test scenarios","title":"Home"},{"location":"#platforms","text":"Docker: Docker EE 17.06 Compute node OS: Windows Server 2016 Not supported Nano server Not supported Windows 10 Instance types: Windows Server containers Not supported Hyper-V isolation level containers Not supported Linux on Windows containers Not supported Hyper-V virtual machines Please refer to unsupported features for more information.","title":"Platforms"},{"location":"For CI admins/Backups/Infrastructure_backups/","text":"Infrastructure backups This document describes current backup system configuration for Contrail Windows CI. Table of contents Current backup setup VM recovery guide Backup scripts update guide Current backup setup Diagram below presents an overview of current backup setup for Contrail Windows CI. Contrail Windows CI underlying infrastructure is based VMware vSphere Services support Contrail Windows CI are running on a set of Infrastructure VMs Infrastructure VMs reside on WinCI-Datastores-Infra datastore cluster WinCI-Datastores-Infra datastore cluster consists of an independent set of SSD-based datastores independent, meaning these datastores are used solely for Infrastructure VMs Backups are performed using Veeam Backup Replication software Veeam Backup Free Edition is used Veeam is installed on winci-veeam , Windows Server 2016 virtual machine winci-veeam has a mounted NFS share winci_nfsbackup which is used as a backups repository Veeam connects to vSphere cluster using vSphere API Veeam performs full VM backups (using VeeamZIP feature in the Free edition) Backups are performed regulary using Windows Scheduled Tasks . Currently scheduled task runs every day, at 10 PM PST. Scheduled task executes a PowerShell Script Backup.ps1 located in C:\\BackupScripts\\ directory. Backup procedure looks as follows: Backups are performed by a set of PowerShell scripts Scripts are located in backups/ directory in contrail-windows-ci repository Backup.ps1 script performs the following steps: Prunes backups by removing all but last N backups N is configurable in the script Uses VeeamZIP PowerShell cmdlets to perform a full backups of VMs listed in Backup.ps1 file If an error occurs for any of the VMs listed, script continues its job. After performing backups for other VMs it terminates with an exception Each full backup is stored in a separate directory, located in \\\\winci_nfsbackups\\Backups\\ directory on backup NFS share VM recovery guide Current backup setup supports only full VM recovery. It needs to be performed, e.g. when: in case of underlying infrastructure hardware failure (e.g. corrupted disk, ESXi host failure) in case of bricking an operating system/virtual machine recovery of VM state from a certain point in time is desirable Prerequisites to backup scripts updating: Windows 10/Windows Server 2016 machine to perform the steps Credentials to winci-veeam server Please contact Windows CI team To recover a VM from backup perform the following steps: Establish a RDP connection winci-veeam server Open Veeam Backup Replication Console program is available through Start menu Veeam login window should open up ensure that localhost and port 9392 are selected and Use Windows session authentication option is selected click Connect In the upper left corner click an option Restore Select backup file to restore windows should open up Click browse In the address bar type in X:\\Backups\\ A list of directories named with date timestamps should show up Select a directory with a date, matching your desired restore point Enter this directory Select a backup file (file with .vbk extension) which has prefix matching VM to be restored Click Open Progress bar named Reading the backup file, please wait... should show up When a table named Contained VMs appears, select a VM to be restored, click Restore ; clicking Restore opens up contextual menu; click option Entire VM (including registration) After a few moments a restore wizard should show up In Virtual machine section From a table Virtual machines to restore select a VM to be restored Click Next In Restore Mode section Select Restore to the original location option Make sure Restore VM tags option is checked Make sure Quick rollback option is unchecked Click Next In Reason section Enter a reason for VM restoring into Restore reasons textbox Click Next In Summary section Verify VM details presented in Summary textbox Check Power on target virtual machine option if you want the restored VM to power on after restoration is completed Click Finish VM Restore window with progress report should appear Window can be closed by clicking Close button, this will not stop the restore procedure To open up VM Restore window again Click History button in the lower left corner of Veeam Console In the tree (in the left middle part of the Veeam Console) select Restore Full VM Restore branch A list of restore jobs should appear in the middle Double-click a job corresponding to a VM you are currently restoring VM Restore window should open up again When a restore job finishes, verify if all services provided by the machine are up and working Backup scripts update guide Backup scripts need to be updated, e.g. when: A list of VMs requiring backup changes A bug was fixed in the backup scripts Prerequisites to backup scripts updating: Windows 10/Windows Server 2016 machine to perform the steps Credentials to winci-veeam server Please contact Windows CI team To update backup scripts perform the following steps: Clone contrail-windows-ci repository and enter backups/ directory PS C:\\Users\\user git clone https://github.com/Juniper/contrail-windows-ci.git PS C:\\Users\\user cd contrail-windows-ci\\backups PS C:\\Users\\user\\contrail-windows-ci\\backups Establish a PowerShell session with winci-veeam server PS C:\\Users\\user\\contrail-windows-ci\\backups $session = New-PSSession -ComputerName 10.84.12.29 -Credentials $(Get-Credential) Copy backups directory's contents to winci-veeam server, to C:\\BackupScripts directory PS C:\\Users\\user\\contrail-windows-ci\\backups Copy-Item .\\* C:\\BackupScripts -ToSession $session Close PowerShell session PS C:\\Users\\user\\contrail-windows-ci\\backups Remove-PSSession $session","title":"Infrastructure backups"},{"location":"For CI admins/Backups/Infrastructure_backups/#infrastructure-backups","text":"This document describes current backup system configuration for Contrail Windows CI. Table of contents Current backup setup VM recovery guide Backup scripts update guide","title":"Infrastructure backups"},{"location":"For CI admins/Backups/Infrastructure_backups/#current-backup-setup","text":"Diagram below presents an overview of current backup setup for Contrail Windows CI. Contrail Windows CI underlying infrastructure is based VMware vSphere Services support Contrail Windows CI are running on a set of Infrastructure VMs Infrastructure VMs reside on WinCI-Datastores-Infra datastore cluster WinCI-Datastores-Infra datastore cluster consists of an independent set of SSD-based datastores independent, meaning these datastores are used solely for Infrastructure VMs Backups are performed using Veeam Backup Replication software Veeam Backup Free Edition is used Veeam is installed on winci-veeam , Windows Server 2016 virtual machine winci-veeam has a mounted NFS share winci_nfsbackup which is used as a backups repository Veeam connects to vSphere cluster using vSphere API Veeam performs full VM backups (using VeeamZIP feature in the Free edition) Backups are performed regulary using Windows Scheduled Tasks . Currently scheduled task runs every day, at 10 PM PST. Scheduled task executes a PowerShell Script Backup.ps1 located in C:\\BackupScripts\\ directory. Backup procedure looks as follows: Backups are performed by a set of PowerShell scripts Scripts are located in backups/ directory in contrail-windows-ci repository Backup.ps1 script performs the following steps: Prunes backups by removing all but last N backups N is configurable in the script Uses VeeamZIP PowerShell cmdlets to perform a full backups of VMs listed in Backup.ps1 file If an error occurs for any of the VMs listed, script continues its job. After performing backups for other VMs it terminates with an exception Each full backup is stored in a separate directory, located in \\\\winci_nfsbackups\\Backups\\ directory on backup NFS share","title":"Current backup setup"},{"location":"For CI admins/Backups/Infrastructure_backups/#vm-recovery-guide","text":"Current backup setup supports only full VM recovery. It needs to be performed, e.g. when: in case of underlying infrastructure hardware failure (e.g. corrupted disk, ESXi host failure) in case of bricking an operating system/virtual machine recovery of VM state from a certain point in time is desirable Prerequisites to backup scripts updating: Windows 10/Windows Server 2016 machine to perform the steps Credentials to winci-veeam server Please contact Windows CI team To recover a VM from backup perform the following steps: Establish a RDP connection winci-veeam server Open Veeam Backup Replication Console program is available through Start menu Veeam login window should open up ensure that localhost and port 9392 are selected and Use Windows session authentication option is selected click Connect In the upper left corner click an option Restore Select backup file to restore windows should open up Click browse In the address bar type in X:\\Backups\\ A list of directories named with date timestamps should show up Select a directory with a date, matching your desired restore point Enter this directory Select a backup file (file with .vbk extension) which has prefix matching VM to be restored Click Open Progress bar named Reading the backup file, please wait... should show up When a table named Contained VMs appears, select a VM to be restored, click Restore ; clicking Restore opens up contextual menu; click option Entire VM (including registration) After a few moments a restore wizard should show up In Virtual machine section From a table Virtual machines to restore select a VM to be restored Click Next In Restore Mode section Select Restore to the original location option Make sure Restore VM tags option is checked Make sure Quick rollback option is unchecked Click Next In Reason section Enter a reason for VM restoring into Restore reasons textbox Click Next In Summary section Verify VM details presented in Summary textbox Check Power on target virtual machine option if you want the restored VM to power on after restoration is completed Click Finish VM Restore window with progress report should appear Window can be closed by clicking Close button, this will not stop the restore procedure To open up VM Restore window again Click History button in the lower left corner of Veeam Console In the tree (in the left middle part of the Veeam Console) select Restore Full VM Restore branch A list of restore jobs should appear in the middle Double-click a job corresponding to a VM you are currently restoring VM Restore window should open up again When a restore job finishes, verify if all services provided by the machine are up and working","title":"VM recovery guide"},{"location":"For CI admins/Backups/Infrastructure_backups/#backup-scripts-update-guide","text":"Backup scripts need to be updated, e.g. when: A list of VMs requiring backup changes A bug was fixed in the backup scripts Prerequisites to backup scripts updating: Windows 10/Windows Server 2016 machine to perform the steps Credentials to winci-veeam server Please contact Windows CI team To update backup scripts perform the following steps: Clone contrail-windows-ci repository and enter backups/ directory PS C:\\Users\\user git clone https://github.com/Juniper/contrail-windows-ci.git PS C:\\Users\\user cd contrail-windows-ci\\backups PS C:\\Users\\user\\contrail-windows-ci\\backups Establish a PowerShell session with winci-veeam server PS C:\\Users\\user\\contrail-windows-ci\\backups $session = New-PSSession -ComputerName 10.84.12.29 -Credentials $(Get-Credential) Copy backups directory's contents to winci-veeam server, to C:\\BackupScripts directory PS C:\\Users\\user\\contrail-windows-ci\\backups Copy-Item .\\* C:\\BackupScripts -ToSession $session Close PowerShell session PS C:\\Users\\user\\contrail-windows-ci\\backups Remove-PSSession $session","title":"Backup scripts update guide"},{"location":"For CI admins/CI/Builder_nodes/","text":"Managing build dependencies How to check what is installed on builders Prerequisites: Checkout github.com/Juniper/contrail-windows-ci repository. Enter ansible/ directory. Prepare your ansible runner environment by going through steps described in README.md . Log into Jenkins Inspect every builder node and note its IP address Create inventory file for [builder] group, that contains their IPs: [windows:children] builder [builder] 10.84.12.A 10.84.12.B 10.84.12.C ... Check version of psget package: Run the following command: ansible builder -i inventory --vault-password-file ~/ansible-vault-key -m win_shell -a Get-Package -Name PSScriptAnalyzer | Select -Expand Version Check versions of chocolatey packages: Run the following command: ansible builder -i inventory --vault-password-file ~/ansible-vault-key -m win_shell -a choco list --local-only","title":"Managing build dependencies"},{"location":"For CI admins/CI/Builder_nodes/#managing-build-dependencies","text":"","title":"Managing build dependencies"},{"location":"For CI admins/CI/Builder_nodes/#how-to-check-what-is-installed-on-builders","text":"","title":"How to check what is installed on builders"},{"location":"For CI admins/CI/Builder_nodes/#prerequisites","text":"Checkout github.com/Juniper/contrail-windows-ci repository. Enter ansible/ directory. Prepare your ansible runner environment by going through steps described in README.md . Log into Jenkins Inspect every builder node and note its IP address Create inventory file for [builder] group, that contains their IPs: [windows:children] builder [builder] 10.84.12.A 10.84.12.B 10.84.12.C ...","title":"Prerequisites:"},{"location":"For CI admins/CI/Builder_nodes/#check-version-of-psget-package","text":"Run the following command: ansible builder -i inventory --vault-password-file ~/ansible-vault-key -m win_shell -a Get-Package -Name PSScriptAnalyzer | Select -Expand Version","title":"Check version of psget package:"},{"location":"For CI admins/CI/Builder_nodes/#check-versions-of-chocolatey-packages","text":"Run the following command: ansible builder -i inventory --vault-password-file ~/ansible-vault-key -m win_shell -a choco list --local-only","title":"Check versions of chocolatey packages:"},{"location":"For CI admins/CI/Merge_development_to_production/","text":"Merge development to production This document describes the steps required to perform a merge from development to production in Contrail Windows CI. Merge development to production Update production branch (following git operations require administrative privileges on GitHub) git fetch --all --prune git checkout development git pull git checkout production git merge development git push Update Zuul configuration Refer to Update Windows CI Zuulv2 documentation Zuul update playbook --- - hosts: zuul remote_user: ciadmin become: yes roles: - zuul Post checks Check Zuul services: zuul-merger zuul-server localhost $ ssh winci-zuulv2-production # with provided credentials; current address 10.84.12.75 systemctl status zuul-merger.service # should be active (running) systemctl status zuul-server.service # should be active (running) Trigger Zuul job Create a dummy PR on Gerrit Post recheck windows on some existing PR Future considerations Slave VM templates creation and promotion Builder/tester deployment","title":"Merge development to production"},{"location":"For CI admins/CI/Merge_development_to_production/#merge-development-to-production","text":"This document describes the steps required to perform a merge from development to production in Contrail Windows CI.","title":"Merge development to production"},{"location":"For CI admins/CI/Merge_development_to_production/#merge-development-to-production_1","text":"Update production branch (following git operations require administrative privileges on GitHub) git fetch --all --prune git checkout development git pull git checkout production git merge development git push Update Zuul configuration Refer to Update Windows CI Zuulv2 documentation","title":"Merge development to production"},{"location":"For CI admins/CI/Merge_development_to_production/#zuul-update-playbook","text":"--- - hosts: zuul remote_user: ciadmin become: yes roles: - zuul","title":"Zuul update playbook"},{"location":"For CI admins/CI/Merge_development_to_production/#post-checks","text":"Check Zuul services: zuul-merger zuul-server localhost $ ssh winci-zuulv2-production # with provided credentials; current address 10.84.12.75 systemctl status zuul-merger.service # should be active (running) systemctl status zuul-server.service # should be active (running) Trigger Zuul job Create a dummy PR on Gerrit Post recheck windows on some existing PR","title":"Post checks"},{"location":"For CI admins/CI/Merge_development_to_production/#future-considerations","text":"Slave VM templates creation and promotion Builder/tester deployment","title":"Future considerations"},{"location":"For CI admins/CI/Run_gerrit_pipeline_on_github_PR/","text":"Run gerrit pipeline on github PR When to do it: You introduce a change to github/gerrit handling code, e.g. the trigger mechanism - code that depends on Zuul or other Jenkins trigger plugins Clone Jenkins job winci-server2016-devel to some temporary tmp-devel-gerrit-check Configure tmp-devel-gerrit-check : Properties content: BRANCH_NAME={branch to test} Branches to build: {branch to test} Schedule tmp-devel-gerrit-check job manually with parameters: ZUUL_PROJECT=Juniper/contrail-controller ZUUL_BRANCH=master ZUUL_REF=None ZUUL_URL=http://10.84.12.75/merge-warrior ZUUL_UUID= (some random UUID - it will determine path on the logserver) ZUUL_CHANGE= (can be left empty) ZUUL_PATCHSET= (can be left empty) Verify that it passes (optional, recommended) Paste a link to successful build logs under your PR on github for the reviewers to verify Remove temporary tmp-devel-gerrit-check job","title":"Run gerrit pipeline on github PR"},{"location":"For CI admins/CI/Run_gerrit_pipeline_on_github_PR/#run-gerrit-pipeline-on-github-pr","text":"When to do it: You introduce a change to github/gerrit handling code, e.g. the trigger mechanism - code that depends on Zuul or other Jenkins trigger plugins","title":"Run gerrit pipeline on github PR"},{"location":"For CI admins/CI/Update_Jenkins_slaves/","text":"Updating Jenkins slaves Rolling out new slaves TODO - update templates - create templates - spawn new slaves - decomission old slaves - tags - how to test if it worked safely Updating existing slaves Some minor maintenance work does not require rollout of new slaves. An example might be an upgrade of some library or other dependency. In this case, one can update the running cluster without having to create a new template and rolling out every machine. However, the template must be updated at the end of the procedure, so that future fresh machines will also have the required changes. The upgrade procedure consists of: 1. Manually upgrade a subset of nodes 1. Test if CI job pass on the new set of nodes 1. Upgrade the rest of the nodes 1. Manually upgrade a subset of nodes Select one Jenkins slave to use. Make sure that there are other slaves that perform the same function (they have the same tags and are up). In Jenkins UI, press Mark this node as temporarily offline , providing the reason. Note the IP address of the slave. Manually run ansible against the node. NOTE: this is a one-off role. It doesn't necessarily need to be commited to version control. Therefore, any \"hacks\" are allowed. For example, to upgrade a chocolatey package, in addition to updating the version parameter of win_chocolatey role, one must also run an additional task of uninstalling the package first. See examples below. Checkout github.com/Juniper/contrail-windows-ci repository. Enter ansible/ directory. Prepare your environment by going through steps described in README.md . Find the ansible role that is responsible for the upgrade. Modify the role to suit your needs. If modification could cause a slave restart, manually disable auto start of jenkins_swarm_client : powershell Stop-Service jenkins_swarm_client Set-Service jenkins_swarm_client -StartupType Manual NOTE : jenkins_swarm_client must be disabled, since a reboot removes the Mark this node as temporarily offline status from the slave. After the update procedure completes, you should re-enable this service to reconnect the builder to Jenkins. Add a tag to every task that needs to be executed. Prepare inventory file. Edit file inventory.devel/groups . Add IP of your selected slave under the correct group. Create one-off playbook. Create an yml file in the root of ansible/ dir. Specify hosts and roles to be executed. Run the playbook, specifying: ansible vault key location, inventory, task tags to execute, prepared playbook. Test if the change worked. Remote into the machine and verify if change was successful. Test if job passes. In Jenkins, go into the selected node view. Remove all tags it has. Add some temporary tag (e.g. 'temp-upgrade'). Open a test pull request to github.com/Juniper/contrail-windows-ci In its Jenksinfile, replace tags of nodes that you removed on t he node with the temporary tag. Open the pull request with \"do not merge\" in the title. Wait for CI to be triggered normally. Wait for it to pass. If autostart of jenkins_swarm_client was disabled: powershell Set-Service jenkins_swarm_client -StartupType Automatic Start-Service jenkins_swarm_client NOTE: the following steps may change depending on case-by-case basic. Open a normal pull request with cleaned up changes to ansible roles. Get it merged. Create a new builder template. (see this ) Rollout the change to other slaves by repeating steps 1-2 but specifying larger subsets of nodes. Examples. Upgrade golang version. ansible/roles/builder/tasks/main.yml: ... - name: Uninstall golang win_chocolatey: name: golang state: absent tags: bump - name: Install golang win_chocolatey: name: golang version: 1.10.0 state: present tags: bump ... ansible/mysite.yml --- - name: 'bump golang' hosts: - builder roles: - builder inventory.devel/groups: ... [builder] 10.84.12.87 ... Command to run: ansible-playbook -i inventory.devel --vault-password-file ~/.ansible-vault --tags bump mysite.yml","title":"Updating Jenkins slaves"},{"location":"For CI admins/CI/Update_Jenkins_slaves/#updating-jenkins-slaves","text":"","title":"Updating Jenkins slaves"},{"location":"For CI admins/CI/Update_Jenkins_slaves/#rolling-out-new-slaves","text":"TODO - update templates - create templates - spawn new slaves - decomission old slaves - tags - how to test if it worked safely","title":"Rolling out new slaves"},{"location":"For CI admins/CI/Update_Jenkins_slaves/#updating-existing-slaves","text":"Some minor maintenance work does not require rollout of new slaves. An example might be an upgrade of some library or other dependency. In this case, one can update the running cluster without having to create a new template and rolling out every machine. However, the template must be updated at the end of the procedure, so that future fresh machines will also have the required changes. The upgrade procedure consists of: 1. Manually upgrade a subset of nodes 1. Test if CI job pass on the new set of nodes 1. Upgrade the rest of the nodes","title":"Updating existing slaves"},{"location":"For CI admins/CI/Update_Jenkins_slaves/#1-manually-upgrade-a-subset-of-nodes","text":"Select one Jenkins slave to use. Make sure that there are other slaves that perform the same function (they have the same tags and are up). In Jenkins UI, press Mark this node as temporarily offline , providing the reason. Note the IP address of the slave. Manually run ansible against the node. NOTE: this is a one-off role. It doesn't necessarily need to be commited to version control. Therefore, any \"hacks\" are allowed. For example, to upgrade a chocolatey package, in addition to updating the version parameter of win_chocolatey role, one must also run an additional task of uninstalling the package first. See examples below. Checkout github.com/Juniper/contrail-windows-ci repository. Enter ansible/ directory. Prepare your environment by going through steps described in README.md . Find the ansible role that is responsible for the upgrade. Modify the role to suit your needs. If modification could cause a slave restart, manually disable auto start of jenkins_swarm_client : powershell Stop-Service jenkins_swarm_client Set-Service jenkins_swarm_client -StartupType Manual NOTE : jenkins_swarm_client must be disabled, since a reboot removes the Mark this node as temporarily offline status from the slave. After the update procedure completes, you should re-enable this service to reconnect the builder to Jenkins. Add a tag to every task that needs to be executed. Prepare inventory file. Edit file inventory.devel/groups . Add IP of your selected slave under the correct group. Create one-off playbook. Create an yml file in the root of ansible/ dir. Specify hosts and roles to be executed. Run the playbook, specifying: ansible vault key location, inventory, task tags to execute, prepared playbook. Test if the change worked. Remote into the machine and verify if change was successful. Test if job passes. In Jenkins, go into the selected node view. Remove all tags it has. Add some temporary tag (e.g. 'temp-upgrade'). Open a test pull request to github.com/Juniper/contrail-windows-ci In its Jenksinfile, replace tags of nodes that you removed on t he node with the temporary tag. Open the pull request with \"do not merge\" in the title. Wait for CI to be triggered normally. Wait for it to pass. If autostart of jenkins_swarm_client was disabled: powershell Set-Service jenkins_swarm_client -StartupType Automatic Start-Service jenkins_swarm_client NOTE: the following steps may change depending on case-by-case basic. Open a normal pull request with cleaned up changes to ansible roles. Get it merged. Create a new builder template. (see this ) Rollout the change to other slaves by repeating steps 1-2 but specifying larger subsets of nodes.","title":"1. Manually upgrade a subset of nodes"},{"location":"For CI admins/CI/Update_Jenkins_slaves/#examples","text":"Upgrade golang version. ansible/roles/builder/tasks/main.yml: ... - name: Uninstall golang win_chocolatey: name: golang state: absent tags: bump - name: Install golang win_chocolatey: name: golang version: 1.10.0 state: present tags: bump ... ansible/mysite.yml --- - name: 'bump golang' hosts: - builder roles: - builder inventory.devel/groups: ... [builder] 10.84.12.87 ... Command to run: ansible-playbook -i inventory.devel --vault-password-file ~/.ansible-vault --tags bump mysite.yml","title":"Examples."},{"location":"For CI admins/CI/Update_Zuul/","text":"Update Windows CI Zuulv2 This document describes a procedure required to update Windows CI Zuulv2 configuration. This procedure updates configuration using development or production branch from contrail-windows-ci repository. Prerequisites Ubuntu 16.04 or Windows Subsystem for Linux with Ubuntu It will serve as Ansible control machine User's public SSH key installed on Zuulv2 instance Please contact Windows CI team Access to ansible vault key Please contact Windows CI team Steps Install required apt dependencies on Ansible control machine apt-get install git python3 python3-pip python3-virtualenv Clone contrail-windows-ci repository cd ~/ git clone https://github.com/Juniper/contrail-windows-ci.git cd contrail-windows-ci Checkout branch that you want to use for configuring Zuul. For example: to checkout development branch: git checkout development to checkout production branch: git fetch --all git branch production origin/production git checkout production Verify that development (or production ) branch contains PRs with required changes to Zuul configuration Assuming PR#2 and PR#1 are required PRs, run the following command: git log --oneline Output will contain a list of merged PRs, from newest to oldest. Required PRs should be at the top: abcdabc PR#2 abcd123 PR#1 e8691f5 Some other PR 3af841e Some other PR, part 2 # ... omitted Example Assume that add contrail-infra-doc to gerrit (#212) is a latest PR that was merged and this change has to be applied on Zuul Then output of git log should look like this: 93c783b add contrail-infra-doc to gerrit (#212) 1f86f26 zuul: setup-zuul-server playbook; variables in prod inventory (#210) # ... omitted Move to ansible directory cd ansible Create a virtualenv and install pip dependencies python3 -m virtualenv -p /usr/bin/python3 venv source venv/bin/activate pip install -r python-requirements.txt Create a file for Ansible vault key and populate it with the key touch ~/ansible-vault-key vim ~/ansible-vault-key # enter ansible vault key into a file Test connection to Zuul with Ansible ansible -i inventory.prod --private-key=YOUR_PRIVATE_KEY zuul -m ping where YOUR_PRIVATE_KEY is a path to your SSH private key Run setup-zuul-server.yml playbook ansible-playbook -i inventory.prod --private-key=YOUR_PRIVATE_KEY setup-zuul-server.yml Verify that the run completed successfully. The output should be following and failed task count must equal zero. # # task list omitted for brevity # PLAY RECAP ******************************************************************************** ********** 10.84.12.75 : ok=22 changed=2 unreachable=0 failed=0 # # ... - task run time omitted for brevity # failed task count should be equal to zero","title":"Update Windows CI Zuulv2"},{"location":"For CI admins/CI/Update_Zuul/#update-windows-ci-zuulv2","text":"This document describes a procedure required to update Windows CI Zuulv2 configuration. This procedure updates configuration using development or production branch from contrail-windows-ci repository.","title":"Update Windows CI Zuulv2"},{"location":"For CI admins/CI/Update_Zuul/#prerequisites","text":"Ubuntu 16.04 or Windows Subsystem for Linux with Ubuntu It will serve as Ansible control machine User's public SSH key installed on Zuulv2 instance Please contact Windows CI team Access to ansible vault key Please contact Windows CI team","title":"Prerequisites"},{"location":"For CI admins/CI/Update_Zuul/#steps","text":"Install required apt dependencies on Ansible control machine apt-get install git python3 python3-pip python3-virtualenv Clone contrail-windows-ci repository cd ~/ git clone https://github.com/Juniper/contrail-windows-ci.git cd contrail-windows-ci Checkout branch that you want to use for configuring Zuul. For example: to checkout development branch: git checkout development to checkout production branch: git fetch --all git branch production origin/production git checkout production Verify that development (or production ) branch contains PRs with required changes to Zuul configuration Assuming PR#2 and PR#1 are required PRs, run the following command: git log --oneline Output will contain a list of merged PRs, from newest to oldest. Required PRs should be at the top: abcdabc PR#2 abcd123 PR#1 e8691f5 Some other PR 3af841e Some other PR, part 2 # ... omitted Example Assume that add contrail-infra-doc to gerrit (#212) is a latest PR that was merged and this change has to be applied on Zuul Then output of git log should look like this: 93c783b add contrail-infra-doc to gerrit (#212) 1f86f26 zuul: setup-zuul-server playbook; variables in prod inventory (#210) # ... omitted Move to ansible directory cd ansible Create a virtualenv and install pip dependencies python3 -m virtualenv -p /usr/bin/python3 venv source venv/bin/activate pip install -r python-requirements.txt Create a file for Ansible vault key and populate it with the key touch ~/ansible-vault-key vim ~/ansible-vault-key # enter ansible vault key into a file Test connection to Zuul with Ansible ansible -i inventory.prod --private-key=YOUR_PRIVATE_KEY zuul -m ping where YOUR_PRIVATE_KEY is a path to your SSH private key Run setup-zuul-server.yml playbook ansible-playbook -i inventory.prod --private-key=YOUR_PRIVATE_KEY setup-zuul-server.yml Verify that the run completed successfully. The output should be following and failed task count must equal zero. # # task list omitted for brevity # PLAY RECAP ******************************************************************************** ********** 10.84.12.75 : ok=22 changed=2 unreachable=0 failed=0 # # ... - task run time omitted for brevity # failed task count should be equal to zero","title":"Steps"},{"location":"For CI admins/CI/contrail_deployment_in_ci/","text":"Contrail deployment in Windows CI Environment Playbooks provision_instances.yml - creates server instances (e.g. on AWS) not used in Contrail Windows CI configure_instances.yml - installs dependencies installs and starts docker install_contrail.yml - installs Contrail and installs orchestrator (optional) Configuration Orchestrators: None OpenStack VCenter Contrail version and kolla version: ocata-master-91 and ocata Docker registry ci-repo.englab.juniper.net or opencontrailnigthly Interfaces Example config Windows-CI config Kolla Kolla == production-ready containers and deployment tools for operating OpenStack contrail-ansible-deployer internally clones Juniper/contrail-kolla-ansible (branch: contrail/ocata) Bugs which may affect us can be introduced in contrail-ansible-deployer as well as in contrail-kolla-ansible Base image CentOS 7.4 (kernel = 3.10.0-693.17.1) Two interfaces: Control Plane Data Plane IP address is required on every interface - can be static More information: https://github.com/Juniper/contrail-ansible-deployer#prerequisites Windows-CI Troubleshooting Try to run playbook locally from laptop Try to look for similar issue on contra-cloud.slack channel: contrail-ansible Check recent commits in contrail-ansible-deployer and contrail-kolla-deployer Ask our devops team Ask on slack channel Zuul V3 Controller image from pull request Correct version of contrail-ansible-deployer Correct version of contrail-kolla-deployer Enable voting on contrail-ansible-deployer and contrail-kolla-deployer","title":"Contrail deployment in Windows CI"},{"location":"For CI admins/CI/contrail_deployment_in_ci/#contrail-deployment-in-windows-ci","text":"","title":"Contrail deployment in Windows CI"},{"location":"For CI admins/CI/contrail_deployment_in_ci/#environment","text":"","title":"Environment"},{"location":"For CI admins/CI/contrail_deployment_in_ci/#playbooks","text":"provision_instances.yml - creates server instances (e.g. on AWS) not used in Contrail Windows CI configure_instances.yml - installs dependencies installs and starts docker install_contrail.yml - installs Contrail and installs orchestrator (optional)","title":"Playbooks"},{"location":"For CI admins/CI/contrail_deployment_in_ci/#configuration","text":"Orchestrators: None OpenStack VCenter Contrail version and kolla version: ocata-master-91 and ocata Docker registry ci-repo.englab.juniper.net or opencontrailnigthly Interfaces Example config Windows-CI config","title":"Configuration"},{"location":"For CI admins/CI/contrail_deployment_in_ci/#kolla","text":"Kolla == production-ready containers and deployment tools for operating OpenStack contrail-ansible-deployer internally clones Juniper/contrail-kolla-ansible (branch: contrail/ocata) Bugs which may affect us can be introduced in contrail-ansible-deployer as well as in contrail-kolla-ansible","title":"Kolla"},{"location":"For CI admins/CI/contrail_deployment_in_ci/#base-image","text":"CentOS 7.4 (kernel = 3.10.0-693.17.1) Two interfaces: Control Plane Data Plane IP address is required on every interface - can be static More information: https://github.com/Juniper/contrail-ansible-deployer#prerequisites","title":"Base image"},{"location":"For CI admins/CI/contrail_deployment_in_ci/#windows-ci","text":"","title":"Windows-CI"},{"location":"For CI admins/CI/contrail_deployment_in_ci/#troubleshooting","text":"Try to run playbook locally from laptop Try to look for similar issue on contra-cloud.slack channel: contrail-ansible Check recent commits in contrail-ansible-deployer and contrail-kolla-deployer Ask our devops team Ask on slack channel","title":"Troubleshooting"},{"location":"For CI admins/CI/contrail_deployment_in_ci/#zuul-v3","text":"Controller image from pull request Correct version of contrail-ansible-deployer Correct version of contrail-kolla-deployer Enable voting on contrail-ansible-deployer and contrail-kolla-deployer","title":"Zuul V3"},{"location":"For CI admins/Infra/List_of_VMs_in_Windows_CI/","text":"List of all important VMs in Windows CI VMware: ci-vc - vCenter Server on Windows CRITICAL ci-vc-um - vCenter Update Manager Base OS templates: Windows - Install OS (manually) + instruction (manually) Preferrably: automate CentOS - Prepared manually, no instruction TODO: Instruction with requirements Requirements: root access SSH (password-based) 2 NICs TODO: Verify if there are no more requirements Ubuntu - No template, required to provision Ubuntu-based VMs TODO: Instruction/automation Requirements: ciadmin user (SSH accessible, password-based) python3 1 NIC TODO: Verify if there are no more requirements CI templates: builder - Ansible, Jenkins job Requires Windows template tester - Ansible, Jenkins job Requires Windows template testbed - Ansible Requires Windows template Infra: builders - Ansible, Jenkins job testers - Ansible, Jenkins job mgmt-dhcp - Created manually, DHCP server + configuration (172.17.0.0/24) CRITICAL impact if down: will cause failure in all production CI jobs due to lack of dhcp for testbed machines will not affect demo-env, dev-env and other machines in public network fix cost: Automate deployment Before automation, backup VMware HA winci-drive - Created manually, network drive with (probably) temporary content impact if down: no impact on production CI degrades debuggability of CI (cannot upload or use ready artifacts) impacts ability to create builder templates fix cost: will need to recreate directory structure and copy contents from one of the builders this process is not documented to this degree of detail (no list of dependencies) Before automation, backup VMware HA winci-jenkins - Created manually, plugins + configuration CRITICAL impact if down: ABSOLUTELY CRITICAL fix cost: BACKUP job configuration job logs logs server ssh keys jenkins plugin list (initial version on contrail-windows-ci; need to check if works) credentials Automate configuration VMware HA winci-mgmt - Created manually, Python requirements.txt have to be installed CRITICAL impact if down: ABSOLUTELY CRITICAL (cannot create testenvs) fix cost: Automate provisioning Ubuntu configured for Ansible Ubuntu configured for Python tests Ubuntu configured for monitoring scripts Backup ansible-vault-key logs server ssh keys github deploy ssh key VMware HA winci-registry \u2013 Created manually @ 10.84.12.27 \u2013 Docker registry with kolla and contrail-openstack-* images from ci-repo.englab.juniper.net:5000 . We're mirroring, because we may use some older snapshot of images, that are already removed from ci-repo registry. impact if down: ABSOLUTELY CRITICAL (cannot create testenvs) In some cases we can switch back to ci-repo. fix cost: Manual provisioning using provisioning manual Backup Images removed from ci-repo may be unavailable, another registry? VMware HA winci-vyos-mgmt CRITICAL impact if down: ABSOLUTELY CRITICAL (cannot deploy contrail controllers) fix cost: Backup Document configuration VMware HA winci-zuulv2-production - Created manually with Ansible script CRITICAL impact if down: ABSOLUTELY CRITICAL (cannot run jobs) fix cost: Already automated Create prod and dev inventories for Zuul Backup: gerrit ssh keys jenkins ssh keys VMware HA winci-purgatory - Probably created manually? What is installed there? CRITICAL impact if down: ABSOLUTELY CRITICAL (cannot run jobs) fix cost: pending PR for automation Hosts: esxi1-5 fix cost: VMware HA on critical virtual machines backup konfiguracji SSD/SATA drivers fix cost: RAID on physical servers: Not possible due to lack of hardware support If not RAID, then shared storage for critical VMs is needed Preferrably some storage array with RAID configured NICs we lack redundancy in case of main NIC failure Backup: Refer to Infrastructure Backups","title":"List of all important VMs in Windows CI"},{"location":"For CI admins/Infra/List_of_VMs_in_Windows_CI/#list-of-all-important-vms-in-windows-ci","text":"VMware: ci-vc - vCenter Server on Windows CRITICAL ci-vc-um - vCenter Update Manager Base OS templates: Windows - Install OS (manually) + instruction (manually) Preferrably: automate CentOS - Prepared manually, no instruction TODO: Instruction with requirements Requirements: root access SSH (password-based) 2 NICs TODO: Verify if there are no more requirements Ubuntu - No template, required to provision Ubuntu-based VMs TODO: Instruction/automation Requirements: ciadmin user (SSH accessible, password-based) python3 1 NIC TODO: Verify if there are no more requirements CI templates: builder - Ansible, Jenkins job Requires Windows template tester - Ansible, Jenkins job Requires Windows template testbed - Ansible Requires Windows template Infra: builders - Ansible, Jenkins job testers - Ansible, Jenkins job mgmt-dhcp - Created manually, DHCP server + configuration (172.17.0.0/24) CRITICAL impact if down: will cause failure in all production CI jobs due to lack of dhcp for testbed machines will not affect demo-env, dev-env and other machines in public network fix cost: Automate deployment Before automation, backup VMware HA winci-drive - Created manually, network drive with (probably) temporary content impact if down: no impact on production CI degrades debuggability of CI (cannot upload or use ready artifacts) impacts ability to create builder templates fix cost: will need to recreate directory structure and copy contents from one of the builders this process is not documented to this degree of detail (no list of dependencies) Before automation, backup VMware HA winci-jenkins - Created manually, plugins + configuration CRITICAL impact if down: ABSOLUTELY CRITICAL fix cost: BACKUP job configuration job logs logs server ssh keys jenkins plugin list (initial version on contrail-windows-ci; need to check if works) credentials Automate configuration VMware HA winci-mgmt - Created manually, Python requirements.txt have to be installed CRITICAL impact if down: ABSOLUTELY CRITICAL (cannot create testenvs) fix cost: Automate provisioning Ubuntu configured for Ansible Ubuntu configured for Python tests Ubuntu configured for monitoring scripts Backup ansible-vault-key logs server ssh keys github deploy ssh key VMware HA winci-registry \u2013 Created manually @ 10.84.12.27 \u2013 Docker registry with kolla and contrail-openstack-* images from ci-repo.englab.juniper.net:5000 . We're mirroring, because we may use some older snapshot of images, that are already removed from ci-repo registry. impact if down: ABSOLUTELY CRITICAL (cannot create testenvs) In some cases we can switch back to ci-repo. fix cost: Manual provisioning using provisioning manual Backup Images removed from ci-repo may be unavailable, another registry? VMware HA winci-vyos-mgmt CRITICAL impact if down: ABSOLUTELY CRITICAL (cannot deploy contrail controllers) fix cost: Backup Document configuration VMware HA winci-zuulv2-production - Created manually with Ansible script CRITICAL impact if down: ABSOLUTELY CRITICAL (cannot run jobs) fix cost: Already automated Create prod and dev inventories for Zuul Backup: gerrit ssh keys jenkins ssh keys VMware HA winci-purgatory - Probably created manually? What is installed there? CRITICAL impact if down: ABSOLUTELY CRITICAL (cannot run jobs) fix cost: pending PR for automation Hosts: esxi1-5 fix cost: VMware HA on critical virtual machines backup konfiguracji SSD/SATA drivers fix cost: RAID on physical servers: Not possible due to lack of hardware support If not RAID, then shared storage for critical VMs is needed Preferrably some storage array with RAID configured NICs we lack redundancy in case of main NIC failure Backup: Refer to Infrastructure Backups","title":"List of all important VMs in Windows CI"},{"location":"For CI admins/Infra/Windows_CI_infrastructure/","text":"Windows CI infrastructure diagram","title":"Windows CI infrastructure diagram"},{"location":"For CI admins/Infra/Windows_CI_infrastructure/#windows-ci-infrastructure-diagram","text":"","title":"Windows CI infrastructure diagram"},{"location":"For CI admins/Infra/Docker registry/Docker_registry_provisioning/","text":"Docker registry provisioning Note. This document describes provisioning procedure. If you want just to update the images, see the updating manual . To provision a docker registry, you will: Prepare the machine Install Docker Start the registry Pull and mirror the images The following sections describe these steps in details. Preparing the machine Spawn a VM in VMWare using the CentOS / Ubuntu image as template (the rest of instruction will assume (CentOS 7). Choose the machine name winci-registry When asked about which customization to use, select DHCP. Log in using ssh and set the hostname: Edit the file /etc/hostname and type the same name as used in step 1. Update the system using yum upgrade and reboot. Set up static IP : Log in using ssh and change the IP configuration to manual, by modifying the following file: /etc/sysconfig/network-scripts/ifcfg-ens192 : Change the line with BOOTPROTO to BOOTPROTO=static and add the following lines at the end of that file: IPADDR=10.84.12.27 NETMASK=255.255.255.0 GATEWAY=10.84.12.254 DNS1=172.21.200.60 DNS2=172.29.131.60 DNS3=8.8.8.8 DOMAIN=\"contrail.juniper.net englab.juniper.net juniper.net jnpr.net\" This IP is used by the controller role in our CI, as a docker_registry parameter Then reboot the machine and log in using the new IP. Installing Docker Install Docker using package manager yum install docker systemctl enable docker systemctl start docker Allow the ci-repo (from which we will be pulling the images) to use HTTP: Add this line to /etc/docker/daemon.json (if the file is not there, create it): { \"insecure-registries\":[\"ci-repo.englab.juniper.net:5000\"] }{} Restart docker to apply changes systemctl restart docker Starting the registry Starting the registry Start the registry container: docker run -d -p 5000:5000 --restart=always --name registry registry:2 The container with the registry is set to autostart, so no further configuration is needed. Pulling and mirroring the images Refer to the registry update manual to pull and start mirroring the images. Possible improvements Perhaps we can also try mirroring base images for speed. References Deploy a registry server \u2014 Docker Documentation","title":"Docker registry provisioning"},{"location":"For CI admins/Infra/Docker registry/Docker_registry_provisioning/#docker-registry-provisioning","text":"Note. This document describes provisioning procedure. If you want just to update the images, see the updating manual . To provision a docker registry, you will: Prepare the machine Install Docker Start the registry Pull and mirror the images The following sections describe these steps in details.","title":"Docker registry provisioning"},{"location":"For CI admins/Infra/Docker registry/Docker_registry_provisioning/#preparing-the-machine","text":"Spawn a VM in VMWare using the CentOS / Ubuntu image as template (the rest of instruction will assume (CentOS 7). Choose the machine name winci-registry When asked about which customization to use, select DHCP. Log in using ssh and set the hostname: Edit the file /etc/hostname and type the same name as used in step 1. Update the system using yum upgrade and reboot. Set up static IP : Log in using ssh and change the IP configuration to manual, by modifying the following file: /etc/sysconfig/network-scripts/ifcfg-ens192 : Change the line with BOOTPROTO to BOOTPROTO=static and add the following lines at the end of that file: IPADDR=10.84.12.27 NETMASK=255.255.255.0 GATEWAY=10.84.12.254 DNS1=172.21.200.60 DNS2=172.29.131.60 DNS3=8.8.8.8 DOMAIN=\"contrail.juniper.net englab.juniper.net juniper.net jnpr.net\" This IP is used by the controller role in our CI, as a docker_registry parameter Then reboot the machine and log in using the new IP.","title":"Preparing the machine"},{"location":"For CI admins/Infra/Docker registry/Docker_registry_provisioning/#installing-docker","text":"Install Docker using package manager yum install docker systemctl enable docker systemctl start docker Allow the ci-repo (from which we will be pulling the images) to use HTTP: Add this line to /etc/docker/daemon.json (if the file is not there, create it): { \"insecure-registries\":[\"ci-repo.englab.juniper.net:5000\"] }{} Restart docker to apply changes systemctl restart docker","title":"Installing Docker"},{"location":"For CI admins/Infra/Docker registry/Docker_registry_provisioning/#starting-the-registry","text":"","title":"Starting the registry"},{"location":"For CI admins/Infra/Docker registry/Docker_registry_provisioning/#starting-the-registry_1","text":"Start the registry container: docker run -d -p 5000:5000 --restart=always --name registry registry:2 The container with the registry is set to autostart, so no further configuration is needed.","title":"Starting the registry"},{"location":"For CI admins/Infra/Docker registry/Docker_registry_provisioning/#pulling-and-mirroring-the-images","text":"Refer to the registry update manual to pull and start mirroring the images.","title":"Pulling and mirroring the images"},{"location":"For CI admins/Infra/Docker registry/Docker_registry_provisioning/#possible-improvements","text":"Perhaps we can also try mirroring base images for speed.","title":"Possible improvements"},{"location":"For CI admins/Infra/Docker registry/Docker_registry_provisioning/#references","text":"Deploy a registry server \u2014 Docker Documentation","title":"References"},{"location":"For CI admins/Infra/Docker registry/Update_private_docker_registry/","text":"Updating private docker registry When is the update needed? What need to be updated? The update of local docker registry is required when the version of Controller used in Windows CI is updated to a newer one. All Contrail images required by Contrail Ansible Deployer running in Windows CI have to be updated to version matching selected Controller version. Some new cached Openstack images may be required by updated Contrail Ansible Deployer. How to update How to update Contrail images Choose Contrail version tag you want to be cached in private docker registry: all available tags can be found here . Log into winci-registry : please contact Windows CI team for access. Check if there is update-docker-registry.py script in root directory. If there is no such script: download it from here : curl https://raw.githubusercontent.com/Juniper/contrail-windows-ci/development/utility/update_docker_registry/update-docker-registry.py --output update-docker-registry.py make it executable: chmod +x ./update-docker-registry.py Run the script with Contrail version tag as parameter, for example: ./update-docker-registry.py ocata-master-206 If new version of Contrail Ansible Deployer uses more images than listed in update-docker-registry.py script: update images list in the script locally, open up a PR with updated images list to contrail-windows-ci. How to update Openstack images New version of Contrail Ansible Deployer may also use some new Openstack images. Currently there is no script that does this automatically. For caching Openstack images manually please do the folowing: Log into winci-registry : please contact Windows CI team for access. Set the name of required image in local variable (replace IMAGE-NAME with proper value: image_name= IMAGE-NAME Pull the image: docker image pull ci-repo.englab.juniper.net:5000/kolla/${image_name}:ocata Get the Image ID of previously pulled image: image_id=$(docker image ls | grep kolla | grep ${image_name} | awk '{ print $3 }' | uniq) verify the ID: echo \"$image_id\" Tag the image: docker tag ${image_id} localhost:5000/kolla/${image_name}:ocata Push the image to local repository: docker push localhost:5000/kolla/${image_name}:ocata Repeat steps 2-6 for all other required images. References Deploy a registry server \u2014 Docker Documentation","title":"Updating private docker registry"},{"location":"For CI admins/Infra/Docker registry/Update_private_docker_registry/#updating-private-docker-registry","text":"","title":"Updating private docker registry"},{"location":"For CI admins/Infra/Docker registry/Update_private_docker_registry/#when-is-the-update-needed-what-need-to-be-updated","text":"The update of local docker registry is required when the version of Controller used in Windows CI is updated to a newer one. All Contrail images required by Contrail Ansible Deployer running in Windows CI have to be updated to version matching selected Controller version. Some new cached Openstack images may be required by updated Contrail Ansible Deployer.","title":"When is the update needed? What need to be updated?"},{"location":"For CI admins/Infra/Docker registry/Update_private_docker_registry/#how-to-update","text":"","title":"How to update"},{"location":"For CI admins/Infra/Docker registry/Update_private_docker_registry/#how-to-update-contrail-images","text":"Choose Contrail version tag you want to be cached in private docker registry: all available tags can be found here . Log into winci-registry : please contact Windows CI team for access. Check if there is update-docker-registry.py script in root directory. If there is no such script: download it from here : curl https://raw.githubusercontent.com/Juniper/contrail-windows-ci/development/utility/update_docker_registry/update-docker-registry.py --output update-docker-registry.py make it executable: chmod +x ./update-docker-registry.py Run the script with Contrail version tag as parameter, for example: ./update-docker-registry.py ocata-master-206 If new version of Contrail Ansible Deployer uses more images than listed in update-docker-registry.py script: update images list in the script locally, open up a PR with updated images list to contrail-windows-ci.","title":"How to update Contrail images"},{"location":"For CI admins/Infra/Docker registry/Update_private_docker_registry/#how-to-update-openstack-images","text":"New version of Contrail Ansible Deployer may also use some new Openstack images. Currently there is no script that does this automatically. For caching Openstack images manually please do the folowing: Log into winci-registry : please contact Windows CI team for access. Set the name of required image in local variable (replace IMAGE-NAME with proper value: image_name= IMAGE-NAME Pull the image: docker image pull ci-repo.englab.juniper.net:5000/kolla/${image_name}:ocata Get the Image ID of previously pulled image: image_id=$(docker image ls | grep kolla | grep ${image_name} | awk '{ print $3 }' | uniq) verify the ID: echo \"$image_id\" Tag the image: docker tag ${image_id} localhost:5000/kolla/${image_name}:ocata Push the image to local repository: docker push localhost:5000/kolla/${image_name}:ocata Repeat steps 2-6 for all other required images.","title":"How to update Openstack images"},{"location":"For CI admins/Infra/Docker registry/Update_private_docker_registry/#references","text":"Deploy a registry server \u2014 Docker Documentation","title":"References"},{"location":"For CI admins/Infra/VMWare cluster/Prepare_Host_for_Maintenance/","text":"Prepare an ESXi Host for Maintenance This document describes steps required to prepare an ESXi host for the maintenance. Examples of such maintenance include: Replacing broken/old hard drives. Reinstalling ESXi system. Prerequisites Requirements for CI admin: Access to Contrail Windows CI infrastructure. Credentials to Contrail Windows CI VMware cluster. Browser with Flash support, preferably Chrome. Before proceeding with the following steps take note of: ESXi host hostname ESXi local datastore list In this document, the ESXi host being prepared for the maintenance will be referred to as maintaned host . Steps Open the vSphere Web Client and login using credentials provided by Contrail Windows CI team. In Navigator pane select Storage tab. Expand WinCI-Datastores-SSD datastore cluster. Identify local datastores of maintained host. For each such datastore perform: Right click on the datastore. Click Move To... option. In the Move To... window, select CI-DC . Click Ok button. Perform the previous step for other datastore clusters: WinCI-Datastores-SATA WinCI-Datastores-Infra NOTE: Testbeds in CI are spawned on datastores from WinCI-Datastores-SSD datastore cluster. Removing host's datastores from it denies new testbeds from being spawned on this host. In Navigator pane select Hosts and Clusters tab. Expand CI-DC datacenter entry. Expand WinCI cluster entry. Find the maintained host entry click on it. Migrate virtual machines off the maintained host. Left click on the host entry in the Navigator pane. In the main pane select VMs tab. For each VM in the table perform the following steps: Right click on the VM. Select Migrate... option. Migrate wizard should open up. In Select the migration type step: Select Change both compute resource and storage option. Select Select compute resource first option. Click Next button. In Select a compute resource step: Expand CI-DC datacenter entry. Expand WinCI cluster entry. Select a host different than maintained host. Click Next button. In Select storage step: Select a datastore local to host selected in the previous step. Click Next button. In Select networks step: Verify that Source Network matches Destination Network in the presented table. Click Next button. In Select vMotion priority step: Select Schedule vMotion with high priority option. Click Next button. In Ready to complete step: Click Finish button. Migrate templates. Left click on the host entry in the Navigator pane. 47 - In the main pane select VMs tab. In the main pane select VMs tab. Below VMs tab, select VM Templates if Folders . For each template perform the following steps: Right click on the template. Select Convert to Virtual Machine... option. Convert Template to Virtual Machine wizard should open up: In Select a compute resource step: Click on WinCI cluster object. In the bottom of the windows, you should see Compatibility checks succeeded . Click Next button. In Ready to Complete step: Click Finish button. Change screen from VM Templates in Folders to Virtual Machines . If the template is listed on the VM list: Perform the same migration steps as for virtual machines from step 11. If the template is not listed: It probably was already located on shared storage and VMware reassigned it to the different host. To confirm search for this template in VMs and Templates tab, in Navigator pane. Right click on this relocated/migrated VM and select Template Convert to Template option. In the Confirm Convert windows, click Yes button. NOTE : testbed templates should stay on NFS-Datastore , since template location on shared storage is a requirement of linked clones. NOTE : If enough space is free on NFS-Datastore , latest templates should be located on it. Switch host to the maintenance mode. Right click on the host entry in the Navigator pane. Select Maintenance Mode Enter Maintenance Mode option. In the Confirm Maintenance Mode window, uncheck Move powered-off and suspended virtual machines to other hosts in the cluster option. Click OK button. Move host out of the WinCI cluster. Right click on the host entry in the Navigator pane. Select Move To... option. Click on CI-DC datacenter object. Click OK button.","title":"Prepare an ESXi Host for Maintenance"},{"location":"For CI admins/Infra/VMWare cluster/Prepare_Host_for_Maintenance/#prepare-an-esxi-host-for-maintenance","text":"This document describes steps required to prepare an ESXi host for the maintenance. Examples of such maintenance include: Replacing broken/old hard drives. Reinstalling ESXi system.","title":"Prepare an ESXi Host for Maintenance"},{"location":"For CI admins/Infra/VMWare cluster/Prepare_Host_for_Maintenance/#prerequisites","text":"Requirements for CI admin: Access to Contrail Windows CI infrastructure. Credentials to Contrail Windows CI VMware cluster. Browser with Flash support, preferably Chrome. Before proceeding with the following steps take note of: ESXi host hostname ESXi local datastore list In this document, the ESXi host being prepared for the maintenance will be referred to as maintaned host .","title":"Prerequisites"},{"location":"For CI admins/Infra/VMWare cluster/Prepare_Host_for_Maintenance/#steps","text":"Open the vSphere Web Client and login using credentials provided by Contrail Windows CI team. In Navigator pane select Storage tab. Expand WinCI-Datastores-SSD datastore cluster. Identify local datastores of maintained host. For each such datastore perform: Right click on the datastore. Click Move To... option. In the Move To... window, select CI-DC . Click Ok button. Perform the previous step for other datastore clusters: WinCI-Datastores-SATA WinCI-Datastores-Infra NOTE: Testbeds in CI are spawned on datastores from WinCI-Datastores-SSD datastore cluster. Removing host's datastores from it denies new testbeds from being spawned on this host. In Navigator pane select Hosts and Clusters tab. Expand CI-DC datacenter entry. Expand WinCI cluster entry. Find the maintained host entry click on it. Migrate virtual machines off the maintained host. Left click on the host entry in the Navigator pane. In the main pane select VMs tab. For each VM in the table perform the following steps: Right click on the VM. Select Migrate... option. Migrate wizard should open up. In Select the migration type step: Select Change both compute resource and storage option. Select Select compute resource first option. Click Next button. In Select a compute resource step: Expand CI-DC datacenter entry. Expand WinCI cluster entry. Select a host different than maintained host. Click Next button. In Select storage step: Select a datastore local to host selected in the previous step. Click Next button. In Select networks step: Verify that Source Network matches Destination Network in the presented table. Click Next button. In Select vMotion priority step: Select Schedule vMotion with high priority option. Click Next button. In Ready to complete step: Click Finish button. Migrate templates. Left click on the host entry in the Navigator pane. 47 - In the main pane select VMs tab. In the main pane select VMs tab. Below VMs tab, select VM Templates if Folders . For each template perform the following steps: Right click on the template. Select Convert to Virtual Machine... option. Convert Template to Virtual Machine wizard should open up: In Select a compute resource step: Click on WinCI cluster object. In the bottom of the windows, you should see Compatibility checks succeeded . Click Next button. In Ready to Complete step: Click Finish button. Change screen from VM Templates in Folders to Virtual Machines . If the template is listed on the VM list: Perform the same migration steps as for virtual machines from step 11. If the template is not listed: It probably was already located on shared storage and VMware reassigned it to the different host. To confirm search for this template in VMs and Templates tab, in Navigator pane. Right click on this relocated/migrated VM and select Template Convert to Template option. In the Confirm Convert windows, click Yes button. NOTE : testbed templates should stay on NFS-Datastore , since template location on shared storage is a requirement of linked clones. NOTE : If enough space is free on NFS-Datastore , latest templates should be located on it. Switch host to the maintenance mode. Right click on the host entry in the Navigator pane. Select Maintenance Mode Enter Maintenance Mode option. In the Confirm Maintenance Mode window, uncheck Move powered-off and suspended virtual machines to other hosts in the cluster option. Click OK button. Move host out of the WinCI cluster. Right click on the host entry in the Navigator pane. Select Move To... option. Click on CI-DC datacenter object. Click OK button.","title":"Steps"},{"location":"For CI admins/Infra/VMWare cluster/Reconnecting_Host/","text":"Reconnecting Host to Contrail Windows CI cluster This document describes a procedure required to reconnected a VMware ESXi host to the Contrail Windows CI cluster. Reconnecting a host to CI cluster is understood as a set to steps leading to usage of this host in Contrail Windows CI. Described procedure must be performed only when: a host with reinstalled ESXi must be reconnected to the cluster, a host with replaced hard drive must be reconnected to the cluster. Prerequisites Requirements for CI admin: Access to Contrail Windows CI infrastructure. Credentials to Contrail Windows CI VMware cluster. Browser with Flash support, preferably Chrome. Requirements for a reconnected host: Host must be connected to vCenter. Steps Open the vSphere Web Client and login using credentials provided by Contrail Windows CI team. In Navigator pane select Hosts and clusters tab. Expand CI-DC datacenter entry. Expand WinCI cluster entry. Find a host entry representing a being reconnected. If a host cannot be found, it is not connected to vCenter. In that case, please contact infrastructure team. if a host is connected to vCenter, but is not a part of WinCI cluster: Right click on the host and select Move To... option. In the Move To... window, expand CI-DC datacenter. Click on WinCI cluster. Click OK button. If a dialog Move Host into This Cluster comes up, select Put all of this host's virtual machines in the cluster's root resource pool option. Click Ok . If a host is marked as being in maintenance mode : Right click on a host entry and select Maintenance mode Exit maintenance mode . Click on the host entry. Host networking reconfiguration: In the middle pane select Configure tab. Select Virtual switches from the list on the left. Select vSwitch0 from the virtual switches list. Click on VM Network port group in the bottom. Click Edit settings button. Type in VM-Network in the Network label input and click Ok button. Select VMkernel adapters from the list on the left. Select on vmk0 adapter from the adapter list. Click Edit settings button. In Port properties wizard page, check vMotion checkbox. Click Ok button. In Navigator pane select Storage tab. Right click on NFS-Datastore and select Mount Datastore to Additional Hosts . Mark a checkbox next to a host entry representing a being reconnected. Click OK . Right click on winci_nfsbackup and select Mount Datastore to Additional Hosts . Mark a checkbox next to a host entry representing a being reconnected. Click OK . In Navigator pane select Hosts and clusters tab. Find a host entry representing a being reconnected and click on it. Reconfiguring ESXi logs location to a remote location. In the middle pane select Configure tab. Select Advanced System Settings from the list on the left. Click Edit button in the upper right corner. Type in logDir in the Filter input box and press Enter. Change the value of Syslog.global.logDir to [NFS-Datastore] logs . Mark the Enabled checkbox in Syslog.global.logDirUnique . Click Ok button. Type in logDir in the Filter input box and press Enter. Look through the filtered list and verify that provided options are saved. To verify that logs are stored in remote location, perform the following steps: In the Navigator pane select Storage tab. Click on NFS-Datastore . In the main pane select Files tab. Navigate to logs directory. Navigate to a directory named with host's hostname. ESXi logs should be stored in this directory. Go back to host's entry in Hosts and clusters tab in Navigator pane. Cleanup of orphaned VMs must be performed. In the middle pane select VMs tab. In the VM table, click Name header to sort VMs by name in ascending order. For each VM perform a following process: Determine if the VM is critical. Refer to List of all important VMs in Windows CI and check if this VM is on the list marked as CRITICAL . In case of any doubts, please contact Contrail Windows CI team. If the VM is marked as Orphaned and it is not critical to CI functioning: Left click on the VM. In the middle pane, scroll down to Related Objects window and check if VM is still located on local datastore. If it is the VM must be reregistered and then removed. Please follow steps in Reregistering a VM to reregister the VM. Right click on the VM and select Delete from disk option. If it is not, the VM can be safely removed from inventory. Right click on the VM. Select All Virtual Infrastructure Actions Remove from Inventory option. If a VM is marked as Orphaned and it is critical to CI functioning: Left click on the VM. In the middle pane, scroll down to Related Objects window and check if VM is still located on local datastore. If it is the VM must be reregistered. Please follow steps in Reregistering a VM reregister the VM. Power on the VM. If it is not, the VM must be restored from backups. Please refer to Infrastructure backups . Assign host's datastores to datastore clusters. In the middle pane select Datastores tab. Please contact a Contrail Windows CI team regarding datastore - cluster association and perform the following steps: Right click on a datastore. Select Move to option. In the Move To... window select a datastore cluster based on information from Contrail Windows CI team. After reconnecting After host is reconnected to Contrail Windows CI cluster, please consider the following: Migrate some of ci-builder-* nodes to this new host. TODO: document Appendices Reregistering a VM This guide assumes that the user is located in the VM window. Take note of a datastore names presented in Related Objects windows. Right click on the VM in the list on the left. Select All Virtual Infrastructure Actions Remove from Inventory option. In the Navigation pane, left click Storage tab. In the datastore list, click on the one of datastores listed previously in Related Objects window. Navigate to Files tab. Locate a folder named like a VM and enter it. If it cannot be found on this datastore, try another one. Right click on [VM-NAME].vmx file and select Register VM option. The Register Virtual Machine wizard should show up. In the Name and Location step select a suitable folder to put a VM in. Click Next . In the Host / Cluster step select a WinCI cluster. Click Next . In the Ready to Complete step click Finish button.","title":"Reconnecting Host to Contrail Windows CI cluster"},{"location":"For CI admins/Infra/VMWare cluster/Reconnecting_Host/#reconnecting-host-to-contrail-windows-ci-cluster","text":"This document describes a procedure required to reconnected a VMware ESXi host to the Contrail Windows CI cluster. Reconnecting a host to CI cluster is understood as a set to steps leading to usage of this host in Contrail Windows CI. Described procedure must be performed only when: a host with reinstalled ESXi must be reconnected to the cluster, a host with replaced hard drive must be reconnected to the cluster.","title":"Reconnecting Host to Contrail Windows CI cluster"},{"location":"For CI admins/Infra/VMWare cluster/Reconnecting_Host/#prerequisites","text":"Requirements for CI admin: Access to Contrail Windows CI infrastructure. Credentials to Contrail Windows CI VMware cluster. Browser with Flash support, preferably Chrome. Requirements for a reconnected host: Host must be connected to vCenter.","title":"Prerequisites"},{"location":"For CI admins/Infra/VMWare cluster/Reconnecting_Host/#steps","text":"Open the vSphere Web Client and login using credentials provided by Contrail Windows CI team. In Navigator pane select Hosts and clusters tab. Expand CI-DC datacenter entry. Expand WinCI cluster entry. Find a host entry representing a being reconnected. If a host cannot be found, it is not connected to vCenter. In that case, please contact infrastructure team. if a host is connected to vCenter, but is not a part of WinCI cluster: Right click on the host and select Move To... option. In the Move To... window, expand CI-DC datacenter. Click on WinCI cluster. Click OK button. If a dialog Move Host into This Cluster comes up, select Put all of this host's virtual machines in the cluster's root resource pool option. Click Ok . If a host is marked as being in maintenance mode : Right click on a host entry and select Maintenance mode Exit maintenance mode . Click on the host entry. Host networking reconfiguration: In the middle pane select Configure tab. Select Virtual switches from the list on the left. Select vSwitch0 from the virtual switches list. Click on VM Network port group in the bottom. Click Edit settings button. Type in VM-Network in the Network label input and click Ok button. Select VMkernel adapters from the list on the left. Select on vmk0 adapter from the adapter list. Click Edit settings button. In Port properties wizard page, check vMotion checkbox. Click Ok button. In Navigator pane select Storage tab. Right click on NFS-Datastore and select Mount Datastore to Additional Hosts . Mark a checkbox next to a host entry representing a being reconnected. Click OK . Right click on winci_nfsbackup and select Mount Datastore to Additional Hosts . Mark a checkbox next to a host entry representing a being reconnected. Click OK . In Navigator pane select Hosts and clusters tab. Find a host entry representing a being reconnected and click on it. Reconfiguring ESXi logs location to a remote location. In the middle pane select Configure tab. Select Advanced System Settings from the list on the left. Click Edit button in the upper right corner. Type in logDir in the Filter input box and press Enter. Change the value of Syslog.global.logDir to [NFS-Datastore] logs . Mark the Enabled checkbox in Syslog.global.logDirUnique . Click Ok button. Type in logDir in the Filter input box and press Enter. Look through the filtered list and verify that provided options are saved. To verify that logs are stored in remote location, perform the following steps: In the Navigator pane select Storage tab. Click on NFS-Datastore . In the main pane select Files tab. Navigate to logs directory. Navigate to a directory named with host's hostname. ESXi logs should be stored in this directory. Go back to host's entry in Hosts and clusters tab in Navigator pane. Cleanup of orphaned VMs must be performed. In the middle pane select VMs tab. In the VM table, click Name header to sort VMs by name in ascending order. For each VM perform a following process: Determine if the VM is critical. Refer to List of all important VMs in Windows CI and check if this VM is on the list marked as CRITICAL . In case of any doubts, please contact Contrail Windows CI team. If the VM is marked as Orphaned and it is not critical to CI functioning: Left click on the VM. In the middle pane, scroll down to Related Objects window and check if VM is still located on local datastore. If it is the VM must be reregistered and then removed. Please follow steps in Reregistering a VM to reregister the VM. Right click on the VM and select Delete from disk option. If it is not, the VM can be safely removed from inventory. Right click on the VM. Select All Virtual Infrastructure Actions Remove from Inventory option. If a VM is marked as Orphaned and it is critical to CI functioning: Left click on the VM. In the middle pane, scroll down to Related Objects window and check if VM is still located on local datastore. If it is the VM must be reregistered. Please follow steps in Reregistering a VM reregister the VM. Power on the VM. If it is not, the VM must be restored from backups. Please refer to Infrastructure backups . Assign host's datastores to datastore clusters. In the middle pane select Datastores tab. Please contact a Contrail Windows CI team regarding datastore - cluster association and perform the following steps: Right click on a datastore. Select Move to option. In the Move To... window select a datastore cluster based on information from Contrail Windows CI team.","title":"Steps"},{"location":"For CI admins/Infra/VMWare cluster/Reconnecting_Host/#after-reconnecting","text":"After host is reconnected to Contrail Windows CI cluster, please consider the following: Migrate some of ci-builder-* nodes to this new host. TODO: document","title":"After reconnecting"},{"location":"For CI admins/Infra/VMWare cluster/Reconnecting_Host/#appendices","text":"","title":"Appendices"},{"location":"For CI admins/Infra/VMWare cluster/Reconnecting_Host/#reregistering-a-vm","text":"This guide assumes that the user is located in the VM window. Take note of a datastore names presented in Related Objects windows. Right click on the VM in the list on the left. Select All Virtual Infrastructure Actions Remove from Inventory option. In the Navigation pane, left click Storage tab. In the datastore list, click on the one of datastores listed previously in Related Objects window. Navigate to Files tab. Locate a folder named like a VM and enter it. If it cannot be found on this datastore, try another one. Right click on [VM-NAME].vmx file and select Register VM option. The Register Virtual Machine wizard should show up. In the Name and Location step select a suitable folder to put a VM in. Click Next . In the Host / Cluster step select a WinCI cluster. Click Next . In the Ready to Complete step click Finish button.","title":"Reregistering a VM"},{"location":"For CI admins/Nightly/nightly_builds/","text":"Nightly builds in Windows CI This document describes how nightly builds are done in Contrail Windows CI. Jenkins jobs: ci-contrail-windows-nightly-* There are two jobs responsible for the builds in CI: ci-contrail-windows-nightly-release - builds Contrail Windows components in release / production mode. ci-contrail-windows-nightly-debug - builds the components in debug mode. Both of them have parameter UPLOAD_ARTIFACTS set to 1. The difference between them is that ci-contrail-windows-nightly-release has an additional parameter BUILD_IN_RELEASE_MODE defined and set to 1 . These jobs are proxy jobs. They execute winci-server2016-devel with additional parameters mentioned above. Specific nightly builds artifacts location To find artifacts from a specific nightly build do the following: Go to the console logs from the build. There is a line containing build ID of winci-server2016-devel : Initial part of the line looks like this: WinContrail \u00bb winci-server2016-devel BUILD_NUMBER completed . Note the BUILD_NUMBER . Use BUILD_NUMBER to find desired nightly artifacts: Uploaded artifacts are located in \\\\ WINCI_DRIVE \\SharedFiles\\WindowsCI-UploadedArtifacts\\WinContrail\\ winci-server2016-devel\\ BUILD_MODE \\ BUILD_NUMBER for now WINCI_DRIVE is an IP to Contrail Windows CI drive BUILD_MODE is a folder named by either production or debug build mode","title":"Nightly builds in Windows CI"},{"location":"For CI admins/Nightly/nightly_builds/#nightly-builds-in-windows-ci","text":"This document describes how nightly builds are done in Contrail Windows CI.","title":"Nightly builds in Windows CI"},{"location":"For CI admins/Nightly/nightly_builds/#jenkins-jobs-ci-contrail-windows-nightly-","text":"There are two jobs responsible for the builds in CI: ci-contrail-windows-nightly-release - builds Contrail Windows components in release / production mode. ci-contrail-windows-nightly-debug - builds the components in debug mode. Both of them have parameter UPLOAD_ARTIFACTS set to 1. The difference between them is that ci-contrail-windows-nightly-release has an additional parameter BUILD_IN_RELEASE_MODE defined and set to 1 . These jobs are proxy jobs. They execute winci-server2016-devel with additional parameters mentioned above.","title":"Jenkins jobs: ci-contrail-windows-nightly-*"},{"location":"For CI admins/Nightly/nightly_builds/#specific-nightly-builds-artifacts-location","text":"To find artifacts from a specific nightly build do the following: Go to the console logs from the build. There is a line containing build ID of winci-server2016-devel : Initial part of the line looks like this: WinContrail \u00bb winci-server2016-devel BUILD_NUMBER completed . Note the BUILD_NUMBER . Use BUILD_NUMBER to find desired nightly artifacts: Uploaded artifacts are located in \\\\ WINCI_DRIVE \\SharedFiles\\WindowsCI-UploadedArtifacts\\WinContrail\\ winci-server2016-devel\\ BUILD_MODE \\ BUILD_NUMBER for now WINCI_DRIVE is an IP to Contrail Windows CI drive BUILD_MODE is a folder named by either production or debug build mode","title":"Specific nightly builds artifacts location"},{"location":"For developers/Contributing/Style guides/C/","text":"We try to maintain the same coding style as the rest of vRouter wherever we can, but still use conventions used by Windows Driver Kit wherever applicable (regarding variable names, P convention instead of * for specifying pointers to native Windows structures). Below every rule, there is an example illustrating it with a code sample. General Styling Use spaces, not tabs. One indentation level is 4 spaces Every line should end with a newline character ('\\n') Newlines commited into the remote repository should be Unix-like (newline character), avoid Windows-like newlines (carriage return; newline) Avoid magic numbers, unless there is a very good reason. Use define directives instead. Statements with blocks (if, while, for, do...) should have the opening bracket in the same line Statements with only one line instead of a whole block can skip brackets if it improves readability Statements if/else should not skip brackets unless it can be done for both if, else and all eventual else ifs if (condition) do_something(); else do_nothing(); if (condition) { do_something(); } else { i+= 1; do_nothing(); } do { ASSERT(FALSE); nobody_uses_this_feature(); } while(condition); If, for, while and similar statements should not be too long. This means, for example creating variables for parts of statement being checked BOOLEAN usable = (value = MIN_VALUE value = MAX_VALUE); BOOLEAN safe = (state == STABLE); if (usable safe) do_something(); Files Files in 'windows' directory are free to modify and do not have to be platform portable (they have to be Windows-compatible) Other files need to be platform independant. This may be done in #ifdef manner, but more elegant solutions are preferable For example callbacks Function names should be: vr_filename.c or vr_filename.h ** Please note vrouter_mod.c is an exception to this Files should be in directory: windows for Windows source files (.c extension) include for header files (.h extension) * dp-core for multiplatform source files (we don't forsee creating any) For header files, use guards to make sure there is no problem if they are included two or more times. #ifndef __VR_DEFS_H__ #define __VR_DEFS_H__ // Some code #endif Use #ifdef, not #pragma once for guards Naming Functions Name functions in snake case, names starting with vr_ or win_. Please note functions in vrouter_mod.c ATM don't follow this rule because of SxLibrary requirements. This will be changed when we get rid of it. vr_reinject_packet vr_ variant should be used for multiplatform functions win_ variant should be used for Windows-specific functions. When defining function, place the type in one line and the rest of it in another line. unsigned int vr_reinject_packet(struct vr_packet *pkt, struct vr_forwarding_md *fmd) When declaring functions, use the extern keyword and write everything in one line. extern unsigned int vr_some_function(); It is preferential but not required to declare which parameters of a function are constant. int vr_function(const char* address) Variables We avoid constant literal variables. Instead, one should avoid magic numbers using #define directive. Const structs are OK. #define MAX_SIZE 32 Global variables can be used whenever necessary, especially when the function being implemented will be called by some pre-existing dp-core code without supplying required arguments. However, it is preferential to avoid them. Static variables are lesser evil. Please check if a global variable can be replaced with a static one Variables should be named using snake case. LARGE_INTEGER current_gmt_time; It is preferential but not required to declare constant variables as such const unsigned int num_of_tries = input + 5; If a value is known to be always positive, one should use unsigned variable. Same for bitfields. unsigned int i = 5; Variables should be initialized before reading them. Types Existing types should be used as-is PNET_BUFFER_LIST nbl; New types and types declared by us should use snake case and begin with vr_. struct vr_struct {}; Do not typedef function, use struct as a part of the name struct vr_router* router = vrouter_get(0); Comments Both block and line comments are allowed int i = pointer- field; // Pointer cannot be null /* TODO: Fix this */ Non-obvious logic should be commented Do not leave commented out code unless there is a very good reason. Platform-specific code Use #ifdefs to specify code that should be compiled on only selected platforms #ifdef __KERNEL__ Prefer #ifdef to #if defined unless more specific statement checking if required. #if defined(_WINDOWS) || defined(__FreeBSD__) Do not use compiler or linker specific features in code if they would break compatibility with other tools. For example, most #pragma statements. #ifdef _WINDOWS #pragma warning(disable : 4018) // Disable warning about signed/unsigned mismatch #endif Try not to modify existing dp-core code if possible, otherwise modify it: Preferential: Creating additional callbacks (can be NULL or empty functions on Linux/FreeBSD) Using #ifdefs Try not to use os, compiler or linker specific features if possible If there is already some code implemented using them, consider reimplementing a native Linux function using Windows libraries. This function should only be compiled on Windows as Linux-like systems will provide their own implementation. Either of these two options will work Write it in a file that will only be included on Windows #ifdef the implementation If the body of a function will be completely different on Linux and Windows, write one function header and footer and #ifdef the rest of the body void print(const char* a) { #ifdef _WINDOWS DbgPrint(a); #else printf(a); #endif } Windows-specific code should be properely ASSERTed. Use this function or ASSERTMSG to check your assumptions Other points If a possible data-race condition arises, find the smallest critical section and wrap it into a mutex, semaphore or a RW lock. Functions should return: In Windows-specific code: NDIS_STATUS and all effects via pointers unless the function cannot fail In multiplatform code: regular return value if functions succeeds and -errno if the function fails","title":"C"},{"location":"For developers/Contributing/Style guides/C/#general","text":"","title":"General"},{"location":"For developers/Contributing/Style guides/C/#styling","text":"Use spaces, not tabs. One indentation level is 4 spaces Every line should end with a newline character ('\\n') Newlines commited into the remote repository should be Unix-like (newline character), avoid Windows-like newlines (carriage return; newline) Avoid magic numbers, unless there is a very good reason. Use define directives instead. Statements with blocks (if, while, for, do...) should have the opening bracket in the same line Statements with only one line instead of a whole block can skip brackets if it improves readability Statements if/else should not skip brackets unless it can be done for both if, else and all eventual else ifs if (condition) do_something(); else do_nothing(); if (condition) { do_something(); } else { i+= 1; do_nothing(); } do { ASSERT(FALSE); nobody_uses_this_feature(); } while(condition); If, for, while and similar statements should not be too long. This means, for example creating variables for parts of statement being checked BOOLEAN usable = (value = MIN_VALUE value = MAX_VALUE); BOOLEAN safe = (state == STABLE); if (usable safe) do_something();","title":"Styling"},{"location":"For developers/Contributing/Style guides/C/#files","text":"Files in 'windows' directory are free to modify and do not have to be platform portable (they have to be Windows-compatible) Other files need to be platform independant. This may be done in #ifdef manner, but more elegant solutions are preferable For example callbacks Function names should be: vr_filename.c or vr_filename.h ** Please note vrouter_mod.c is an exception to this Files should be in directory: windows for Windows source files (.c extension) include for header files (.h extension) * dp-core for multiplatform source files (we don't forsee creating any) For header files, use guards to make sure there is no problem if they are included two or more times. #ifndef __VR_DEFS_H__ #define __VR_DEFS_H__ // Some code #endif Use #ifdef, not #pragma once for guards","title":"Files"},{"location":"For developers/Contributing/Style guides/C/#naming","text":"","title":"Naming"},{"location":"For developers/Contributing/Style guides/C/#functions","text":"Name functions in snake case, names starting with vr_ or win_. Please note functions in vrouter_mod.c ATM don't follow this rule because of SxLibrary requirements. This will be changed when we get rid of it. vr_reinject_packet vr_ variant should be used for multiplatform functions win_ variant should be used for Windows-specific functions. When defining function, place the type in one line and the rest of it in another line. unsigned int vr_reinject_packet(struct vr_packet *pkt, struct vr_forwarding_md *fmd) When declaring functions, use the extern keyword and write everything in one line. extern unsigned int vr_some_function(); It is preferential but not required to declare which parameters of a function are constant. int vr_function(const char* address)","title":"Functions"},{"location":"For developers/Contributing/Style guides/C/#variables","text":"We avoid constant literal variables. Instead, one should avoid magic numbers using #define directive. Const structs are OK. #define MAX_SIZE 32 Global variables can be used whenever necessary, especially when the function being implemented will be called by some pre-existing dp-core code without supplying required arguments. However, it is preferential to avoid them. Static variables are lesser evil. Please check if a global variable can be replaced with a static one Variables should be named using snake case. LARGE_INTEGER current_gmt_time; It is preferential but not required to declare constant variables as such const unsigned int num_of_tries = input + 5; If a value is known to be always positive, one should use unsigned variable. Same for bitfields. unsigned int i = 5; Variables should be initialized before reading them.","title":"Variables"},{"location":"For developers/Contributing/Style guides/C/#types","text":"Existing types should be used as-is PNET_BUFFER_LIST nbl; New types and types declared by us should use snake case and begin with vr_. struct vr_struct {}; Do not typedef function, use struct as a part of the name struct vr_router* router = vrouter_get(0);","title":"Types"},{"location":"For developers/Contributing/Style guides/C/#comments","text":"Both block and line comments are allowed int i = pointer- field; // Pointer cannot be null /* TODO: Fix this */ Non-obvious logic should be commented Do not leave commented out code unless there is a very good reason. Platform-specific code Use #ifdefs to specify code that should be compiled on only selected platforms #ifdef __KERNEL__ Prefer #ifdef to #if defined unless more specific statement checking if required. #if defined(_WINDOWS) || defined(__FreeBSD__) Do not use compiler or linker specific features in code if they would break compatibility with other tools. For example, most #pragma statements. #ifdef _WINDOWS #pragma warning(disable : 4018) // Disable warning about signed/unsigned mismatch #endif Try not to modify existing dp-core code if possible, otherwise modify it: Preferential: Creating additional callbacks (can be NULL or empty functions on Linux/FreeBSD)","title":"Comments"},{"location":"For developers/Contributing/Style guides/C/#using-ifdefs","text":"Try not to use os, compiler or linker specific features if possible If there is already some code implemented using them, consider reimplementing a native Linux function using Windows libraries. This function should only be compiled on Windows as Linux-like systems will provide their own implementation. Either of these two options will work Write it in a file that will only be included on Windows #ifdef the implementation If the body of a function will be completely different on Linux and Windows, write one function header and footer and #ifdef the rest of the body void print(const char* a) { #ifdef _WINDOWS DbgPrint(a); #else printf(a); #endif } Windows-specific code should be properely ASSERTed. Use this function or ASSERTMSG to check your assumptions","title":"Using #ifdefs"},{"location":"For developers/Contributing/Style guides/C/#other-points","text":"If a possible data-race condition arises, find the smallest critical section and wrap it into a mutex, semaphore or a RW lock. Functions should return: In Windows-specific code: NDIS_STATUS and all effects via pointers unless the function cannot fail In multiplatform code: regular return value if functions succeeds and -errno if the function fails","title":"Other points"},{"location":"For developers/Contributing/Style guides/Cpp/","text":"We aim to use a subset of all C++ features. This includes most features present in C++ up to C++11. General Platform-specific code Use #ifdef to differentiate Windows and non-Windows code Try to implement Windows-specific code on the lowest possible abstraction layer so that the higher ones can be platform agnostic. When providing alternate body of a function, use one function header/signature but two implementations in its body, encased in #ifdef guards. Practices Use class declarations in header files if the definition is not necessary. class BgpAttr; In header files, use #ifdef guards to prevent any issues arising from including the same header twice #ifndef ctrlplane_ksync_sock_h #define ctrlplane_ksync_sock_h // Some code #endif Use of many for-each variants instead of iterating over an index when accessing an array for(auto element : array) { //do something... } auto itr = strvec.begin(); while(itr != strvec.end()) { //do something... ++itr; } Style Files Use .cc files for C++ source code Use .h files for C++ header code Naming Files Files should be named in snake case. iroute_aggregator.h Classes Classes should be named in camel case with the first letter capital class IRouteAggregator { }; Functions/Methods Methods and functions should be named in upper camel case class IRouteAggregator { void Initialize(); }; Variables/Fields Variables and fields should be named according to the snake case convention ** Note: Private fields often have a single underscore appended, as this makes creating a getter/setter more intuitive Design Paradigm Agent is designed with traditional object-oriented model of C++ application, therefore we should try to maintain that style. Agent does not use exceptions, instead relying on old-school return code error handling. int func() { if (error) return -ENOMEM; } Agent does use virtual and abstract (pure virtual) classes. class Class { public: virtual void Func() = 0; virtual void Func2(); void Func3(); private: }; Agent uses proper visibility settings, ie. uses public, private and protected specifiers in class definitions. Templating is used and encouraged.","title":"Cpp"},{"location":"For developers/Contributing/Style guides/Cpp/#general","text":"","title":"General"},{"location":"For developers/Contributing/Style guides/Cpp/#platform-specific-code","text":"Use #ifdef to differentiate Windows and non-Windows code Try to implement Windows-specific code on the lowest possible abstraction layer so that the higher ones can be platform agnostic. When providing alternate body of a function, use one function header/signature but two implementations in its body, encased in #ifdef guards.","title":"Platform-specific code"},{"location":"For developers/Contributing/Style guides/Cpp/#practices","text":"Use class declarations in header files if the definition is not necessary. class BgpAttr; In header files, use #ifdef guards to prevent any issues arising from including the same header twice #ifndef ctrlplane_ksync_sock_h #define ctrlplane_ksync_sock_h // Some code #endif Use of many for-each variants instead of iterating over an index when accessing an array for(auto element : array) { //do something... } auto itr = strvec.begin(); while(itr != strvec.end()) { //do something... ++itr; }","title":"Practices"},{"location":"For developers/Contributing/Style guides/Cpp/#style","text":"","title":"Style"},{"location":"For developers/Contributing/Style guides/Cpp/#files","text":"Use .cc files for C++ source code Use .h files for C++ header code","title":"Files"},{"location":"For developers/Contributing/Style guides/Cpp/#naming","text":"","title":"Naming"},{"location":"For developers/Contributing/Style guides/Cpp/#files_1","text":"Files should be named in snake case. iroute_aggregator.h Classes Classes should be named in camel case with the first letter capital class IRouteAggregator { };","title":"Files"},{"location":"For developers/Contributing/Style guides/Cpp/#functionsmethods","text":"Methods and functions should be named in upper camel case class IRouteAggregator { void Initialize(); };","title":"Functions/Methods"},{"location":"For developers/Contributing/Style guides/Cpp/#variablesfields","text":"Variables and fields should be named according to the snake case convention ** Note: Private fields often have a single underscore appended, as this makes creating a getter/setter more intuitive","title":"Variables/Fields"},{"location":"For developers/Contributing/Style guides/Cpp/#design","text":"","title":"Design"},{"location":"For developers/Contributing/Style guides/Cpp/#paradigm","text":"Agent is designed with traditional object-oriented model of C++ application, therefore we should try to maintain that style. Agent does not use exceptions, instead relying on old-school return code error handling. int func() { if (error) return -ENOMEM; } Agent does use virtual and abstract (pure virtual) classes. class Class { public: virtual void Func() = 0; virtual void Func2(); void Func3(); private: }; Agent uses proper visibility settings, ie. uses public, private and protected specifiers in class definitions. Templating is used and encouraged.","title":"Paradigm"},{"location":"For developers/Contributing/Style guides/Go/","text":"Since docker driver written in Go is the only module of OpenContrail we've written completely by ourselves, we can introduce the best programming practices here, without regard for compatibility with legacy code. For issues not discussed here, look at Official Code Review Comments. General Coding style Avoid nesting if possible Try to exit from the function early in a case of an error Use tabs for indentations. One tab is one indentation level. Code should be as compliant with gometalinter. If a line would be longer than 100 characters (cound tab as 4 spaces for this), divide it into a few lines, so every line is shorter than said length. Errors Handle errors as soon as possible Functions that are not sure to be successful should return error as one of returned values func SomeFunc(param string) (string, error) { Other guidelines Try to use libraries to do required functionality, instead of calling PowerShell commands or otherwise using external features if possible Often it's not possible Use log module to provide logging capability Remember to use different message levels correctly, ie. error for errors, info for regular info, debug for info which should usually be hidden, etc. log.Infoln( Deleting HNS network , hnsID) Design Modules Keep modules independent if possible Otherwise, make the dependency explicit Dependencies can often be avoided by using interfaces When importing not inbuilt modules (eg. from github), keep a copy of them in vendor directory. Use govendor to upkeep it. Functions Functions should only accept as arguments what they really need, ie. io.Writer instead of *os.File Dependencies can be avoided by using interfaces Avoid functions concurrent by default. By exposing synchronous API one can easily choose how to call the function, the other way around doesn't work. Use chans to communicate with goroutines Unexported functions should be named in camel case with first letter lowercase Functions should be named in camel case with first letter capital for exported functions func CreateHNSNetwork(configuration *hcsshim.HNSNetwork) (string, error) { Testing All features should be tested if possible Features implemented in file.go should be tested in file_test.go Used frameworks should be ginkgo and gomega","title":"Go"},{"location":"For developers/Contributing/Style guides/Go/#general","text":"","title":"General"},{"location":"For developers/Contributing/Style guides/Go/#coding-style","text":"Avoid nesting if possible Try to exit from the function early in a case of an error Use tabs for indentations. One tab is one indentation level. Code should be as compliant with gometalinter. If a line would be longer than 100 characters (cound tab as 4 spaces for this), divide it into a few lines, so every line is shorter than said length.","title":"Coding style"},{"location":"For developers/Contributing/Style guides/Go/#errors","text":"Handle errors as soon as possible Functions that are not sure to be successful should return error as one of returned values func SomeFunc(param string) (string, error) {","title":"Errors"},{"location":"For developers/Contributing/Style guides/Go/#other-guidelines","text":"Try to use libraries to do required functionality, instead of calling PowerShell commands or otherwise using external features if possible Often it's not possible Use log module to provide logging capability Remember to use different message levels correctly, ie. error for errors, info for regular info, debug for info which should usually be hidden, etc. log.Infoln( Deleting HNS network , hnsID)","title":"Other guidelines"},{"location":"For developers/Contributing/Style guides/Go/#design","text":"","title":"Design"},{"location":"For developers/Contributing/Style guides/Go/#modules","text":"Keep modules independent if possible Otherwise, make the dependency explicit Dependencies can often be avoided by using interfaces When importing not inbuilt modules (eg. from github), keep a copy of them in vendor directory. Use govendor to upkeep it.","title":"Modules"},{"location":"For developers/Contributing/Style guides/Go/#functions","text":"Functions should only accept as arguments what they really need, ie. io.Writer instead of *os.File Dependencies can be avoided by using interfaces Avoid functions concurrent by default. By exposing synchronous API one can easily choose how to call the function, the other way around doesn't work. Use chans to communicate with goroutines Unexported functions should be named in camel case with first letter lowercase Functions should be named in camel case with first letter capital for exported functions func CreateHNSNetwork(configuration *hcsshim.HNSNetwork) (string, error) {","title":"Functions"},{"location":"For developers/Contributing/Style guides/Go/#testing","text":"All features should be tested if possible Features implemented in file.go should be tested in file_test.go Used frameworks should be ginkgo and gomega","title":"Testing"},{"location":"For developers/Developer guide/Research/Experiment_dockerized_builder_using_ansible/","text":"Failed experiment: dockerized builder using ansible This document describes what would be needed create a builder docker image (for building vrouter agent and extension) by reusing ansible scripts that are already used in the contrail-windows-ci repository to deploy builder machines. This method of building is not yet supported, but this document describes the steps needed to make it work. It also tries to list the potential obstacles. This document describes the process from the developer's point of view, but it could be adapted to running in CI too. Windows Currently, this process is supported only for Windows machines. Docker Install Docker Community Edition for Windows . Note that: You need to sign in / create to accout to download the installer. You need to choose windows containers (not linux) when installing. TODO: Is there another installation method? Ansible To execute ansible on Windows Docker containers you need to have: A container image prepared for connecting via ansible to. A (virtual) linux machine to run ansible. (Optionally) To troubleshoot remote connection to container, you need to have the container IP in trusted hosts lists . Depending on the configuration of your system, this may require changing the policy in your domain. Preparing container for ansible Notes: Perhaps there exists a ready container prepared for ansible? There exists an example script ConfigureRemotingForAnsible.ps1 . Unfortunately, the microsoft/windowsservercore image does not have the firewall service used by the script, so steps in this scripts need to be run manually. This wasn't finished, and it needs some additional investigation. Machine for ansible The preferred way to use ansible on windows machine is to use the Linux Subsystem on Windows. You can install Ubuntu 18.04 from the Windows Store . (18.04 version is needed for the ansible 2.4 to be available. On the older version you can probably install ansible using pip) Note: Alternatively, you can install ansible and its dependencies also using this python-requirements.txt file from contrail-windows-ci repository with pip install -r python-requirements.txt Install ansible = 2.4 using apt : sudo apt install ansible There are additional python packages that need to be installed to let ansible connect to Windows machine: sudo apt install python-requests sudo apt install python-pip pip install pywinrm pip install requests-credssp The group_vars/windows also need to be configured for windows: NOTE: group_vars , inventory and other scripts would be already prepared, so this step won't be needed for the end user. ansible_port: 5986 ansible_connection: winrm ansible_winrm_transport: credssp ansible_winrm_server_cert_validation: ignore ansible_winrm_operation_timeout_sec: 60 ansible_winrm_read_timeout_sec: 120 ansible_user: Administrator@localhost ansible_password: 'hunter2' Container components Build tools and .NET Installation of .NET on microsoft/windowsservercore is not supported, but microsoft provides the microsoft/dotnet-framework image. TODO: Do we need to install multiple .NET versions? If not, this image would be sufficient. The build tools can be installed on the container following the instruction on this page in microsoft docs . Unfortunately, I haven't managed to finish it, due to some HCS errors. Perhaps it was related to insufficient disk space (before running the Dockerfile I had 50GB free space). Perhaps there is some other way of installing build tools, which I haven't tested. Troubleshooting When encountering weird errors in docker build (the errors may come from HCS), it's probable that they're caused by: Insufficient available RAM/swap on the host machine. Insufficient RAM assigned to container. Insufficient disk space on the host machine. Insufficient disk space assigned to container.","title":"Failed experiment: dockerized builder using ansible"},{"location":"For developers/Developer guide/Research/Experiment_dockerized_builder_using_ansible/#failed-experiment-dockerized-builder-using-ansible","text":"This document describes what would be needed create a builder docker image (for building vrouter agent and extension) by reusing ansible scripts that are already used in the contrail-windows-ci repository to deploy builder machines. This method of building is not yet supported, but this document describes the steps needed to make it work. It also tries to list the potential obstacles. This document describes the process from the developer's point of view, but it could be adapted to running in CI too.","title":"Failed experiment: dockerized builder using ansible"},{"location":"For developers/Developer guide/Research/Experiment_dockerized_builder_using_ansible/#windows","text":"Currently, this process is supported only for Windows machines.","title":"Windows"},{"location":"For developers/Developer guide/Research/Experiment_dockerized_builder_using_ansible/#docker","text":"Install Docker Community Edition for Windows . Note that: You need to sign in / create to accout to download the installer. You need to choose windows containers (not linux) when installing. TODO: Is there another installation method?","title":"Docker"},{"location":"For developers/Developer guide/Research/Experiment_dockerized_builder_using_ansible/#ansible","text":"To execute ansible on Windows Docker containers you need to have: A container image prepared for connecting via ansible to. A (virtual) linux machine to run ansible. (Optionally) To troubleshoot remote connection to container, you need to have the container IP in trusted hosts lists . Depending on the configuration of your system, this may require changing the policy in your domain.","title":"Ansible"},{"location":"For developers/Developer guide/Research/Experiment_dockerized_builder_using_ansible/#preparing-container-for-ansible","text":"Notes: Perhaps there exists a ready container prepared for ansible? There exists an example script ConfigureRemotingForAnsible.ps1 . Unfortunately, the microsoft/windowsservercore image does not have the firewall service used by the script, so steps in this scripts need to be run manually. This wasn't finished, and it needs some additional investigation.","title":"Preparing container for ansible"},{"location":"For developers/Developer guide/Research/Experiment_dockerized_builder_using_ansible/#machine-for-ansible","text":"The preferred way to use ansible on windows machine is to use the Linux Subsystem on Windows. You can install Ubuntu 18.04 from the Windows Store . (18.04 version is needed for the ansible 2.4 to be available. On the older version you can probably install ansible using pip) Note: Alternatively, you can install ansible and its dependencies also using this python-requirements.txt file from contrail-windows-ci repository with pip install -r python-requirements.txt Install ansible = 2.4 using apt : sudo apt install ansible There are additional python packages that need to be installed to let ansible connect to Windows machine: sudo apt install python-requests sudo apt install python-pip pip install pywinrm pip install requests-credssp The group_vars/windows also need to be configured for windows: NOTE: group_vars , inventory and other scripts would be already prepared, so this step won't be needed for the end user. ansible_port: 5986 ansible_connection: winrm ansible_winrm_transport: credssp ansible_winrm_server_cert_validation: ignore ansible_winrm_operation_timeout_sec: 60 ansible_winrm_read_timeout_sec: 120 ansible_user: Administrator@localhost ansible_password: 'hunter2'","title":"Machine for ansible"},{"location":"For developers/Developer guide/Research/Experiment_dockerized_builder_using_ansible/#container-components","text":"","title":"Container components"},{"location":"For developers/Developer guide/Research/Experiment_dockerized_builder_using_ansible/#build-tools-and-net","text":"Installation of .NET on microsoft/windowsservercore is not supported, but microsoft provides the microsoft/dotnet-framework image. TODO: Do we need to install multiple .NET versions? If not, this image would be sufficient. The build tools can be installed on the container following the instruction on this page in microsoft docs . Unfortunately, I haven't managed to finish it, due to some HCS errors. Perhaps it was related to insufficient disk space (before running the Dockerfile I had 50GB free space). Perhaps there is some other way of installing build tools, which I haven't tested.","title":"Build tools and .NET"},{"location":"For developers/Developer guide/Research/Experiment_dockerized_builder_using_ansible/#troubleshooting","text":"When encountering weird errors in docker build (the errors may come from HCS), it's probable that they're caused by: Insufficient available RAM/swap on the host machine. Insufficient RAM assigned to container. Insufficient disk space on the host machine. Insufficient disk space assigned to container.","title":"Troubleshooting"},{"location":"For developers/Developer guide/Research/dns_in_contrail_overview/","text":"Few words about DNS in Contrail This document describes DNS modes in Contrail and gives some hints about troubleshooting. DNS Modes In Contrail DNS queries from specific VM located on Contrail compute node can be handled differently depending on chosen DNS mode in configuration of IPAM in which the VM resides. There are 3 available DNS modes in Contrail: None - No DNS support for VMs Default - DNS requests from VMs are resolved via the fabric name servers Tenant - DNS requests from VMs are resolved via DNS servers deployed on tenant\u2019s VMs Virtual - provides virtual DNS server which resolves DNS requests from VMs How to specify DNS mode in Contrail First, modify agent's config. The config needs to have specified DNS server with IP of Contrail control node. Example of config: ... [DNS] # Client port used by vrouter-agent while connecting to contrail-named dns_client_port=53 # List of IPAdress:Port of DNS Servers separated by space. servers= controller's ip :53 # Timeout for DNS server queries in milli-seconds dns_timeout=10000 # Maximum retries for DNS server queries dns_max_retries=5 ... Next, follow the steps at: DNS and IPAM configuration Debugging Common troubleshooting: Go to http:// hypervisor's_ip :8085/Snh_SandeshTraceRequest?x=DnsBind for overall DNS information received/sent by agent If agent knows doesn't know any DNS resolver then there should be \"...no DNS resolver for...\" log message. Go to http:// hypervisor's_ip :8085/Snh_DnsInfo for detailed information about DNS queries received/responces sent by agent since it had been running Specific mode troubleshooting: Default: Verify if Windows hypervisor where agent is running knows any DNS servers: Compile the code from this link Execute the program on the machine with agent, if no IP is shown, then OS doesn't know any DNS resolver. If the program prints nothing, then it could be compiled for wrong architecture(i.e. x86 program running on x64 machine) \"...no DNS resolver for...\" log message present when hypervisor knows some DNS servers could indicate a bug in implementation of agent's function responsible for building the default list of DNS resolvers. Tenant: To find out if Tenant DNS mode works correctly, deploy a simple (i.e. python) DNS server in one of VMs and pass IP of the VM as a a DNS server in configuration of IPAM. Virtual: Refer to: vDNS debugging for additional help.","title":"Few words about DNS in Contrail"},{"location":"For developers/Developer guide/Research/dns_in_contrail_overview/#few-words-about-dns-in-contrail","text":"This document describes DNS modes in Contrail and gives some hints about troubleshooting.","title":"Few words about DNS in Contrail"},{"location":"For developers/Developer guide/Research/dns_in_contrail_overview/#dns-modes-in-contrail","text":"DNS queries from specific VM located on Contrail compute node can be handled differently depending on chosen DNS mode in configuration of IPAM in which the VM resides. There are 3 available DNS modes in Contrail: None - No DNS support for VMs Default - DNS requests from VMs are resolved via the fabric name servers Tenant - DNS requests from VMs are resolved via DNS servers deployed on tenant\u2019s VMs Virtual - provides virtual DNS server which resolves DNS requests from VMs","title":"DNS Modes In Contrail"},{"location":"For developers/Developer guide/Research/dns_in_contrail_overview/#how-to-specify-dns-mode-in-contrail","text":"First, modify agent's config. The config needs to have specified DNS server with IP of Contrail control node. Example of config: ... [DNS] # Client port used by vrouter-agent while connecting to contrail-named dns_client_port=53 # List of IPAdress:Port of DNS Servers separated by space. servers= controller's ip :53 # Timeout for DNS server queries in milli-seconds dns_timeout=10000 # Maximum retries for DNS server queries dns_max_retries=5 ... Next, follow the steps at: DNS and IPAM configuration","title":"How to specify DNS mode in Contrail"},{"location":"For developers/Developer guide/Research/dns_in_contrail_overview/#debugging","text":"Common troubleshooting: Go to http:// hypervisor's_ip :8085/Snh_SandeshTraceRequest?x=DnsBind for overall DNS information received/sent by agent If agent knows doesn't know any DNS resolver then there should be \"...no DNS resolver for...\" log message. Go to http:// hypervisor's_ip :8085/Snh_DnsInfo for detailed information about DNS queries received/responces sent by agent since it had been running Specific mode troubleshooting: Default: Verify if Windows hypervisor where agent is running knows any DNS servers: Compile the code from this link Execute the program on the machine with agent, if no IP is shown, then OS doesn't know any DNS resolver. If the program prints nothing, then it could be compiled for wrong architecture(i.e. x86 program running on x64 machine) \"...no DNS resolver for...\" log message present when hypervisor knows some DNS servers could indicate a bug in implementation of agent's function responsible for building the default list of DNS resolvers. Tenant: To find out if Tenant DNS mode works correctly, deploy a simple (i.e. python) DNS server in one of VMs and pass IP of the VM as a a DNS server in configuration of IPAM. Virtual: Refer to: vDNS debugging for additional help.","title":"Debugging"},{"location":"For developers/Developer guide/Setting up/Juni_lab/","text":"Dev environment in Juniper lab Windows CI, as an additional feature, allows for deployment of devenvs similar to the ones used in CI. This guide describes how to do it. Prerequisites Access to Juniper network. Account for Windows CI Jenkins. Account for Windows CI VMWare cluster. Steps 1. Choosing VLAN for the demo setup1. TODO this is tedious and should be automated in the future. Navigate to VCenter (currently at https://ci-vc.englab.juniper.net/ui) Log in Go to \"Network\" tab \u2192 CI-DC \u2192 dvs-ContrailWinCI Click through VLANs 502-505 and find one that has \"Virtual Machines: 0\"\\ Note it is very important that you don't cause any collisions with other users of dev environments. 2. Running Jenkins job Navigate to Jenkins webui (currently at http://10.84.12.26:8080/) Log in Navigate to WinContrail-infra/deploy-demo-env Press Build with Parameters : DEMOENV_NAME - choose a short prefix for your VMs hostnames DEMOENV_VLAN - choose an empty VLAN (the one you've found) Wait for 15 minutes Deploy (install) artifacts manually 3. Destroying demo env After you're done using dev environment, remove all virtual machines created by deploy-demo-env Navigate to Jenkins and log in (as described above) Navigate to WinContrail-infra/destroy-demo-env Press Build with Parameters , giving exactly the same DEMOENV_NAME as was used for deploying this demo env.","title":"Dev environment in Juniper lab"},{"location":"For developers/Developer guide/Setting up/Juni_lab/#dev-environment-in-juniper-lab","text":"Windows CI, as an additional feature, allows for deployment of devenvs similar to the ones used in CI. This guide describes how to do it.","title":"Dev environment in Juniper lab"},{"location":"For developers/Developer guide/Setting up/Juni_lab/#prerequisites","text":"Access to Juniper network. Account for Windows CI Jenkins. Account for Windows CI VMWare cluster.","title":"Prerequisites"},{"location":"For developers/Developer guide/Setting up/Juni_lab/#steps","text":"","title":"Steps"},{"location":"For developers/Developer guide/Setting up/Juni_lab/#1-choosing-vlan-for-the-demo-setup1","text":"TODO this is tedious and should be automated in the future. Navigate to VCenter (currently at https://ci-vc.englab.juniper.net/ui) Log in Go to \"Network\" tab \u2192 CI-DC \u2192 dvs-ContrailWinCI Click through VLANs 502-505 and find one that has \"Virtual Machines: 0\"\\ Note it is very important that you don't cause any collisions with other users of dev environments.","title":"1. Choosing VLAN for the demo setup1."},{"location":"For developers/Developer guide/Setting up/Juni_lab/#2-running-jenkins-job","text":"Navigate to Jenkins webui (currently at http://10.84.12.26:8080/) Log in Navigate to WinContrail-infra/deploy-demo-env Press Build with Parameters : DEMOENV_NAME - choose a short prefix for your VMs hostnames DEMOENV_VLAN - choose an empty VLAN (the one you've found) Wait for 15 minutes Deploy (install) artifacts manually","title":"2. Running Jenkins job"},{"location":"For developers/Developer guide/Setting up/Juni_lab/#3-destroying-demo-env","text":"After you're done using dev environment, remove all virtual machines created by deploy-demo-env Navigate to Jenkins and log in (as described above) Navigate to WinContrail-infra/destroy-demo-env Press Build with Parameters , giving exactly the same DEMOENV_NAME as was used for deploying this demo env.","title":"3. Destroying demo env"},{"location":"For developers/Interpreting CI logs/Windows_CI_logs_layout/","text":"Windows CI logs layout This document describes a log structure generated by ci-contrail-windows-production job. One can access these logs by following a link ci-contrail-windows-production in the comment posted by Contrail Windows CI used, after a job has finished. Log structure log.txt.gz - full Jenkins job log unittests-logs - logs for vRouter unit tests ran by Windows CI TestReports/CISelfcheck - test report for Windows CI unit tests suite for information about more detailed logs please refer to CI Selfcheck TestReports/WindowsCompute - test report for Contrail Windows integration tests suite for information about more detailed logs please refer to WindowsCompute CISelfcheck CISelfcheck is a set of unit tests for PowerShell scripts used in Windows CI. TestReports/CISelfcheck directory holds the following: TestReports/CISelfcheck/pretty_test_report - contains an HTML test report example test report tests marked green have passed tests marked red have failed tests marked orange are marked as inconclusive which means they are currently disabled in CI TestReports/CISelfcheck/raw_NUnit - contains NUnit XML files generated by PowerShell test runner WindowsCompute WindowsCompute is a set of integration tests for Contrail Windows. This test suite is run on a 3-node cluster consisting of 1 Contrail Controller node (all-in-one deployment) and 2 Contrail Windows compute nodes. A fresh cluster is provisioned before each test. Contrail Controller is provisioned using contrail-ansible-deployer. In the future Contrail Windows compute nodes will also be provisioned with contrail-ansible-deployer. TestReports/WindowsCompute directory holds the following: TestReports/WindowsCompute/pretty_test_report - contains an HTML test report example test report tests marked green have passed tests marked red have failed tests marked orange are marked as inconclusive which means they are currently disabled in CI TestReports/WindowsCompute/ddriver_junit_test_logs - contains JUnit XML files generated by Contrail Windows Docker Driver test runner TestReports/WindowsCompute/detailed_logs - contains logs dumped from test environment for each test; log file can contain logs from: Contrail Windows Docker Driver running on compute nodes Windows containers ran on compute nodes TestReports/WindowsCompute/raw_NUnit - contains NUnit XML files generated by PowerShell test runner","title":"Windows CI logs layout"},{"location":"For developers/Interpreting CI logs/Windows_CI_logs_layout/#windows-ci-logs-layout","text":"This document describes a log structure generated by ci-contrail-windows-production job. One can access these logs by following a link ci-contrail-windows-production in the comment posted by Contrail Windows CI used, after a job has finished.","title":"Windows CI logs layout"},{"location":"For developers/Interpreting CI logs/Windows_CI_logs_layout/#log-structure","text":"log.txt.gz - full Jenkins job log unittests-logs - logs for vRouter unit tests ran by Windows CI TestReports/CISelfcheck - test report for Windows CI unit tests suite for information about more detailed logs please refer to CI Selfcheck TestReports/WindowsCompute - test report for Contrail Windows integration tests suite for information about more detailed logs please refer to WindowsCompute","title":"Log structure"},{"location":"For developers/Interpreting CI logs/Windows_CI_logs_layout/#ciselfcheck","text":"CISelfcheck is a set of unit tests for PowerShell scripts used in Windows CI. TestReports/CISelfcheck directory holds the following: TestReports/CISelfcheck/pretty_test_report - contains an HTML test report example test report tests marked green have passed tests marked red have failed tests marked orange are marked as inconclusive which means they are currently disabled in CI TestReports/CISelfcheck/raw_NUnit - contains NUnit XML files generated by PowerShell test runner","title":"CISelfcheck"},{"location":"For developers/Interpreting CI logs/Windows_CI_logs_layout/#windowscompute","text":"WindowsCompute is a set of integration tests for Contrail Windows. This test suite is run on a 3-node cluster consisting of 1 Contrail Controller node (all-in-one deployment) and 2 Contrail Windows compute nodes. A fresh cluster is provisioned before each test. Contrail Controller is provisioned using contrail-ansible-deployer. In the future Contrail Windows compute nodes will also be provisioned with contrail-ansible-deployer. TestReports/WindowsCompute directory holds the following: TestReports/WindowsCompute/pretty_test_report - contains an HTML test report example test report tests marked green have passed tests marked red have failed tests marked orange are marked as inconclusive which means they are currently disabled in CI TestReports/WindowsCompute/ddriver_junit_test_logs - contains JUnit XML files generated by Contrail Windows Docker Driver test runner TestReports/WindowsCompute/detailed_logs - contains logs dumped from test environment for each test; log file can contain logs from: Contrail Windows Docker Driver running on compute nodes Windows containers ran on compute nodes TestReports/WindowsCompute/raw_NUnit - contains NUnit XML files generated by PowerShell test runner","title":"WindowsCompute"},{"location":"For developers/Technical documentation/Blueprint_5.0/","text":"Rationale and Scope Windows Server 2016 comes with support for containers and docker. Pros and cons of using containers are well known. This also sets up Contrail for possible Hyper-V support in the future. The scope is to port Compute node to Windows (Agent and vRouter) and integrate with Windows docker and Windows Containers. Implementors Rajagopalan Sivaramakrishnan (raja@juniper.net) Sagar Chitnis (sagarc@juniper.net) CodiLime dev team (windows-contrail@codilime.com) Target Release For release 1: 5.0 For release 2+: tbd User-visible changes On Linux, none. On Windows, one can use Windows docker command line tools to spawn Windows containers and connect them to Contrail networks. (orchestration is not in scope of release 1) For deployment, MSI installers are provided. Internal changes 3 components are affected: Windows docker driver is added. It is roughly equivalent to Nova Agent, but runs as a Windows service and implements docker driver APIs. It communicates with config, Agent and HNS (Host Network Service - Windows container management service). Written in Golang. vRouter Agent. Parts of the codebase are not cross platform. Changes involve rewriting those and fixing related bugs. vRouter Forwarding Extension. Implements vRouter kernel module functionality in terms of a Hyper-V Forwarding Extension, which is basically a kernel mode \"plugin\" for Windows virtual switch. Linux communication channels between (2) and (3) are also ported using Named Pipes (for ksync and pkt0) and Windows shared memory (for flow). More info is available under spec url.","title":"Blueprint 5.0"},{"location":"For developers/Technical documentation/Blueprint_5.0/#rationale-and-scope","text":"Windows Server 2016 comes with support for containers and docker. Pros and cons of using containers are well known. This also sets up Contrail for possible Hyper-V support in the future. The scope is to port Compute node to Windows (Agent and vRouter) and integrate with Windows docker and Windows Containers.","title":"Rationale and Scope"},{"location":"For developers/Technical documentation/Blueprint_5.0/#implementors","text":"Rajagopalan Sivaramakrishnan (raja@juniper.net) Sagar Chitnis (sagarc@juniper.net) CodiLime dev team (windows-contrail@codilime.com)","title":"Implementors"},{"location":"For developers/Technical documentation/Blueprint_5.0/#target-release","text":"For release 1: 5.0 For release 2+: tbd","title":"Target Release"},{"location":"For developers/Technical documentation/Blueprint_5.0/#user-visible-changes","text":"On Linux, none. On Windows, one can use Windows docker command line tools to spawn Windows containers and connect them to Contrail networks. (orchestration is not in scope of release 1) For deployment, MSI installers are provided.","title":"User-visible changes"},{"location":"For developers/Technical documentation/Blueprint_5.0/#internal-changes","text":"3 components are affected: Windows docker driver is added. It is roughly equivalent to Nova Agent, but runs as a Windows service and implements docker driver APIs. It communicates with config, Agent and HNS (Host Network Service - Windows container management service). Written in Golang. vRouter Agent. Parts of the codebase are not cross platform. Changes involve rewriting those and fixing related bugs. vRouter Forwarding Extension. Implements vRouter kernel module functionality in terms of a Hyper-V Forwarding Extension, which is basically a kernel mode \"plugin\" for Windows virtual switch. Linux communication channels between (2) and (3) are also ported using Named Pipes (for ksync and pkt0) and Windows shared memory (for flow). More info is available under spec url.","title":"Internal changes"},{"location":"For developers/Technical documentation/Communication_Agent_Extension/","text":"There are three ways in which vRouter Agent exchanges information with the Forwarding Extension, which mimics their Linux behaviour. Ksync This communication channel is used for inserting forwarding rules into the Forwarding Extension, like routes and next hops. On Linux, Netlink sockets are used for this. Equivalent Windows mechanism is a Named Pipe. Only vRouter Agent and other userspace programs can initialize the communiation. Howerver, different programs (for example, Agent and utils) may talk via ksync at the same time, so process separation (session layer) is required. This communication is performed over a binary protocol called Sandesh (the same one which is used by Contrail Analytics Nodes). Flow All flows are inserted into the Forwarding Extension using shared memory mechanism. Shared memory is allocated inside Forwarding Extension, and is written to by Agent. Only Agent may allocate flows inside the shared memory. Forwarding Extension may modify the shared memory contents, but it never directly communicates with vRouter Agent using it or creates new flows - it mostly updates flow statistics. Only Agent and \"flow\" util need access to shared memory from userspace. Since kernel memory is mapped into userspace, so the team ensured that the implementation is secure. pkt0 This channel is asynchronous - both vRouter Agent and Forwarding Extension can send packets through it at arbitrary times. When a \"new\" packet arrives at Forwarding Extension (for example, ARP or DHCP request), it will transfer it using pkt0 to vRouter Agent. vRouter Agent will then analyze the packet (possibly sending it to Contrail Controller for even more analysis), and insert ksync rules or flows into the Forwarding Extension. Then, it re-injects the packet into the Forwarding Extension. Pkt0 is implemented on Linux using a traditional networking interface, of AF_PACKET family. On Windows, this is done using a named pipe. The reason for this is that Windows implementation of Berkeley sockets does not allow any manipulation of any layer below IP. Named pipe server is created by the Forwarding Extension. Sandesh Sandesh is a binary protocol based on Apache Thrift. It's a set of header files to be included in projects that use it, but also a code generation tool. Sandesh uses own Interface Definition Language to generate code in many different languages, like Java, C, Python and Golang. Generated code consists of definitions of data structures and getter/setter functions.","title":"Communication Agent Extension"},{"location":"For developers/Technical documentation/Communication_Agent_Extension/#ksync","text":"This communication channel is used for inserting forwarding rules into the Forwarding Extension, like routes and next hops. On Linux, Netlink sockets are used for this. Equivalent Windows mechanism is a Named Pipe. Only vRouter Agent and other userspace programs can initialize the communiation. Howerver, different programs (for example, Agent and utils) may talk via ksync at the same time, so process separation (session layer) is required. This communication is performed over a binary protocol called Sandesh (the same one which is used by Contrail Analytics Nodes).","title":"Ksync"},{"location":"For developers/Technical documentation/Communication_Agent_Extension/#flow","text":"All flows are inserted into the Forwarding Extension using shared memory mechanism. Shared memory is allocated inside Forwarding Extension, and is written to by Agent. Only Agent may allocate flows inside the shared memory. Forwarding Extension may modify the shared memory contents, but it never directly communicates with vRouter Agent using it or creates new flows - it mostly updates flow statistics. Only Agent and \"flow\" util need access to shared memory from userspace. Since kernel memory is mapped into userspace, so the team ensured that the implementation is secure.","title":"Flow"},{"location":"For developers/Technical documentation/Communication_Agent_Extension/#pkt0","text":"This channel is asynchronous - both vRouter Agent and Forwarding Extension can send packets through it at arbitrary times. When a \"new\" packet arrives at Forwarding Extension (for example, ARP or DHCP request), it will transfer it using pkt0 to vRouter Agent. vRouter Agent will then analyze the packet (possibly sending it to Contrail Controller for even more analysis), and insert ksync rules or flows into the Forwarding Extension. Then, it re-injects the packet into the Forwarding Extension. Pkt0 is implemented on Linux using a traditional networking interface, of AF_PACKET family. On Windows, this is done using a named pipe. The reason for this is that Windows implementation of Berkeley sockets does not allow any manipulation of any layer below IP. Named pipe server is created by the Forwarding Extension.","title":"pkt0"},{"location":"For developers/Technical documentation/Communication_Agent_Extension/#sandesh","text":"Sandesh is a binary protocol based on Apache Thrift. It's a set of header files to be included in projects that use it, but also a code generation tool. Sandesh uses own Interface Definition Language to generate code in many different languages, like Java, C, Python and Golang. Generated code consists of definitions of data structures and getter/setter functions.","title":"Sandesh"},{"location":"For developers/Technical documentation/Container_creation_lifecycle/","text":"Docker client tells docker daemon to run a container and attach it to a Docker network (that corresponds to a chunk of a Contrail subnet). Docker deamon delegates the network configuration of the container that is being created to the remote docker driver. Docker driver queries docker daemon about metadata associated with docker's network resource and receives Contrail tenant name and subnet CIDR. Docker driver performs a series of queries against Contrail Controller that create various Contrail resources, most notably: virtual-machine, virtual-machine-interface, instance-ip. Docker driver receives Instance IP, MAC and default gateway to configure the newly created container's interface with. It also receives UUID of virtual-machine-interface resource Docker driver knows all the information necessary to recreate HNS Network's name. It uses this name to identify which HNS network to attach the container to. Docker driver sends requests to HNS to configure endpoint with newly received IP, MAC and Default gateway. In the background, HNS creates the network compartment for the container and configures its interface. The moment the interface is created, the Forwarding Extension receives an event about new adapter being connected. Forwarding Extension doesn't know what to do with it yet, so it stores it in a hash map, where the key is adapters FriendlyName. Then it waits, dropping all packets except the ones sent from native Hyper-V driver. Meanwhile, if container creation was successful, HNS returns ID of the endpoint to docker driver. Docker driver sends \"add port\" request to vRouter Agent, with all the necessary information to identify the port both in Contrail terms (UUID of virtual-machine-interface) as well as in Windows terms (using Endpoint ID, which is a part of FriendlyName). vRouter Agent communicates with Contrail Controller to let it know about newly added port. vRouter Agent inserts basic rules and flows into the Forwarding Extension. Forwarding Extension uses the FriendlyName to determine which port seen in userspace corresponds to port waiting in Forwarding Extension's hash map.","title":"Container creation lifecycle"},{"location":"For developers/Technical documentation/Docker_on_Windows/","text":"Docker Docker is a popular tools for managing containers on Linux. Since Windows Server 2016, docker is the main tool used for managing Windows Containers. Docker client Docker command line tool, which communicates with Docker Daemon over a named pipe. Docker daemon A program that runs as a service. It exposes an API over a named pipe. It's responsible for managing containers, from parsing Dockerfiles and pulling layers to basic networking. Docker is cross-platform and is integrated into many different network orchestration systems. This is achieved thanks to its plugin architecture. One can implement all kinds of plugins for docker, for example for volume or network management. These plugins can either be integrated into docker's code directly, or can be implemented as Remote drivers. The advantage of Remote drivers is that one doesn't have to build the whole Docker if she wants to implement a driver. Remote drivers on Windows can use tcp sockets or named pipes for communication. In either case, docker daemon will look for a \"spec file\" in a specific path: $Env:ProgramData\\docker\\plugins. The name of the spec file must be the same as driver's name. The file itself contains a protocol and address on which docker daemon will try to initiate communication. Since docker is written in Go, Microsoft engineers had to expose an API for managing Windows Containers written in this language. Host compute service (HCS) host network service (HNS) Golang wrapper for HCS and HNS is implemented in a public hcsshim repository: www.github.com/Microsoft/hcsshim. HCS and HNS are Windows Services responsible for light container virtualization as well as setting up virtual networking for them. They are implemented in some system dynamic libraries, which interact with the kernel directly. How it works isn't documented. Host compute service Deals with volumes, filesystem layers etc. It's related to HNS, but it's not inside scope of the project directly. Host network service HNS is a wrapper for syscalling functions in vmcompute.dll. Only one file in the whole hcsshim repository is used by HNS - 'hnsfuncs.go'. Windows Containers Windows Containers are a feature shipped with Windows Server 2016, Windows 10 and Windows Nano Server. These act similarly to lxc containers. On Windows, they are implemented using various namespaces, as well as network namespace equivalent (called network compartment). Network compartments are not documented, but vmcompute.dll contains some functions related to how they work. Unlike on Linux, Windows supports two types/levels of isolation for its containers: Windows Server containers, which work just like normal Linux containers - they all share the kernel Hyper-V containers. These are essentialy small VMs (running Nano Server), that run Windows Containers which then run the container we wanted. This additional level of virtualization is useful mostly for security reasons. It provides some pros and cons of both containers and VMs. To run a container in this mode, one needs to add \"--isolation-level=hyperv\" to \"docker run\" command. There are no differences on how networking is implemented for both of these types of containers. There are two base images for docker container creation: microsoft/windowsservercore which is headless version of normal Windows Server Core microsoft/nanoserver which is very lightweight, stripped of most functionality version of Windows Server Core","title":"Docker"},{"location":"For developers/Technical documentation/Docker_on_Windows/#docker","text":"Docker is a popular tools for managing containers on Linux. Since Windows Server 2016, docker is the main tool used for managing Windows Containers.","title":"Docker"},{"location":"For developers/Technical documentation/Docker_on_Windows/#docker-client","text":"Docker command line tool, which communicates with Docker Daemon over a named pipe.","title":"Docker client"},{"location":"For developers/Technical documentation/Docker_on_Windows/#docker-daemon","text":"A program that runs as a service. It exposes an API over a named pipe. It's responsible for managing containers, from parsing Dockerfiles and pulling layers to basic networking. Docker is cross-platform and is integrated into many different network orchestration systems. This is achieved thanks to its plugin architecture. One can implement all kinds of plugins for docker, for example for volume or network management. These plugins can either be integrated into docker's code directly, or can be implemented as Remote drivers. The advantage of Remote drivers is that one doesn't have to build the whole Docker if she wants to implement a driver. Remote drivers on Windows can use tcp sockets or named pipes for communication. In either case, docker daemon will look for a \"spec file\" in a specific path: $Env:ProgramData\\docker\\plugins. The name of the spec file must be the same as driver's name. The file itself contains a protocol and address on which docker daemon will try to initiate communication. Since docker is written in Go, Microsoft engineers had to expose an API for managing Windows Containers written in this language.","title":"Docker daemon"},{"location":"For developers/Technical documentation/Docker_on_Windows/#host-compute-service-hcs-host-network-service-hns","text":"Golang wrapper for HCS and HNS is implemented in a public hcsshim repository: www.github.com/Microsoft/hcsshim. HCS and HNS are Windows Services responsible for light container virtualization as well as setting up virtual networking for them. They are implemented in some system dynamic libraries, which interact with the kernel directly. How it works isn't documented.","title":"Host compute service (HCS) &amp; host network service (HNS)"},{"location":"For developers/Technical documentation/Docker_on_Windows/#host-compute-service","text":"Deals with volumes, filesystem layers etc. It's related to HNS, but it's not inside scope of the project directly.","title":"Host compute service"},{"location":"For developers/Technical documentation/Docker_on_Windows/#host-network-service","text":"HNS is a wrapper for syscalling functions in vmcompute.dll. Only one file in the whole hcsshim repository is used by HNS - 'hnsfuncs.go'.","title":"Host network service"},{"location":"For developers/Technical documentation/Docker_on_Windows/#windows-containers","text":"Windows Containers are a feature shipped with Windows Server 2016, Windows 10 and Windows Nano Server. These act similarly to lxc containers. On Windows, they are implemented using various namespaces, as well as network namespace equivalent (called network compartment). Network compartments are not documented, but vmcompute.dll contains some functions related to how they work. Unlike on Linux, Windows supports two types/levels of isolation for its containers: Windows Server containers, which work just like normal Linux containers - they all share the kernel Hyper-V containers. These are essentialy small VMs (running Nano Server), that run Windows Containers which then run the container we wanted. This additional level of virtualization is useful mostly for security reasons. It provides some pros and cons of both containers and VMs. To run a container in this mode, one needs to add \"--isolation-level=hyperv\" to \"docker run\" command. There are no differences on how networking is implemented for both of these types of containers. There are two base images for docker container creation: microsoft/windowsservercore which is headless version of normal Windows Server Core microsoft/nanoserver which is very lightweight, stripped of most functionality version of Windows Server Core","title":"Windows Containers"},{"location":"For developers/Technical documentation/Forwarding_extension/","text":"On Linux, a special vRouter Kernel Module is enabled to perform the heavy-duty, high-performance datapath functionality of Contrail. The Forwarding Extension is a Windows equivalent. It's implemented as a forwarding extension of Window's Hyper-V Extensible Switch. Note: Even though the name \"forwarding extension\" refers to the type of extension of Hyper-V Switch, the team calls the equivalent of Linux kernel module as \"the Forwarding Extension\". DP-core is a cross platform module that is responsible for datapath logic. Even though its design is cross platform (DP-core exposes a set of function pointers, that need to be implemented differently on different operating systems, for example memory allocation, packet inspection procedures, timer functions etc.), its implementation uses many gcc-specific built ins, which require porting over to Windows. Note: In theory, kernel extension compiled with GCC might work on Windows, but Visual Studio compiler is recommended. The Extension implements an NDIS (Network Driver Interface Specification) API. The NDIS API is event driven, and is realized in terms of callback functions. There is two-way interaction between DP-core and NDIS: Since the Extension is event driven, whenever an event occurs, a specific NDIS callback is ran - for example, if a packet is received, or a new port is connected. In those cases, DP-core's functions will need to be used inside the callback to enact vRouter logic and reach a forwarding decision. However, DP-core sometimes needs additional information to reach those decisions. It uses the \"callbacks\" in form of aforementioned function pointers to OS-specific implementations of those functionalities. Note: In other words, there are two \"kinds\" of callbacks: \"NDIS callbacks\" and \"DP-core callbacks\". NDIS callbacks are a consequence of event-driven architecture of Hyper-V Extensions, while DP-core callbacks are actually just platform specific \"dependency injections\" or \"C interfaces\". The team usually refers to those \"DP-core callbacks\" as just callbacks. NDIS callbacks are usually referred to as \"Extension/NDIS API implementation\". Note: There is a convenience library provided by Microsoft around the raw NDIS API. The library is referred to as SxBase, but it's just 4 source files. They can be found here: https://github.com/Microsoft/Windows-driver-samples/tree/master/network/ndis/extension/base. SxBase is used in the current implementation, but it will HAVE TO be replaced in the future, due to licensing issues. The Forwarding Extension does not directly communicate with Contrail Controller. Instead, vRouter Agent is responsible for creating all the objects and flows inside the Extension.","title":"Forwarding extension"},{"location":"For developers/Technical documentation/HyperV_extensible_switch/","text":"Windows Server 2012 introduced Hyper-V Extensible Switch. It\u2019s a virtual switch that runs in OS running in Hyper-V\u2019s parent partition. It\u2019s mainly used for forwarding packets flowing in and out of child partitions (VMs and containers). Three types of switches can be instantiated: private - only interfaces connected to the vSwitch can communicate internal - like private, but the host (hypervisor) is also connected external - like internal, but external network adapter is also connected. Usually, this is the physical adapter. Multiple external switches may exist at any given time. Hyper-V Extensible Switch is \"Extensible\", which means it can be extended using third party extensions (plugins). Multiple extensions can be enabled at the same time. Hyper-V Switch's plugin architecture resembles a stack. The order in which ingress and egress packets will be processed by a specific extension depends on their position in the filter stack. There are three types of extensions: capture - which can inspect packets, and, for example, log them somewhere filter - like capture, but has the ability to drop packets forwarding - like filter, but has the ability to forward and modify the packets. Multiple instances of capture and filter extensions can be enabled in arbitrary locations in the extensions stack, but only one forwarding extension can be enabled at a time. It must be located at the bottom of the stack as well, which means that it is the last to process ingress packets, but the first to process egress packets.","title":"HyperV extensible switch"},{"location":"For developers/Technical documentation/Linux_Overview/","text":"Contrail Controller Contrail Controller is a logical component. In fact, it's a distributed system consisting of many nodes, most importantly: Config nodes. These nodes expose a REST API used by various applications to configure Contrail resources. Contrail's web client communicates with it, and so does Docker Network Driver. Config information is eventually propagated to Control nodes. Control nodes, responsible for interacting with vRouter Agent via XMPP protocol. These interactions include inserting forwarding rules and flows and sending packets for further analysis. Analytics nodes, which collect high volume information using Sandesh binary protocol. vRouter Agent mainly interacts with Control and Analytics nodes, while Docker network driver interacts with Config nodes. Compute Node Compute Node is a Contrail component. It's a hypervisor that is able to spawn VMs or containers. Every compute node must have an instance of vRouter Agent running. Windows Server 2016 and Nano Server are new versions of Microsoft's OS, that can act both as Hyper-V hypervisor as well as Windows Containers host. Nano Server running as Compute Node is currently not supported, due to lacking the capability of installing Hyper-V networking extensions. vRouter Agent Note: The team sometimes refers to vRouter Agent simply as \"vRouter\", \"Agent\" or \"vRA\". vRouter Agent is a Contrail agent that runs in userspace on a compute node. It has many functions. Most importantly: it communicates with Contrail Controller via XMPP protocol. Controller will inject rules and flows into the agent so that it enacts it's routing policies and implements network virtualization, exposes an API for the hypervisor, that is used for registering newly created virtual machines or containers. This * is most notably used in Contrail-OpenStack integration, where Nova-Agent informs vRouter Agent whenever a new VM is spawned. On Windows, the API is exposed over a named pipe. communicates with the Forwarding Extension (three communication channels in total). Since vRouter Agent is a control plane component, it must inject all low level information like next hops, virtual interface information, routes and flows into the datapath component (the Forwarding Extension). sometimes forwards packets from Forwarding Extension to Contrail Controller sometimes injects packets sent from Contrail Controller into the Forwarding Extension","title":"Contrail Controller"},{"location":"For developers/Technical documentation/Linux_Overview/#contrail-controller","text":"Contrail Controller is a logical component. In fact, it's a distributed system consisting of many nodes, most importantly: Config nodes. These nodes expose a REST API used by various applications to configure Contrail resources. Contrail's web client communicates with it, and so does Docker Network Driver. Config information is eventually propagated to Control nodes. Control nodes, responsible for interacting with vRouter Agent via XMPP protocol. These interactions include inserting forwarding rules and flows and sending packets for further analysis. Analytics nodes, which collect high volume information using Sandesh binary protocol. vRouter Agent mainly interacts with Control and Analytics nodes, while Docker network driver interacts with Config nodes.","title":"Contrail Controller"},{"location":"For developers/Technical documentation/Linux_Overview/#compute-node","text":"Compute Node is a Contrail component. It's a hypervisor that is able to spawn VMs or containers. Every compute node must have an instance of vRouter Agent running. Windows Server 2016 and Nano Server are new versions of Microsoft's OS, that can act both as Hyper-V hypervisor as well as Windows Containers host. Nano Server running as Compute Node is currently not supported, due to lacking the capability of installing Hyper-V networking extensions.","title":"Compute Node"},{"location":"For developers/Technical documentation/Linux_Overview/#vrouter-agent","text":"Note: The team sometimes refers to vRouter Agent simply as \"vRouter\", \"Agent\" or \"vRA\". vRouter Agent is a Contrail agent that runs in userspace on a compute node. It has many functions. Most importantly: it communicates with Contrail Controller via XMPP protocol. Controller will inject rules and flows into the agent so that it enacts it's routing policies and implements network virtualization, exposes an API for the hypervisor, that is used for registering newly created virtual machines or containers. This * is most notably used in Contrail-OpenStack integration, where Nova-Agent informs vRouter Agent whenever a new VM is spawned. On Windows, the API is exposed over a named pipe. communicates with the Forwarding Extension (three communication channels in total). Since vRouter Agent is a control plane component, it must inject all low level information like next hops, virtual interface information, routes and flows into the datapath component (the Forwarding Extension). sometimes forwards packets from Forwarding Extension to Contrail Controller sometimes injects packets sent from Contrail Controller into the Forwarding Extension","title":"vRouter Agent"},{"location":"For developers/Technical documentation/NBL_processing/","text":"NET_BUFFER_LIST processing NET_BUFFER_LIST and vr_packet relationship NET_BUFFER_LIST is a linked list of packet containers NET_BUFFER represents a single packet MDL linked list represent a series of memory buffers Windows API mostly accepts NET_BUFFER_LIST structures vRouter expects vr_packet to be a single network packet Thus Windows vRouter code must assure that vr_packet will encapsulate: single NET_BUFFER_LIST single NET_BUFFER Receiving fragmented packets from container Scenario : Packet is fragmented in container, before reaching vRouter Extension receives a single NET_BUFFER_LIST with a set of NET_BUFFER structures Each NET_BUFFER should be in its own NET_BUFFER_LIST to fulfill single network packet - vr_packet relationship required by dp-core Fragmenting egress tunneled packets Scenario : Packet must be tunneled, but it's too big, thus must be fragmented by vRouter Windows API supports dividing packet into fragments Although it's vRouter responsibility to: Add fragmentation info to IP headers Fix packet headers (e.g. header checksums) Note: vRouter on Linux fragments the inner packet As a result each fragment is encapsulated separately vRouter on Windows implements the same behaviour","title":"NET_BUFFER_LIST processing"},{"location":"For developers/Technical documentation/NBL_processing/#net_buffer_list-processing","text":"","title":"NET_BUFFER_LIST processing"},{"location":"For developers/Technical documentation/NBL_processing/#net_buffer_list-and-vr_packet-relationship","text":"NET_BUFFER_LIST is a linked list of packet containers NET_BUFFER represents a single packet MDL linked list represent a series of memory buffers Windows API mostly accepts NET_BUFFER_LIST structures vRouter expects vr_packet to be a single network packet Thus Windows vRouter code must assure that vr_packet will encapsulate: single NET_BUFFER_LIST single NET_BUFFER","title":"NET_BUFFER_LIST and vr_packet relationship"},{"location":"For developers/Technical documentation/NBL_processing/#receiving-fragmented-packets-from-container","text":"Scenario : Packet is fragmented in container, before reaching vRouter Extension receives a single NET_BUFFER_LIST with a set of NET_BUFFER structures Each NET_BUFFER should be in its own NET_BUFFER_LIST to fulfill single network packet - vr_packet relationship required by dp-core","title":"Receiving fragmented packets from container"},{"location":"For developers/Technical documentation/NBL_processing/#fragmenting-egress-tunneled-packets","text":"Scenario : Packet must be tunneled, but it's too big, thus must be fragmented by vRouter Windows API supports dividing packet into fragments Although it's vRouter responsibility to: Add fragmentation info to IP headers Fix packet headers (e.g. header checksums) Note: vRouter on Linux fragments the inner packet As a result each fragment is encapsulated separately vRouter on Windows implements the same behaviour","title":"Fragmenting egress tunneled packets"},{"location":"For developers/Technical documentation/NDIS_driver_types_overview/","text":"NDIS (Network Driver Interface Specification) driver types overview Miniport drivers The lowest drivers in NDIS driver stack. They manage a network interface card, send and receive data through the NIC. Protocol drivers The highest drivers in NDIS driver stack. The lowest drivers in transport driver stack. They allocate packets, copy data from sending application and pass the packets to lower level driver. Filter drivers Filter drivers are placed between protocol and miniport drivers in the driver stack. Multiple filter drivers can be placed one above the other. They are transparent to overlaying protocol driver and underlying miniport driver. They can be dynamically inserted, removed and reconfigured without stopping the whole driver stack. Filter drivers can be divided into two categories: - Monitoring filter drivers - They receive all network request and indications and pass them to other drivers. They cannot modify or originate data. They also cannot modify behavior of the driver stack. May be used for monitoring network requests. Modifying filter drivers - These drivers are allowed to modify the driver stack. The type of modification depends on the type of the driver: Capturing extensions - They can monitor packets like Monitoring filter drivers. They also cannot modify or drop packets received from overlaying driver. However, they can originate their own packet traffic and intercept indications for this traffic. They may be used for example for monitoring and sending network statistics. Filtering extensions - In addition to capturing extensions, they can inspect, modify and drop packets. Forwarding extensions - In addition to filtering extensions, they can forward packets and change theirs destination (packets routing). Only one forwarding extension can be enabled for each extensible switch. Intermediate drivers Miniport drivers see them as a Protocol drivers. Protocol drivers see them as a Miniport drivers. They translate between different network media. May be used for balancing transmission across more than one NIC. WFP (Windows Filtering Platform) A framework used for creating network filtering applications. Allows writing functions that may interact with packet processing at several network layers. Mainly used for implementing firewalls, intrusion detection systems, antivirus programs, network monitoring tools, and parental controls.","title":"NDIS (Network Driver Interface Specification) driver types overview"},{"location":"For developers/Technical documentation/NDIS_driver_types_overview/#ndis-network-driver-interface-specification-driver-types-overview","text":"","title":"NDIS (Network Driver Interface Specification) driver types overview"},{"location":"For developers/Technical documentation/NDIS_driver_types_overview/#miniport-drivers","text":"The lowest drivers in NDIS driver stack. They manage a network interface card, send and receive data through the NIC.","title":"Miniport drivers"},{"location":"For developers/Technical documentation/NDIS_driver_types_overview/#protocol-drivers","text":"The highest drivers in NDIS driver stack. The lowest drivers in transport driver stack. They allocate packets, copy data from sending application and pass the packets to lower level driver.","title":"Protocol drivers"},{"location":"For developers/Technical documentation/NDIS_driver_types_overview/#filter-drivers","text":"Filter drivers are placed between protocol and miniport drivers in the driver stack. Multiple filter drivers can be placed one above the other. They are transparent to overlaying protocol driver and underlying miniport driver. They can be dynamically inserted, removed and reconfigured without stopping the whole driver stack. Filter drivers can be divided into two categories: - Monitoring filter drivers - They receive all network request and indications and pass them to other drivers. They cannot modify or originate data. They also cannot modify behavior of the driver stack. May be used for monitoring network requests. Modifying filter drivers - These drivers are allowed to modify the driver stack. The type of modification depends on the type of the driver: Capturing extensions - They can monitor packets like Monitoring filter drivers. They also cannot modify or drop packets received from overlaying driver. However, they can originate their own packet traffic and intercept indications for this traffic. They may be used for example for monitoring and sending network statistics. Filtering extensions - In addition to capturing extensions, they can inspect, modify and drop packets. Forwarding extensions - In addition to filtering extensions, they can forward packets and change theirs destination (packets routing). Only one forwarding extension can be enabled for each extensible switch.","title":"Filter drivers"},{"location":"For developers/Technical documentation/NDIS_driver_types_overview/#intermediate-drivers","text":"Miniport drivers see them as a Protocol drivers. Protocol drivers see them as a Miniport drivers. They translate between different network media. May be used for balancing transmission across more than one NIC.","title":"Intermediate drivers"},{"location":"For developers/Technical documentation/NDIS_driver_types_overview/#wfp-windows-filtering-platform","text":"A framework used for creating network filtering applications. Allows writing functions that may interact with packet processing at several network layers. Mainly used for implementing firewalls, intrusion detection systems, antivirus programs, network monitoring tools, and parental controls.","title":"WFP (Windows Filtering Platform)"},{"location":"For developers/Technical documentation/Network_creation_lifecycle/","text":"There is a slight discrepancy between Docker's and Contrail's networking model. Contrail can implement a logical, overlay network for containers. However, docker can only create a network locally, on the hypervisor. This means that \"docker network create\" command needs to be ran on each hypervisor that will contain any container that could be connected to a specific network. In other words, \"local\" networks must be prepared on each host. Furthermore, Docker's \"network\" is actually equivalent to Contrail's \"subnet\". This means, that during docker network creation, specific Contrail subnet (using CIDR notation) must be specified as a parameter. Docker client tells docker daemon to create a docker network that will represent a chunk of a Contrail subnet. Tenant name, network name and subnet's CIDR are passed as parameters. Docker daemon creates a network resource in its own, local database, and then delegates handling of network configuration to the docker driver. Docker driver queries Contrail Controller whether specified tenant, network and subnet combination exists. It also asks for their details. Contrail Controller returns subnet's default gateway address as well as subnet's UUID Docker driver calls HNS function to create a HNS network with subnet's CIDR and default gateway address. It also sets the HNS network's name as a concatenation of \"Contrail\" with tenant name, network name and subnet ID. Docker driver tells docker daemon to store tenant and network names in docker network's metadata. Docker daemon stores this information in its own, local database.","title":"Network creation lifecycle"},{"location":"For developers/Technical documentation/Offloading_IP_checksums_and_other_tasks/","text":"Offloading IP Checksums and other tasks Introduction Note: Offloading basically means \"let somebody else do it\". There are various tasks related to the IP stack that can be offloaded, such as computing checksums or segmenting large TCP packets. It is beneficial to perform these offloads as late as possible. In the best case, the offloading is performed in the network card hardware, so eg. no CPU cycles are wasted on computing the TCP checksum. Sometimes the offloading is not supported by the hardware, but still can be performed in the software. This is best described on an example \u2014 the LSO (Large (TCP) Send Offload) means that for the most time spent in the kernel the TCP packet can be stored as one single NET_BUFFER , bigger than a single Ethernet frame. That means that all operations like filtering, routing, etc. can operate efficiently (because they look at a single packet instead of many) and the actual segmenting will be performed just before passing the buffer to the hardware. In the case of container-to-container connection, the actual segmentation can even never happen! As they say, the fastest code is the code that never runs. Offloading in Hyper-V Virtual Switches Note: Refer to the blog post about VMSwitches and offloading for more information. Usually, the OS Networking stack asks the network device about types of offloading it supports and offloads only the tasks that the hardware is capable of offloading. With VMSwitch it's not that simple, because it's hard to propagate the hardware capabilities to all the ports in VMSwitch (especially because when sending a packet into VMSwitch, we don't know where will it end up: a container, a virtual machine, a real network card?). To sidestep this uneven-offload-support issue, VMSwitch always advertises on all its ports that most offloads are supported. Therefore, any VMSwitch Extension must be prepared to handle NBLs with these offloads requested. The offloads that are always supported are: IP, TCP, UDP send checksum offload. LSO (Large Send Offload), which is basically TCP Segmentation offload. Receive checksum offload (VMSwitch with the help of hardware or by itself will set appropriate flags that mark the validity of checksums in incoming packets). So yes, we \u2014 vRouter VMSwitch Extension \u2014 must handle all these offload requests. But this relationship is symmetric \u2014 when sending packets we can rely on support for these offloads on all vPorts! Other offloads than listed above are not always advertised as supported. Encapsulated Packet Offload The most interesting type of offload which is not always advertised as supported by VMSwitch is the Encapsulated Packet Offload . In theory it enables to perform all the standard offload tasks (like computing checksums and Large Send Offload) on the inner packet, despite it being wrapped in a tunnel. It also allows to simultaneously offload the checksums of the tunneling headers themselves. Unfortunately it requires support from the network device or its driver. To check for support and enable the feature you can use the following commandlests: Get-NetAdapterEncapsulatedPacketTaskOffload Enable-NetAdapterEncapsulatedPacketTaskOffload The drivers used in our test environments (vmxnet3) do not seem to support this feature (at least, we haven't managed to enable it using these PowerShell commandlets). If we try to use it anyway by setting appropriate NBL Info, the packet gets rejected. Only NVGRE? The documentation on Encapsulated Packet Offload page suggests that this feature can only be used with NVGRE tunneling. It never explicitly states it cannot be used with other kinds of tunneling though. On the other hand, the fields of the union that describes the parameters for EPO allow specifying arbitrary offsets between outer and inner headers, suggesting that this feature can be used with different tunneling protocols. In general, this feature is a little bit underdocumented, so it's hard to get a definite answer whether other tunnellings than NVGRE are supported. Note: There's a small chance that the reason our experiments with Encapsulated Packet Offload had failed was not the lack of support from the driver, but rather the fact we were trying to use MPLSoGRE, not NVGRE. This seems very unlikely though. Handling Net Buffer List Info All the information about offloading requests/responses is stored in various Net Buffer List Info unions . There are a few important things to note about these: Many of these unions have Transmit and Receive variants. A care should be taken when reading these, as only one variant is active at once \u2014 packets incoming from vHost or containers' ports are using Transmit variants and packets incoming from external interface are using Receive variants. Reading from inactive variant will result in reading garbage. Net Buffer List Info is not copied when using NdisAllocateCloneNetBufferList . When performing offload tasks in software, it's important to disable offloading requests by zeroing these fields. Not doing that will result in unexpected results. It's also important to remember that some offload tasks require writing additional info on Complete, like number of transfered bytes in LSO. When the packet is wrapped in a tunnel, the fields refer to the outer packet. A care should be taken to check whether these fields are still correct. Interaction with dp-core The LSO (Large Send Offload) feature on Windows is really similar to GSO (Generic Send Offload) on Linux and dp-core. To support it, we only need to provide LSO-requested MSS in a getter. vr_packet has some flags for GRO (Generic Receive Offload). Analogous Windows feature is LRO (Large Receive Offload). How to handle it in dp-core and whether it is necessary wasn't investigated yet. When the checksum offload was requested, packets will have the partial checksum already computed and stored in the TCP/UDP checksum field. When dp-core tries to incrementally update checksums, it needs to know whether the checksum fields contains full or partial checksum, in order to correctly update the field. The VR_FLAG_CSUM_PARTIAL flag must be set accordingly. Possible performance improvements The offloading request for TCP checksum contains the offset of TCP header \u2014 that means it should be possible to leverage the TCP checksum offload also on tunnelled packets (by providing adjusted offset to inner TCP header, which includes the outer headers length). Checksums of outer headers could be offloaded unconditionally. Currently, we do TCP segmentation of tunnelled packets manually. It should be possible to use Encapsulated Packet Offload on a supported hardware. References TCP/IP Task Offload (docs.microsoft.com) Hyper-V Virtual Switch Performance \u2014 Offloads Network Virtualization using Generic Routing Encapsulation (NVGRE) Task Offload TCP Checksum Calculation and the TCP \"Pseudo Header\"","title":"Offloading IP Checksums and other tasks"},{"location":"For developers/Technical documentation/Offloading_IP_checksums_and_other_tasks/#offloading-ip-checksums-and-other-tasks","text":"","title":"Offloading IP Checksums and other tasks"},{"location":"For developers/Technical documentation/Offloading_IP_checksums_and_other_tasks/#introduction","text":"Note: Offloading basically means \"let somebody else do it\". There are various tasks related to the IP stack that can be offloaded, such as computing checksums or segmenting large TCP packets. It is beneficial to perform these offloads as late as possible. In the best case, the offloading is performed in the network card hardware, so eg. no CPU cycles are wasted on computing the TCP checksum. Sometimes the offloading is not supported by the hardware, but still can be performed in the software. This is best described on an example \u2014 the LSO (Large (TCP) Send Offload) means that for the most time spent in the kernel the TCP packet can be stored as one single NET_BUFFER , bigger than a single Ethernet frame. That means that all operations like filtering, routing, etc. can operate efficiently (because they look at a single packet instead of many) and the actual segmenting will be performed just before passing the buffer to the hardware. In the case of container-to-container connection, the actual segmentation can even never happen! As they say, the fastest code is the code that never runs.","title":"Introduction"},{"location":"For developers/Technical documentation/Offloading_IP_checksums_and_other_tasks/#offloading-in-hyper-v-virtual-switches","text":"Note: Refer to the blog post about VMSwitches and offloading for more information. Usually, the OS Networking stack asks the network device about types of offloading it supports and offloads only the tasks that the hardware is capable of offloading. With VMSwitch it's not that simple, because it's hard to propagate the hardware capabilities to all the ports in VMSwitch (especially because when sending a packet into VMSwitch, we don't know where will it end up: a container, a virtual machine, a real network card?). To sidestep this uneven-offload-support issue, VMSwitch always advertises on all its ports that most offloads are supported. Therefore, any VMSwitch Extension must be prepared to handle NBLs with these offloads requested. The offloads that are always supported are: IP, TCP, UDP send checksum offload. LSO (Large Send Offload), which is basically TCP Segmentation offload. Receive checksum offload (VMSwitch with the help of hardware or by itself will set appropriate flags that mark the validity of checksums in incoming packets). So yes, we \u2014 vRouter VMSwitch Extension \u2014 must handle all these offload requests. But this relationship is symmetric \u2014 when sending packets we can rely on support for these offloads on all vPorts! Other offloads than listed above are not always advertised as supported.","title":"Offloading in Hyper-V Virtual Switches"},{"location":"For developers/Technical documentation/Offloading_IP_checksums_and_other_tasks/#encapsulated-packet-offload","text":"The most interesting type of offload which is not always advertised as supported by VMSwitch is the Encapsulated Packet Offload . In theory it enables to perform all the standard offload tasks (like computing checksums and Large Send Offload) on the inner packet, despite it being wrapped in a tunnel. It also allows to simultaneously offload the checksums of the tunneling headers themselves. Unfortunately it requires support from the network device or its driver. To check for support and enable the feature you can use the following commandlests: Get-NetAdapterEncapsulatedPacketTaskOffload Enable-NetAdapterEncapsulatedPacketTaskOffload The drivers used in our test environments (vmxnet3) do not seem to support this feature (at least, we haven't managed to enable it using these PowerShell commandlets). If we try to use it anyway by setting appropriate NBL Info, the packet gets rejected.","title":"Encapsulated Packet Offload"},{"location":"For developers/Technical documentation/Offloading_IP_checksums_and_other_tasks/#only-nvgre","text":"The documentation on Encapsulated Packet Offload page suggests that this feature can only be used with NVGRE tunneling. It never explicitly states it cannot be used with other kinds of tunneling though. On the other hand, the fields of the union that describes the parameters for EPO allow specifying arbitrary offsets between outer and inner headers, suggesting that this feature can be used with different tunneling protocols. In general, this feature is a little bit underdocumented, so it's hard to get a definite answer whether other tunnellings than NVGRE are supported. Note: There's a small chance that the reason our experiments with Encapsulated Packet Offload had failed was not the lack of support from the driver, but rather the fact we were trying to use MPLSoGRE, not NVGRE. This seems very unlikely though.","title":"Only NVGRE?"},{"location":"For developers/Technical documentation/Offloading_IP_checksums_and_other_tasks/#handling-net-buffer-list-info","text":"All the information about offloading requests/responses is stored in various Net Buffer List Info unions . There are a few important things to note about these: Many of these unions have Transmit and Receive variants. A care should be taken when reading these, as only one variant is active at once \u2014 packets incoming from vHost or containers' ports are using Transmit variants and packets incoming from external interface are using Receive variants. Reading from inactive variant will result in reading garbage. Net Buffer List Info is not copied when using NdisAllocateCloneNetBufferList . When performing offload tasks in software, it's important to disable offloading requests by zeroing these fields. Not doing that will result in unexpected results. It's also important to remember that some offload tasks require writing additional info on Complete, like number of transfered bytes in LSO. When the packet is wrapped in a tunnel, the fields refer to the outer packet. A care should be taken to check whether these fields are still correct.","title":"Handling Net Buffer List Info"},{"location":"For developers/Technical documentation/Offloading_IP_checksums_and_other_tasks/#interaction-with-dp-core","text":"The LSO (Large Send Offload) feature on Windows is really similar to GSO (Generic Send Offload) on Linux and dp-core. To support it, we only need to provide LSO-requested MSS in a getter. vr_packet has some flags for GRO (Generic Receive Offload). Analogous Windows feature is LRO (Large Receive Offload). How to handle it in dp-core and whether it is necessary wasn't investigated yet. When the checksum offload was requested, packets will have the partial checksum already computed and stored in the TCP/UDP checksum field. When dp-core tries to incrementally update checksums, it needs to know whether the checksum fields contains full or partial checksum, in order to correctly update the field. The VR_FLAG_CSUM_PARTIAL flag must be set accordingly.","title":"Interaction with dp-core"},{"location":"For developers/Technical documentation/Offloading_IP_checksums_and_other_tasks/#possible-performance-improvements","text":"The offloading request for TCP checksum contains the offset of TCP header \u2014 that means it should be possible to leverage the TCP checksum offload also on tunnelled packets (by providing adjusted offset to inner TCP header, which includes the outer headers length). Checksums of outer headers could be offloaded unconditionally. Currently, we do TCP segmentation of tunnelled packets manually. It should be possible to use Encapsulated Packet Offload on a supported hardware.","title":"Possible performance improvements"},{"location":"For developers/Technical documentation/Offloading_IP_checksums_and_other_tasks/#references","text":"TCP/IP Task Offload (docs.microsoft.com) Hyper-V Virtual Switch Performance \u2014 Offloads Network Virtualization using Generic Routing Encapsulation (NVGRE) Task Offload TCP Checksum Calculation and the TCP \"Pseudo Header\"","title":"References"},{"location":"For developers/Technical documentation/Root_HNS_network/","text":"HNS operates by using two virtual switches: an external vswitch, which is connected directly to physical adapter. This adapter is called \"vEthernet (HNSTransparent)\", and the switch is called \"Layered Etherenet0\" depending on which physicial adapter it's connected to. If there is physical NIC teaming, then its name contains all the physical adapters separated by a coma, like \"Layered Ethernet0,Ethernet1\" an internal vswitch, for the purpose of NATing. The adapter that connects to the host is named \"vEthernet (HNS Internal NIC)\", and the switch is called \"nat\". Those virtual switches are not created unless there is at least one HNS network using corresponding Mode: \"transparent\" or \"nat\". If the last network of corresponding Mode is removed, the vswitch is also removed. Creation/deletion of vswitch takes a couple seconds and disrupts network connectivity. This dynamism of creating/deleting vswitches poses a problem, because we want vRouter Forwarding Extension to be persistent, no matter if there are many or no virtual networks. That's why, when Docker Driver initializes, it creates a \"dummy\" Root HNS Network, which makes HNS create an external vswitch \"Layered Ethernet*\". Forwarding Extension is then enabled for that vswitch.","title":"Root HNS network"},{"location":"For developers/Technical documentation/Utility_tools/","text":"A set of utility tools (sometimes referred to simply as \"Utils\") useful for manually injecting rules into the datapath or for introspect and debugging. They communicate directly with the Forwarding Extension. There are split into two categories: Basic utils These are used for basic vRouter introspection, but also for manual insertion of own rules and objects into the Forwarding Extension. vif - used for showing and creating virtual interfaces nh - used for showing and creating next hops rt - used for showing and creating routes flow - used for showing flows The fifth \"basic util\" is a test util. Since kernel drivers are hard to test without moving them into userspace, Juniper decided to implement a tool that simulates normal usage. vtest - runs playback tests against the Forwarding Extension Extended utils These are mainly used for statistics and advanced debugging. vxlan vrmemstats vrfstats qosmap mpls dropstats mirror","title":"Utility tools"},{"location":"For developers/Technical documentation/Utility_tools/#basic-utils","text":"These are used for basic vRouter introspection, but also for manual insertion of own rules and objects into the Forwarding Extension. vif - used for showing and creating virtual interfaces nh - used for showing and creating next hops rt - used for showing and creating routes flow - used for showing flows The fifth \"basic util\" is a test util. Since kernel drivers are hard to test without moving them into userspace, Juniper decided to implement a tool that simulates normal usage. vtest - runs playback tests against the Forwarding Extension","title":"Basic utils"},{"location":"For developers/Technical documentation/Utility_tools/#extended-utils","text":"These are mainly used for statistics and advanced debugging. vxlan vrmemstats vrfstats qosmap mpls dropstats mirror","title":"Extended utils"},{"location":"For developers/Technical documentation/vRouter_Linux_implementation/","text":"On Linux, vRouter implementation consists of two parts: a userspace agent and a kernel module, along with all the userspace utility command line tools. The agent communicates with OpenContrail\u2019s control nodes and receives configuration state, like forwarding information. Its main responsibility is installation of forwarding state into the kernel module. Functionality of the data plane, like flow tables, is contained in dp-core component. Translation between the agent\u2019s object model and low-level data model used by datapath is realized by the KSync module, which is utilized by both the agent and the kernel module. On Linux, communication between the forwarding plane, which runs in kernel space, and the user space agent, is done via use of Netlink sockets and shared memory. However, neither BSD nor Windows support Netlink sockets. That\u2019s why, on FreeBSD, they are emulated over raw sockets.","title":"vRouter Linux implementation"},{"location":"For developers/Updating this documentation/How_to_update_this_documentation/","text":"How to update this documentation How does it work Documentation is stored on master branch of Juniper/contrail-windows-docs repository. It is stored in Markdown (.md) format. Documentation is served as HTML on github.io. HTML pages are generated by MkDocs tool. HTML pages need to be redeployed manually after modification. Documentation update procedure (procedure for developer) Open a pull request to master branch in repository Juniper/contrail-windows-docs with your changes. Once PR is accepted, an administrator is going to deploy new modifications to gh-pages branch. How to preview modifications (procedure for developer) Install MkDocs (see below). Apply required modifications to the documentation. Invoke mkdocs serve in main repository directory. It's going to start local HTTP server with documentation preview. Follow instructions displayed by mkdocs to see the outcome. How to install MkDocs (procedure for administrator and developer) MkDocs requires Python (both Python 2 and Python 3 are supported) and PIP. Follow instructions on http://www.mkdocs.org/#installation . Create a virtualenv mkdir venv virtualenv --python=python2 venv source venv/bin/activate Install requirements pip install -r requirements.txt How to deploy modifications (procedure for administrator) After master branch of Juniper/contrail-windows-docs repository is updated, github.io pages need to be redeployed manually. To do this, follow these steps: Install MkDocs. Checkout master branch of repository Juniper/contrail-windows-docs. Invoke mkdocs gh-deploy in main repository directory and follow displayed instructions. Additional details are described in http://www.mkdocs.org/user-guide/deploying-your-docs/ .","title":"How to update this documentation"},{"location":"For developers/Updating this documentation/How_to_update_this_documentation/#how-to-update-this-documentation","text":"","title":"How to update this documentation"},{"location":"For developers/Updating this documentation/How_to_update_this_documentation/#how-does-it-work","text":"Documentation is stored on master branch of Juniper/contrail-windows-docs repository. It is stored in Markdown (.md) format. Documentation is served as HTML on github.io. HTML pages are generated by MkDocs tool. HTML pages need to be redeployed manually after modification.","title":"How does it work"},{"location":"For developers/Updating this documentation/How_to_update_this_documentation/#documentation-update-procedure-procedure-for-developer","text":"Open a pull request to master branch in repository Juniper/contrail-windows-docs with your changes. Once PR is accepted, an administrator is going to deploy new modifications to gh-pages branch.","title":"Documentation update procedure (procedure for developer)"},{"location":"For developers/Updating this documentation/How_to_update_this_documentation/#how-to-preview-modifications-procedure-for-developer","text":"Install MkDocs (see below). Apply required modifications to the documentation. Invoke mkdocs serve in main repository directory. It's going to start local HTTP server with documentation preview. Follow instructions displayed by mkdocs to see the outcome.","title":"How to preview modifications (procedure for developer)"},{"location":"For developers/Updating this documentation/How_to_update_this_documentation/#how-to-install-mkdocs-procedure-for-administrator-and-developer","text":"MkDocs requires Python (both Python 2 and Python 3 are supported) and PIP. Follow instructions on http://www.mkdocs.org/#installation . Create a virtualenv mkdir venv virtualenv --python=python2 venv source venv/bin/activate Install requirements pip install -r requirements.txt","title":"How to install MkDocs (procedure for administrator and developer)"},{"location":"For developers/Updating this documentation/How_to_update_this_documentation/#how-to-deploy-modifications-procedure-for-administrator","text":"After master branch of Juniper/contrail-windows-docs repository is updated, github.io pages need to be redeployed manually. To do this, follow these steps: Install MkDocs. Checkout master branch of repository Juniper/contrail-windows-docs. Invoke mkdocs gh-deploy in main repository directory and follow displayed instructions. Additional details are described in http://www.mkdocs.org/user-guide/deploying-your-docs/ .","title":"How to deploy modifications (procedure for administrator)"},{"location":"Known_issues/HNS_error_bestiary/","text":"HNS errors bestiary HNS error messages ofter aren't descriptive enough, and the cleanup is quite hard. The following document describes common cleanup techniques and error interpretations. HNS Decontamination procedures Before working with HNS, one must realize the risks. There are many ways in which HNS can stop working and affect other parts of the system. For a developer, it is important to be able to recognize those, which can be recovered from. Below are some techniques which may help. Always try cleaning using the lowest level procedure. The higher the level, the more potential damage the cleanup can cause. A word of caution Never develop HNS on a baremetal or own laptop. Level Alpha decontamination procedure First, stop the docker service. Stop-Service docker Remove all virtual switches, container networks and NAT Get-ContainerNetwork | Remove-ContainerNetwork Get-VMSwitch | Remove-VMSwitch Get-NetNAT | Remove-NetNAT Restart HNS and docker services. Restart-Service hns Restart-Service docker Level Beta decontamination procedure Remove all container networks. Always do this because some vmswitches may not be removed properly if you just execute the next command. Get-ContainerNetwork | Remove-ContainerNetwork Manually remove HNS.data file. net command sometimes works better at restarting HNS than PowerShell's Restart-Service . Warning: this is not recommended by Microsoft, but they do it in their official cleanup script, so I guess it's not that bad. net stop hns; del C:\\programdata\\Microsoft\\Windows\\HNS\\HNS.data; net start hns; Level Gamma decontamination procedure Use official Microsoft script to cleanup. It will also cleanup some registry entries and do much more. https://github.com/MicrosoftDocs/Virtualization-Documentation/tree/live/windows-server-container-tools/CleanupContainerHostNetworking Level [REDACTED] decontamination procedure Everything is lost. All we can do wipe all devices and hope that the contamination won't spread to other hosts. Warning: this might BSOD. // perform as single command because you will lose connectivity during netcfg -D netcfg -D; Restart-Computer -force HNS error list The following chapter contains a list of HNS errors encountered throughout development, along with steps to reproduce and occurence scenarios. 1. HNS Unspecified Error When it happens When attempting to create a transparent HNS network when creation of another network or VMSwitch is already in progress (eg. we've just started Docker service and it tries to create the NAT network). We can work around this bug by retrying after a few seconds. Steps to reproduce Try to create multiple HNS networks in a loop simultaneously with multiple processes. We suspect that this error occurs during a high load. 2. HNS Invalid Parameter When it happens TODO Steps to reproduce TODO 3. HNS Element not found When it happens [Hypothesis] When attempting to create a transparent docker network when no other transparent docker network exists and Ethernet adapter to be used by the transparent networks has no IP address or it's invalid. Steps to reproduce See https://github.com/Microsoft/hcsshim/issues/95 4. HNS failed with error : {Object Exists} An attempt was made to create an object and the object name already exists When it happens This error probably happens when docker tries to create NAT network, but HNS left over some trash after last NAT network. Cleanup everything as explained in the Decontamination Procedures chapter. If the problem still persists, just create a random NAT network: New-ContainerNetwork foo Steps to reproduce TODO 5. Container creation fails with error: CreateContainer: failure in a Windows system call When it happens This error happens occasionally when Docker tries to create a container. The container is actually created (it enters CREATED state), but can not be run (Docker doesn't start it automatically and manual start fails). Such a faulty container can be removed. Then one may try to create container again - this is expected to succeed (no case has been observed, when second attempt failed). Steps to reproduce There's no obvious correlation with any special circumstances. On a VM that's not heavily loaded, it is expected that hundreds of tries might be needed to reproduce this error. Creating and removing containers in a loop is enough.","title":"HNS errors bestiary"},{"location":"Known_issues/HNS_error_bestiary/#hns-errors-bestiary","text":"HNS error messages ofter aren't descriptive enough, and the cleanup is quite hard. The following document describes common cleanup techniques and error interpretations.","title":"HNS errors bestiary"},{"location":"Known_issues/HNS_error_bestiary/#hns-decontamination-procedures","text":"Before working with HNS, one must realize the risks. There are many ways in which HNS can stop working and affect other parts of the system. For a developer, it is important to be able to recognize those, which can be recovered from. Below are some techniques which may help. Always try cleaning using the lowest level procedure. The higher the level, the more potential damage the cleanup can cause.","title":"HNS Decontamination procedures"},{"location":"Known_issues/HNS_error_bestiary/#a-word-of-caution","text":"Never develop HNS on a baremetal or own laptop.","title":"A word of caution"},{"location":"Known_issues/HNS_error_bestiary/#level-alpha-decontamination-procedure","text":"First, stop the docker service. Stop-Service docker Remove all virtual switches, container networks and NAT Get-ContainerNetwork | Remove-ContainerNetwork Get-VMSwitch | Remove-VMSwitch Get-NetNAT | Remove-NetNAT Restart HNS and docker services. Restart-Service hns Restart-Service docker","title":"Level Alpha decontamination procedure"},{"location":"Known_issues/HNS_error_bestiary/#level-beta-decontamination-procedure","text":"Remove all container networks. Always do this because some vmswitches may not be removed properly if you just execute the next command. Get-ContainerNetwork | Remove-ContainerNetwork Manually remove HNS.data file. net command sometimes works better at restarting HNS than PowerShell's Restart-Service . Warning: this is not recommended by Microsoft, but they do it in their official cleanup script, so I guess it's not that bad. net stop hns; del C:\\programdata\\Microsoft\\Windows\\HNS\\HNS.data; net start hns;","title":"Level Beta decontamination procedure"},{"location":"Known_issues/HNS_error_bestiary/#level-gamma-decontamination-procedure","text":"Use official Microsoft script to cleanup. It will also cleanup some registry entries and do much more. https://github.com/MicrosoftDocs/Virtualization-Documentation/tree/live/windows-server-container-tools/CleanupContainerHostNetworking","title":"Level Gamma decontamination procedure"},{"location":"Known_issues/HNS_error_bestiary/#level-redacted-decontamination-procedure","text":"Everything is lost. All we can do wipe all devices and hope that the contamination won't spread to other hosts. Warning: this might BSOD. // perform as single command because you will lose connectivity during netcfg -D netcfg -D; Restart-Computer -force","title":"Level [REDACTED] decontamination procedure"},{"location":"Known_issues/HNS_error_bestiary/#hns-error-list","text":"The following chapter contains a list of HNS errors encountered throughout development, along with steps to reproduce and occurence scenarios.","title":"HNS error list"},{"location":"Known_issues/HNS_error_bestiary/#1-hns-unspecified-error","text":"","title":"1. HNS Unspecified Error"},{"location":"Known_issues/HNS_error_bestiary/#when-it-happens","text":"When attempting to create a transparent HNS network when creation of another network or VMSwitch is already in progress (eg. we've just started Docker service and it tries to create the NAT network). We can work around this bug by retrying after a few seconds.","title":"When it happens"},{"location":"Known_issues/HNS_error_bestiary/#steps-to-reproduce","text":"Try to create multiple HNS networks in a loop simultaneously with multiple processes. We suspect that this error occurs during a high load.","title":"Steps to reproduce"},{"location":"Known_issues/HNS_error_bestiary/#2-hns-invalid-parameter","text":"","title":"2. HNS Invalid Parameter"},{"location":"Known_issues/HNS_error_bestiary/#when-it-happens_1","text":"TODO","title":"When it happens"},{"location":"Known_issues/HNS_error_bestiary/#steps-to-reproduce_1","text":"TODO","title":"Steps to reproduce"},{"location":"Known_issues/HNS_error_bestiary/#3-hns-element-not-found","text":"","title":"3. HNS Element not found"},{"location":"Known_issues/HNS_error_bestiary/#when-it-happens_2","text":"[Hypothesis] When attempting to create a transparent docker network when no other transparent docker network exists and Ethernet adapter to be used by the transparent networks has no IP address or it's invalid.","title":"When it happens"},{"location":"Known_issues/HNS_error_bestiary/#steps-to-reproduce_2","text":"See https://github.com/Microsoft/hcsshim/issues/95","title":"Steps to reproduce"},{"location":"Known_issues/HNS_error_bestiary/#4-hns-failed-with-error-object-exists-an-attempt-was-made-to-create-an-object-and-the-object-name-already-exists","text":"","title":"4. HNS failed with error : {Object Exists} An attempt was made to create an object and the object name already exists"},{"location":"Known_issues/HNS_error_bestiary/#when-it-happens_3","text":"This error probably happens when docker tries to create NAT network, but HNS left over some trash after last NAT network. Cleanup everything as explained in the Decontamination Procedures chapter. If the problem still persists, just create a random NAT network: New-ContainerNetwork foo","title":"When it happens"},{"location":"Known_issues/HNS_error_bestiary/#steps-to-reproduce_3","text":"TODO","title":"Steps to reproduce"},{"location":"Known_issues/HNS_error_bestiary/#5-container-creation-fails-with-error-createcontainer-failure-in-a-windows-system-call","text":"","title":"5. Container creation fails with error: CreateContainer: failure in a Windows system call"},{"location":"Known_issues/HNS_error_bestiary/#when-it-happens_4","text":"This error happens occasionally when Docker tries to create a container. The container is actually created (it enters CREATED state), but can not be run (Docker doesn't start it automatically and manual start fails). Such a faulty container can be removed. Then one may try to create container again - this is expected to succeed (no case has been observed, when second attempt failed).","title":"When it happens"},{"location":"Known_issues/HNS_error_bestiary/#steps-to-reproduce_4","text":"There's no obvious correlation with any special circumstances. On a VM that's not heavily loaded, it is expected that hundreds of tries might be needed to reproduce this error. Creating and removing containers in a loop is enough.","title":"Steps to reproduce"},{"location":"Known_issues/Known_issues/","text":"Workarounds for known issues Below is a list of known issues with workarounds. DNS doesn't work in my networks For DNS to work in containers, DHCP has to be turned on in Contrail network via WebUI. Please refer to this bug: https://bugs.launchpad.net/opencontrail/+bug/1535856 I can't get IP fragmentation to work On Windows Server 2016, NAT network on compute node must be disabled in order for IP packet fragmentation to work. To do that, use the following commands: Get-NetNat | Remove-NetNat # Remove existing NAT networks Stop-Service winnat # Stop WinNAT service Set-Service -StartupType Disabled -Name winnat # Disable autostart of WinNAT upon reboot If you alread have some HNS networks, you will probably need to remove them as well ( Warning this will remove all container networks present on the Compute node): # Remove all container networks twice for good measure, because HNS sometimes fails Get-ContainerNetwork | Remove-ContainerNetwork -ErrorAction SilentlyContinue -Force Get-ContainerNetwork | Remove-ContainerNetwork -Force You also need to edit Docker config file located at C:\\ProgramData\\Docker\\config\\daemon.json to contain \"bridge\": \"none\" entry, like so: { bridge : none } Otherwise, Docker will try to reenable WinNAT upon restart. When I run Get-Net* command inside a container, it hangs Symptom: running Get-NetAdapter , Get-NetIpInterface or other Get-Net* commands inside containers hangs. The workaround consists of restarting the container. I see that vRouter is enabled, but not running After finished deployment of Windows Contrail compute nodes, sometimes vRouter enters a state when it is enabled but not running. To check run: Get-VMSwitch | Get-VMSwitchExtension -Name vRouter forwarding extension | Select Enabled, Running Workaround requires uninstalling and installing vRouter (possibly few times). msiexec /x vRouter.msi msiexec /i vRouter.msi I have received a cryptic HNS error Please refer to HNS error bestiary .","title":"Workarounds for known issues"},{"location":"Known_issues/Known_issues/#workarounds-for-known-issues","text":"Below is a list of known issues with workarounds.","title":"Workarounds for known issues"},{"location":"Known_issues/Known_issues/#dns-doesnt-work-in-my-networks","text":"For DNS to work in containers, DHCP has to be turned on in Contrail network via WebUI. Please refer to this bug: https://bugs.launchpad.net/opencontrail/+bug/1535856","title":"DNS doesn't work in my networks"},{"location":"Known_issues/Known_issues/#i-cant-get-ip-fragmentation-to-work","text":"On Windows Server 2016, NAT network on compute node must be disabled in order for IP packet fragmentation to work. To do that, use the following commands: Get-NetNat | Remove-NetNat # Remove existing NAT networks Stop-Service winnat # Stop WinNAT service Set-Service -StartupType Disabled -Name winnat # Disable autostart of WinNAT upon reboot If you alread have some HNS networks, you will probably need to remove them as well ( Warning this will remove all container networks present on the Compute node): # Remove all container networks twice for good measure, because HNS sometimes fails Get-ContainerNetwork | Remove-ContainerNetwork -ErrorAction SilentlyContinue -Force Get-ContainerNetwork | Remove-ContainerNetwork -Force You also need to edit Docker config file located at C:\\ProgramData\\Docker\\config\\daemon.json to contain \"bridge\": \"none\" entry, like so: { bridge : none } Otherwise, Docker will try to reenable WinNAT upon restart.","title":"I can't get IP fragmentation to work"},{"location":"Known_issues/Known_issues/#when-i-run-get-net-command-inside-a-container-it-hangs","text":"Symptom: running Get-NetAdapter , Get-NetIpInterface or other Get-Net* commands inside containers hangs. The workaround consists of restarting the container.","title":"When I run Get-Net* command inside a container, it hangs"},{"location":"Known_issues/Known_issues/#i-see-that-vrouter-is-enabled-but-not-running","text":"After finished deployment of Windows Contrail compute nodes, sometimes vRouter enters a state when it is enabled but not running. To check run: Get-VMSwitch | Get-VMSwitchExtension -Name vRouter forwarding extension | Select Enabled, Running Workaround requires uninstalling and installing vRouter (possibly few times). msiexec /x vRouter.msi msiexec /i vRouter.msi","title":"I see that vRouter is enabled, but not running"},{"location":"Known_issues/Known_issues/#i-have-received-a-cryptic-hns-error","text":"Please refer to HNS error bestiary .","title":"I have received a cryptic HNS error"},{"location":"Known_issues/Not_supported/","text":"Unsupported features Platform Nano Server running as Compute Node is currently not supported, as it lacks the ability to install Hyper-V networking extensions. Windows 10 is not supported due to it having a different version of Docker (Community, instead of Enterprise). Networking On Windows Server 2016, NAT network on compute node must be disabled in order for IP packet fragmentation to work. IPv6 in overlay is not supported. Containers Hyper-V and Linux containers are not supported. Only Windows Server containers are supported, due to differences in network handling with Hyper-V Containers. Container networking Multiple endpoints (interfaces) per container are not supported. Multiple IPs/networks per container endpoint (interface) are not supported. Multiple config nodes not supported in CNM plugin. docker network inspect on Contrail network does not show correct IPAM subnet, if subnet was not specified on creation. Please refer to this bug: https://bugs.launchpad.net/opencontrail/+bug/1789237 Authentication Only Keystone v2 API is supported.","title":"Unsupported features"},{"location":"Known_issues/Not_supported/#unsupported-features","text":"","title":"Unsupported features"},{"location":"Known_issues/Not_supported/#platform","text":"Nano Server running as Compute Node is currently not supported, as it lacks the ability to install Hyper-V networking extensions. Windows 10 is not supported due to it having a different version of Docker (Community, instead of Enterprise).","title":"Platform"},{"location":"Known_issues/Not_supported/#networking","text":"On Windows Server 2016, NAT network on compute node must be disabled in order for IP packet fragmentation to work. IPv6 in overlay is not supported.","title":"Networking"},{"location":"Known_issues/Not_supported/#containers","text":"Hyper-V and Linux containers are not supported. Only Windows Server containers are supported, due to differences in network handling with Hyper-V Containers.","title":"Containers"},{"location":"Known_issues/Not_supported/#container-networking","text":"Multiple endpoints (interfaces) per container are not supported. Multiple IPs/networks per container endpoint (interface) are not supported. Multiple config nodes not supported in CNM plugin. docker network inspect on Contrail network does not show correct IPAM subnet, if subnet was not specified on creation. Please refer to this bug: https://bugs.launchpad.net/opencontrail/+bug/1789237","title":"Container networking"},{"location":"Known_issues/Not_supported/#authentication","text":"Only Keystone v2 API is supported.","title":"Authentication"},{"location":"Known_issues/Troubleshooting/","text":"Troubleshooting Please follow steps below if you encounter issues when using Windows Contrail. 1. Use contrail-status TODO contrail-status is not implemented yet. Please move on to next troubleshooting steps. 2. Diagnostic check Run Invoke-DiagnosticCheck.ps1 script from tools repository on Windows Computes that cause problems. Refer to README in root of the repository for script configuration info. Note : the script can be ran with or without Administrator privileges. However, some checks will not be performed without them. 3. Check the logs Diagnostic check already looks for issues in logs of Contrail Components running on the node. However, you might want to check them manually. Logs are located in $Env:ProgramData\\contrail\\var\\logs\\contrail\\ . 4. Verify the configuration You might want to verify the configuration of all Contrail services running on the node. Config files are located in $Env:ProgramData\\contrail\\etc\\contrail\\ 5. Known issues Search known issues section for symptoms of your issue. The section contains recovery procedures. 6. Use Contrail utility tools Note : the following steps require experience using Contrail utils. Behaviour of Windows Contrail utility tools is similar to their Linux counterparts. They are: vif , nh , rt , flow , mpls , vrfstats , dropstats , vxlan , vrouter , vrmemstats . They should be in $Path . If not, they can be found under $Env:ProgramFiles\\Juniper Networks\\vRouter utilities\\ . 7. Crashdumps If you encounter any kernel panics or service crashes, debugging dump files may prove useful: Kernel crashdumps: $Env:SystemRoot\\MEMORY.DMP and $Env:SystemRoot\\Minidump\\*dmp Usermode crashdumps: $Env:LocalAppData\\CrashDumps 8. Total cleanup Run Clear-ComputeNode.ps1 script from tools repository on Windows Computes that cause problems. Refer to README in root of the repository for script configuration info. Warning : The script tries to clear all traces of Windows Contrail along with any leftover state. This includes any workload instances (containers). Note : For ease of use, you can use Invoke-ScriptInRemoteSessions.ps1 script that will run the cleanup on a remote machine. Redeployment of Windows Compute node is required after runnig this script. Please refer to deployment instructions .","title":"Troubleshooting"},{"location":"Known_issues/Troubleshooting/#troubleshooting","text":"Please follow steps below if you encounter issues when using Windows Contrail.","title":"Troubleshooting"},{"location":"Known_issues/Troubleshooting/#1-use-contrail-status","text":"TODO contrail-status is not implemented yet. Please move on to next troubleshooting steps.","title":"1. Use contrail-status"},{"location":"Known_issues/Troubleshooting/#2-diagnostic-check","text":"Run Invoke-DiagnosticCheck.ps1 script from tools repository on Windows Computes that cause problems. Refer to README in root of the repository for script configuration info. Note : the script can be ran with or without Administrator privileges. However, some checks will not be performed without them.","title":"2. Diagnostic check"},{"location":"Known_issues/Troubleshooting/#3-check-the-logs","text":"Diagnostic check already looks for issues in logs of Contrail Components running on the node. However, you might want to check them manually. Logs are located in $Env:ProgramData\\contrail\\var\\logs\\contrail\\ .","title":"3. Check the logs"},{"location":"Known_issues/Troubleshooting/#4-verify-the-configuration","text":"You might want to verify the configuration of all Contrail services running on the node. Config files are located in $Env:ProgramData\\contrail\\etc\\contrail\\","title":"4. Verify the configuration"},{"location":"Known_issues/Troubleshooting/#5-known-issues","text":"Search known issues section for symptoms of your issue. The section contains recovery procedures.","title":"5. Known issues"},{"location":"Known_issues/Troubleshooting/#6-use-contrail-utility-tools","text":"Note : the following steps require experience using Contrail utils. Behaviour of Windows Contrail utility tools is similar to their Linux counterparts. They are: vif , nh , rt , flow , mpls , vrfstats , dropstats , vxlan , vrouter , vrmemstats . They should be in $Path . If not, they can be found under $Env:ProgramFiles\\Juniper Networks\\vRouter utilities\\ .","title":"6. Use Contrail utility tools"},{"location":"Known_issues/Troubleshooting/#7-crashdumps","text":"If you encounter any kernel panics or service crashes, debugging dump files may prove useful: Kernel crashdumps: $Env:SystemRoot\\MEMORY.DMP and $Env:SystemRoot\\Minidump\\*dmp Usermode crashdumps: $Env:LocalAppData\\CrashDumps","title":"7. Crashdumps"},{"location":"Known_issues/Troubleshooting/#8-total-cleanup","text":"Run Clear-ComputeNode.ps1 script from tools repository on Windows Computes that cause problems. Refer to README in root of the repository for script configuration info. Warning : The script tries to clear all traces of Windows Contrail along with any leftover state. This includes any workload instances (containers). Note : For ease of use, you can use Invoke-ScriptInRemoteSessions.ps1 script that will run the cleanup on a remote machine. Redeployment of Windows Compute node is required after runnig this script. Please refer to deployment instructions .","title":"8. Total cleanup"},{"location":"Quick_start/connection_scenarios/","text":"Examples and scenarios Creating virtual network(s), containers and interactive sessions Via Contrail WebUI ( https://CONTROLLER_IP:8143 ): Create a networking policy (Configuration- Networking- Policies) with the following rule: pass protocol any network any ports any network any ports any Create virtual network called testnet1 with single subnet: 10.0.1.0/24 . Leave every field as default except IP pool (set it to be from 10.0.1.10 to 10.0.1.100 ) and network policy (set it to policy created in first step). Create virtual network called testnet2 with single subnet: 10.0.2.0/24 . Leave every field as default except IP pool (set it to be from 10.0.2.10 to 10.0.2.100 ) and network policy (set it to policy created in first step). On both Windows VMs: Verify if Agent service is running: Get-Service ContrailAgent State should be Running Verify if VMSwitch Extension is running and enabled: Get-VMSwitch -Name \"Layered*\" | Get-VMSwitchExtension Find block of data describing to vRouter Both Enabled and Running fields should be True Verify if Docker Driver service is running: Get-Service contrail-docker-driver State should be Running 1. Create docker network testnet1 on the 1st Windows compute code: docker network create --ipam-driver windows -d Contrail --opt tenant=admin --opt network=testnet1 testnet1 Run a docker container in the prepared network on the 1st Windows compute node: docker run -id --network testnet1 --name container1 microsoft/windowsservercore powershell Note : docker will pull heavy images from dockerhub and it may take a while. You can start configuration of another Windows compute node in the meantime. Inspect container1 on the 1st Windows compute node. Find the part that describes endpoints and verify that it has a correct IP address assigned (from the previously configured IP pool). docker inspect container1 Run interactive shell inside the container1 on the 1st Windows compute node: docker exec -it container1 powershell Create docker network \u2018testnet2\u2019 on the 2nd Windows compute node: docker network create --ipam-driver windows -d Contrail --opt tenant=admin --opt network=testnet2 testnet2 Run a docker container in the prepared network (\u2018testnet2\u2019) on the 2nd Windows compute node: docker run -id --network testnet2 --name container2 microsoft/windowsservercore powershell Inspect container2 on 2nd Windows compute node. Find the part that describes endpoints and verify that it has a correct IP address assigned (from the previously configured IP pool). docker inspect container2 Run a 2nd docker container in the prepared network (\u2018testnet2\u2019) on the 2nd Windows compute node: docker run -id --network testnet2 --name container3 microsoft/iis powershell Inspect container3 on the 2nd Windows compute node. Find the part that describes endpoints and verify that it has a correct IP address assigned (from the previously configured IP pool). docker inspect container3 Run interactive shell inside the container2 on the 2nd Windows compute node: docker exec -it container2 powershell Run interactive shell inside the container3 on the 2nd Windows compute node. Remember that Powershell console that has been used in previous step is now running a session in container2. To operate on a host system, open new Powershell console. docker exec -it container3 powershell ICMP Verify that ICMP protocol works within single virtual network. To do that invoke the following Powershell command in interactive session of container2 (on the 2nd Windows compute Node): ping IP address of container3 Verify that ICMP protocol works across virtual network boundaries. To do that invoke the following Powershell command in interactive session of container1 (on the 1st Windows Compute Node): ping IP address of container2 TCP Invoke the following commands in container3\u2019s interactive session (on the 2nd Windows Compute Node): cd \\inetpub\\wwwroot echo \"We come in peace.\" index.html Invoke the following command in container1\u2019s interactive session (on the 1st Windows Compute Node). Verify that you receive server response with webpage prepared in previous step. Invoke-WebRequest http:// container3's IP address -UseBasicParsing UDP Set up UDP listener on container3 (on the 2nd Windows Compute Node). To do that invoke the following commands in container3\u2019s interactive console. Last instruction is a blocking call waiting for incoming packet. $RemoteIPEndpoint = New-Object System.Net.IPEndPoint([IPAddress]::Any, 0) $UDPRcvSocket = New-Object System.Net.Sockets.UdpClient 3000 $Payload = $UDPRcvSocket.Receive([ref]$RemoteIPEndpoint) Send UDP packet from container1 (on the 1st Windows Compute Node). To do that invoke the following commands in container1\u2019s interactive console. $ListenerAddress = New-Object System.Net.IPEndPoint([IPAddress]::Parse(\"10.0.2.4\"), 3000) $UDPSenderSocket = New-Object System.Net.Sockets.UdpClient 0 $Payload = [Text.Encoding]::UTF8.GetBytes(\"We come in peace.\") $UDPSenderSocket.Send($Payload, $Payload.Length, $ListenerAddress) When packet is received by container3, blocking call to System.Net.Sockets.UdpClient.Receive (in container3 on the 2nd Windows Compute Node) is going to return. If that doesn\u2019t happen immediately, repeat invocation of $UDPSenderSocket.Send (in container1 on the 1st Windows Compute Node). Invoke the following command in container1\u2019s interactive session to display payload of received UDP packet to verify that it\u2019s consisted with sent message: [Text.Encoding]::UTF8.GetString($Payload) Network policies and security groups Create additional policies and security groups (through WebUI) with various settings and demonstrate that they affect communication between networks. One may follow ICMP/TCP/UDP test procedures with various policies and security groups. Revert network configuration (policies and security groups) to its original state not to affect other experiments. ECMP Create Floating IP Pool through Contrail WebUI assigned to testnet2 . Create Floating IP through WebUI. Assign it to previously created pool and set specific IP that is a correct address in testnet2 (e.g. 10.0.2.101). Assign Floating IP created in second step to port ( Ports tab) that belongs to container2. Assign Floating IP created in second step to port ( Ports tab) that belongs to container3. Set up an UDP listener on container2. Enter the following Powershell instructions in interactive console of container2 on the 2nd Windows Compute Node: $Sock = [System.Net.Sockets.UdpClient]::new(5000) $Endpoint = [System.Net.IPEndPoint]::new([IPAddress]::Any, 5000) while ($True) { $Data = $Sock.Receive([ref]$Endpoint) Write-Host \"Received packet from $Endpoint\" } Set up an UDP listener on container3. Enter the following Powershell instructions in interactive console of container3 on the 2nd Windows Compute Node: $Sock = [System.Net.Sockets.UdpClient]::new(5000) $Endpoint = [System.Net.IPEndPoint]::new([IPAddress]::Any, 5000) while ($True) { $Data = $Sock.Receive([ref]$Endpoint) Write-Host \"Received packet from $Endpoint\" } Send multiple UDP packets from container1 on the 1st Windows Compute Node. Use different source ports. Set the floating IP (created in step 2) as destination address. Use 5000 (as used in step 6) as destination port. Then verify that some packets were received by container2 while some other packets were received by container3. Receiving container is going to display a message: Received packet from sender address . Use the following Powershell command to send 10 UDP packets from various source ports: 1..100 | % { $s = [System.Net.Sockets.UdpClient]::new(10000+$_) $s.Send(\"C\".ToCharArray(), 1, \"10.0.2.101\", 5000) }","title":"Examples and scenarios"},{"location":"Quick_start/connection_scenarios/#examples-and-scenarios","text":"","title":"Examples and scenarios"},{"location":"Quick_start/connection_scenarios/#creating-virtual-networks-containers-and-interactive-sessions","text":"Via Contrail WebUI ( https://CONTROLLER_IP:8143 ): Create a networking policy (Configuration- Networking- Policies) with the following rule: pass protocol any network any ports any network any ports any Create virtual network called testnet1 with single subnet: 10.0.1.0/24 . Leave every field as default except IP pool (set it to be from 10.0.1.10 to 10.0.1.100 ) and network policy (set it to policy created in first step). Create virtual network called testnet2 with single subnet: 10.0.2.0/24 . Leave every field as default except IP pool (set it to be from 10.0.2.10 to 10.0.2.100 ) and network policy (set it to policy created in first step). On both Windows VMs: Verify if Agent service is running: Get-Service ContrailAgent State should be Running Verify if VMSwitch Extension is running and enabled: Get-VMSwitch -Name \"Layered*\" | Get-VMSwitchExtension Find block of data describing to vRouter Both Enabled and Running fields should be True Verify if Docker Driver service is running: Get-Service contrail-docker-driver State should be Running 1. Create docker network testnet1 on the 1st Windows compute code: docker network create --ipam-driver windows -d Contrail --opt tenant=admin --opt network=testnet1 testnet1 Run a docker container in the prepared network on the 1st Windows compute node: docker run -id --network testnet1 --name container1 microsoft/windowsservercore powershell Note : docker will pull heavy images from dockerhub and it may take a while. You can start configuration of another Windows compute node in the meantime. Inspect container1 on the 1st Windows compute node. Find the part that describes endpoints and verify that it has a correct IP address assigned (from the previously configured IP pool). docker inspect container1 Run interactive shell inside the container1 on the 1st Windows compute node: docker exec -it container1 powershell Create docker network \u2018testnet2\u2019 on the 2nd Windows compute node: docker network create --ipam-driver windows -d Contrail --opt tenant=admin --opt network=testnet2 testnet2 Run a docker container in the prepared network (\u2018testnet2\u2019) on the 2nd Windows compute node: docker run -id --network testnet2 --name container2 microsoft/windowsservercore powershell Inspect container2 on 2nd Windows compute node. Find the part that describes endpoints and verify that it has a correct IP address assigned (from the previously configured IP pool). docker inspect container2 Run a 2nd docker container in the prepared network (\u2018testnet2\u2019) on the 2nd Windows compute node: docker run -id --network testnet2 --name container3 microsoft/iis powershell Inspect container3 on the 2nd Windows compute node. Find the part that describes endpoints and verify that it has a correct IP address assigned (from the previously configured IP pool). docker inspect container3 Run interactive shell inside the container2 on the 2nd Windows compute node: docker exec -it container2 powershell Run interactive shell inside the container3 on the 2nd Windows compute node. Remember that Powershell console that has been used in previous step is now running a session in container2. To operate on a host system, open new Powershell console. docker exec -it container3 powershell","title":"Creating virtual network(s), containers and interactive sessions"},{"location":"Quick_start/connection_scenarios/#icmp","text":"Verify that ICMP protocol works within single virtual network. To do that invoke the following Powershell command in interactive session of container2 (on the 2nd Windows compute Node): ping IP address of container3 Verify that ICMP protocol works across virtual network boundaries. To do that invoke the following Powershell command in interactive session of container1 (on the 1st Windows Compute Node): ping IP address of container2","title":"ICMP"},{"location":"Quick_start/connection_scenarios/#tcp","text":"Invoke the following commands in container3\u2019s interactive session (on the 2nd Windows Compute Node): cd \\inetpub\\wwwroot echo \"We come in peace.\" index.html Invoke the following command in container1\u2019s interactive session (on the 1st Windows Compute Node). Verify that you receive server response with webpage prepared in previous step. Invoke-WebRequest http:// container3's IP address -UseBasicParsing","title":"TCP"},{"location":"Quick_start/connection_scenarios/#udp","text":"Set up UDP listener on container3 (on the 2nd Windows Compute Node). To do that invoke the following commands in container3\u2019s interactive console. Last instruction is a blocking call waiting for incoming packet. $RemoteIPEndpoint = New-Object System.Net.IPEndPoint([IPAddress]::Any, 0) $UDPRcvSocket = New-Object System.Net.Sockets.UdpClient 3000 $Payload = $UDPRcvSocket.Receive([ref]$RemoteIPEndpoint) Send UDP packet from container1 (on the 1st Windows Compute Node). To do that invoke the following commands in container1\u2019s interactive console. $ListenerAddress = New-Object System.Net.IPEndPoint([IPAddress]::Parse(\"10.0.2.4\"), 3000) $UDPSenderSocket = New-Object System.Net.Sockets.UdpClient 0 $Payload = [Text.Encoding]::UTF8.GetBytes(\"We come in peace.\") $UDPSenderSocket.Send($Payload, $Payload.Length, $ListenerAddress) When packet is received by container3, blocking call to System.Net.Sockets.UdpClient.Receive (in container3 on the 2nd Windows Compute Node) is going to return. If that doesn\u2019t happen immediately, repeat invocation of $UDPSenderSocket.Send (in container1 on the 1st Windows Compute Node). Invoke the following command in container1\u2019s interactive session to display payload of received UDP packet to verify that it\u2019s consisted with sent message: [Text.Encoding]::UTF8.GetString($Payload)","title":"UDP"},{"location":"Quick_start/connection_scenarios/#network-policies-and-security-groups","text":"Create additional policies and security groups (through WebUI) with various settings and demonstrate that they affect communication between networks. One may follow ICMP/TCP/UDP test procedures with various policies and security groups. Revert network configuration (policies and security groups) to its original state not to affect other experiments.","title":"Network policies and security groups"},{"location":"Quick_start/connection_scenarios/#ecmp","text":"Create Floating IP Pool through Contrail WebUI assigned to testnet2 . Create Floating IP through WebUI. Assign it to previously created pool and set specific IP that is a correct address in testnet2 (e.g. 10.0.2.101). Assign Floating IP created in second step to port ( Ports tab) that belongs to container2. Assign Floating IP created in second step to port ( Ports tab) that belongs to container3. Set up an UDP listener on container2. Enter the following Powershell instructions in interactive console of container2 on the 2nd Windows Compute Node: $Sock = [System.Net.Sockets.UdpClient]::new(5000) $Endpoint = [System.Net.IPEndPoint]::new([IPAddress]::Any, 5000) while ($True) { $Data = $Sock.Receive([ref]$Endpoint) Write-Host \"Received packet from $Endpoint\" } Set up an UDP listener on container3. Enter the following Powershell instructions in interactive console of container3 on the 2nd Windows Compute Node: $Sock = [System.Net.Sockets.UdpClient]::new(5000) $Endpoint = [System.Net.IPEndPoint]::new([IPAddress]::Any, 5000) while ($True) { $Data = $Sock.Receive([ref]$Endpoint) Write-Host \"Received packet from $Endpoint\" } Send multiple UDP packets from container1 on the 1st Windows Compute Node. Use different source ports. Set the floating IP (created in step 2) as destination address. Use 5000 (as used in step 6) as destination port. Then verify that some packets were received by container2 while some other packets were received by container3. Receiving container is going to display a message: Received packet from sender address . Use the following Powershell command to send 10 UDP packets from various source ports: 1..100 | % { $s = [System.Net.Sockets.UdpClient]::new(10000+$_) $s.Send(\"C\".ToCharArray(), 1, \"10.0.2.101\", 5000) }","title":"ECMP"},{"location":"Quick_start/deployment/","text":"Heterogenous Contrail Deployment: Linux + Windows The following procedure allows the operator to bring up heterogenous Contrail deployment. It will consist of at least one Contrail Controller (CentOS machine) and at least one Windows Compute node. Note : if you encounter any problems during or after deployment, see troubleshooting section . 1. Prerequisites To deploy Windows Contrail, you will need: Machine with CentOS 7.5 installed (for Contrail Controller), Machine(s) with Windows Server 2016 (for Windows compute nodes - please see requirements below), Machine for running Ansible playbooks (Linux CentOS or Windows with WSL). This can be your laptop. You need Ansible v2.4.2. Requirements for Windows Server 2016 machine: Minimum hardware requirements: 2 CPU, 4 GB RAM, 60 GB HDD. Virtualization support must be enabled: in case of a bare metal - enable VT-x in BIOS, in case of a virtual machine - enable nested virtualization. Newest Windows updates should be installed. Windows machines should have different hostnames. Windows machines should be accessible using the same set of credentials. 2. Enable Ansible remoting On each of the Windows hosts enable Ansible remoting: # PowerShell Invoke-WebRequest https://raw.githubusercontent.com/ansible/ansible/devel/examples/scripts/ConfigureRemotingForAnsible.ps1 -OutFile ConfigureRemotingForAnsible.ps1 .\\ConfigureRemotingForAnsible.ps1 -DisableBasicAuth -EnableCredSSP -ForceNewSSLCert -SkipNetworkProfileCheck 3. Configure Contrail-Ansible-Deployer On the Ansible machine: Clone the Contrail-Ansible-Deployer repository: # bash git clone https://Juniper/contrail-ansible-deployer cd contrail-ansible-deployer vim config/instances.yaml Fill in the config/instances.yaml file. See the instructions below on how to do it. Example configurations Refer to examples of instances.yaml file in Contrail-Ansible-Deployer repository: config/instances.yaml.bms_win_example if you have already deployed Contrail controller and you only want Windows compute nodes config/instances.yaml.bms_win_full_example if you want to deploy Contrail controller, Openstack and Windows compute nodes together. This is only useful if you want to use Keystone for auth. config/instances.yaml.bms_win_no_openstack_example if you want to deploy Contrail controller (without OpenStack) and Windows compute nodes together Instances You will need to know the IP addresses of CentOS and Windows hosts. For Windows computes use bms_win dict instead of regular bms . You need to add vrouter and win_docker_driver roles for Windows compute nodes. Set WINDOWS_PHYSICAL_INTERFACE to dataplane interface name (run Get-NetAdapter from PowerShell to list available interfaces on Windows compute node). If your Compute nodes have only one interface, specify it. Otherwise, you can split data and control planes between two interfaces - you can choose. Refer to Contrail documentation regarding data and control plane separation. If interface name contains spaces, enclose it between quotation marks. [FIXME] Windows Contrail dev build - deployment configuration Currently, only unsigned and debug builds of Windows Contrail components are available. As a result, the following configuration is also required: In BIOS of every Windows node you need to disable secure boot. Add WINDOWS_ENABLE_TEST_SIGNING option and leave it empty. This option configures Windows Server to allow installation of unsigned drivers. Set WINDOWS_DEBUG_DLLS_PATH to path on Ansible machine containing MSVC 2015 debug dll libraries.Since user space Contrail components are build in debug mode, to run them on Windows Server the following dlls are required: msvcp140d.dll , ucrtbased.dll , vcruntime140d.dll MSVC 2015 debug DLLs can be obtained by installing Visual Studio 2015. After installing Visual Studio they should be located in: mscvp140d.dll and vcruntime140d.dll - C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\redist\\debug_nonredist\\x64\\Microsoft.VC140.DebugCRT , ucrtbased.dll - C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x64\\ucrt . You should copy them to the ~/dll directory on your Ansible machine and point WINDOWS_DEBUG_DLLS_PATH variable to this directory. Make sure they are 64-bit. Contrail controller configuration Configure Contrail Controller it would be a normal controller node for a Linux ecosystem. Refer to Contrail-Ansible-Deployer wiki . If you wish to use keystone as authentication service on controller: Add openstack-* roles to the controller node and set CLOUD_ORCHESTRATOR to openstack Fill Keystone credentials and Kolla config. Refer to config/instances.yaml.bms_win_full_example . Otherwise: Set CLOUD_ORCHESTRATOR to none . 4. Run Contrail-Ansible-Deployer Proceed with running Ansible playbooks: If you have already deployed the Controller or if you want to deploy Controller without OpenStack (noauth mode): sudo -H ansible-playbook -i inventory/ playbooks/configure_instances.yml sudo -H ansible-playbook -i inventory/ playbooks/install_contrail.yml If you want to deploy controller with OpenStack (Keystone auth): sudo -H ansible-playbook -e orchestrator=openstack -i inventory/ playbooks/configure_instances.yml sudo -H ansible-playbook -i inventory playbooks/install_openstack.yml sudo -H ansible-playbook -e orchestrator=openstack -i inventory/ playbooks/install_contrail.yml Important : you can re-run any ansible-playbook , but you shouldn't change their order. E.g. if you already ran install_contrail.yml , then rerunning configure_instances.yml may lead to errors. 5. Verify deployment Run Invoke-DiagnosticCheck.ps1 script from tools repository . If deployment went correctly, all checks should pass. Note : to quickly have the ability to run this script on your Windows machine, you can use the following snippet: Invoke-WebRequest https://raw.githubusercontent.com/Juniper/contrail-windows-tools/master/Invoke-DiagnosticCheck.ps1 -OutFile Invoke-DiagnosticCheck.ps1 Consult the README on how to configure the diagnostic script (it's safe to run, so don't worry about misconfiguration). Refer to usage documentation to learn how to create networks and containers. (Optional) Refer to usage examples and run manual tests. Refer to this document . 6. Maintain (Optional) Upgrade Windows Contrail to newest version. See upgrading section .","title":"Heterogenous Contrail Deployment: Linux + Windows"},{"location":"Quick_start/deployment/#heterogenous-contrail-deployment-linux-windows","text":"The following procedure allows the operator to bring up heterogenous Contrail deployment. It will consist of at least one Contrail Controller (CentOS machine) and at least one Windows Compute node. Note : if you encounter any problems during or after deployment, see troubleshooting section .","title":"Heterogenous Contrail Deployment: Linux + Windows"},{"location":"Quick_start/deployment/#1-prerequisites","text":"To deploy Windows Contrail, you will need: Machine with CentOS 7.5 installed (for Contrail Controller), Machine(s) with Windows Server 2016 (for Windows compute nodes - please see requirements below), Machine for running Ansible playbooks (Linux CentOS or Windows with WSL). This can be your laptop. You need Ansible v2.4.2. Requirements for Windows Server 2016 machine: Minimum hardware requirements: 2 CPU, 4 GB RAM, 60 GB HDD. Virtualization support must be enabled: in case of a bare metal - enable VT-x in BIOS, in case of a virtual machine - enable nested virtualization. Newest Windows updates should be installed. Windows machines should have different hostnames. Windows machines should be accessible using the same set of credentials.","title":"1. Prerequisites"},{"location":"Quick_start/deployment/#2-enable-ansible-remoting","text":"On each of the Windows hosts enable Ansible remoting: # PowerShell Invoke-WebRequest https://raw.githubusercontent.com/ansible/ansible/devel/examples/scripts/ConfigureRemotingForAnsible.ps1 -OutFile ConfigureRemotingForAnsible.ps1 .\\ConfigureRemotingForAnsible.ps1 -DisableBasicAuth -EnableCredSSP -ForceNewSSLCert -SkipNetworkProfileCheck","title":"2. Enable Ansible remoting"},{"location":"Quick_start/deployment/#3-configure-contrail-ansible-deployer","text":"On the Ansible machine: Clone the Contrail-Ansible-Deployer repository: # bash git clone https://Juniper/contrail-ansible-deployer cd contrail-ansible-deployer vim config/instances.yaml Fill in the config/instances.yaml file. See the instructions below on how to do it.","title":"3. Configure Contrail-Ansible-Deployer"},{"location":"Quick_start/deployment/#example-configurations","text":"Refer to examples of instances.yaml file in Contrail-Ansible-Deployer repository: config/instances.yaml.bms_win_example if you have already deployed Contrail controller and you only want Windows compute nodes config/instances.yaml.bms_win_full_example if you want to deploy Contrail controller, Openstack and Windows compute nodes together. This is only useful if you want to use Keystone for auth. config/instances.yaml.bms_win_no_openstack_example if you want to deploy Contrail controller (without OpenStack) and Windows compute nodes together","title":"Example configurations"},{"location":"Quick_start/deployment/#instances","text":"You will need to know the IP addresses of CentOS and Windows hosts. For Windows computes use bms_win dict instead of regular bms . You need to add vrouter and win_docker_driver roles for Windows compute nodes. Set WINDOWS_PHYSICAL_INTERFACE to dataplane interface name (run Get-NetAdapter from PowerShell to list available interfaces on Windows compute node). If your Compute nodes have only one interface, specify it. Otherwise, you can split data and control planes between two interfaces - you can choose. Refer to Contrail documentation regarding data and control plane separation. If interface name contains spaces, enclose it between quotation marks.","title":"Instances"},{"location":"Quick_start/deployment/#fixme-windows-contrail-dev-build-deployment-configuration","text":"Currently, only unsigned and debug builds of Windows Contrail components are available. As a result, the following configuration is also required: In BIOS of every Windows node you need to disable secure boot. Add WINDOWS_ENABLE_TEST_SIGNING option and leave it empty. This option configures Windows Server to allow installation of unsigned drivers. Set WINDOWS_DEBUG_DLLS_PATH to path on Ansible machine containing MSVC 2015 debug dll libraries.Since user space Contrail components are build in debug mode, to run them on Windows Server the following dlls are required: msvcp140d.dll , ucrtbased.dll , vcruntime140d.dll MSVC 2015 debug DLLs can be obtained by installing Visual Studio 2015. After installing Visual Studio they should be located in: mscvp140d.dll and vcruntime140d.dll - C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\redist\\debug_nonredist\\x64\\Microsoft.VC140.DebugCRT , ucrtbased.dll - C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x64\\ucrt . You should copy them to the ~/dll directory on your Ansible machine and point WINDOWS_DEBUG_DLLS_PATH variable to this directory. Make sure they are 64-bit.","title":"[FIXME] Windows Contrail dev build - deployment configuration"},{"location":"Quick_start/deployment/#contrail-controller-configuration","text":"Configure Contrail Controller it would be a normal controller node for a Linux ecosystem. Refer to Contrail-Ansible-Deployer wiki . If you wish to use keystone as authentication service on controller: Add openstack-* roles to the controller node and set CLOUD_ORCHESTRATOR to openstack Fill Keystone credentials and Kolla config. Refer to config/instances.yaml.bms_win_full_example . Otherwise: Set CLOUD_ORCHESTRATOR to none .","title":"Contrail controller configuration"},{"location":"Quick_start/deployment/#4-run-contrail-ansible-deployer","text":"Proceed with running Ansible playbooks: If you have already deployed the Controller or if you want to deploy Controller without OpenStack (noauth mode): sudo -H ansible-playbook -i inventory/ playbooks/configure_instances.yml sudo -H ansible-playbook -i inventory/ playbooks/install_contrail.yml If you want to deploy controller with OpenStack (Keystone auth): sudo -H ansible-playbook -e orchestrator=openstack -i inventory/ playbooks/configure_instances.yml sudo -H ansible-playbook -i inventory playbooks/install_openstack.yml sudo -H ansible-playbook -e orchestrator=openstack -i inventory/ playbooks/install_contrail.yml Important : you can re-run any ansible-playbook , but you shouldn't change their order. E.g. if you already ran install_contrail.yml , then rerunning configure_instances.yml may lead to errors.","title":"4. Run Contrail-Ansible-Deployer"},{"location":"Quick_start/deployment/#5-verify-deployment","text":"Run Invoke-DiagnosticCheck.ps1 script from tools repository . If deployment went correctly, all checks should pass. Note : to quickly have the ability to run this script on your Windows machine, you can use the following snippet: Invoke-WebRequest https://raw.githubusercontent.com/Juniper/contrail-windows-tools/master/Invoke-DiagnosticCheck.ps1 -OutFile Invoke-DiagnosticCheck.ps1 Consult the README on how to configure the diagnostic script (it's safe to run, so don't worry about misconfiguration). Refer to usage documentation to learn how to create networks and containers. (Optional) Refer to usage examples and run manual tests. Refer to this document .","title":"5. Verify deployment"},{"location":"Quick_start/deployment/#6-maintain","text":"(Optional) Upgrade Windows Contrail to newest version. See upgrading section .","title":"6. Maintain"},{"location":"Quick_start/upgrading/","text":"Upgrading existing deployment Deprecation warning : the need for cleanup phase will be removed in the future. This documentation section will be updated when it happens. Warning : this will remove all traces of Windows Contrail, along with any containers, networking and configuration. Redeployment procedure will configure all the services correctly. However, the containers and networking must be recreated manually by the user (according to usage ). Currently, upgrading Windows Contrail deployment consists of two steps: cleanup of Compute nodes, redeployment using Ansible playbooks. This will result in rolling out the newest containers from opencontrailnightly repository. [FIXME] 1. Cleanup Run Clear-ComputeNode.ps1 script from tools repository for each Windows Compute node that needs upgrading. Note : to quickly have the ability to run this script on your Windows machine, you can use the following snippet: Invoke-WebRequest https://raw.githubusercontent.com/Juniper/contrail-windows-tools/master/Clear-ComputeNode.ps1 -OutFile Clear-ComputeNode.ps1 Consult the README on how to configure the script. Note : you can use Invoke-ScriptInRemoteSessions.ps1 script from tools repository to execute the cleanup script on multiple remote nodes at the same time. 2. Redeploy Follow the deployment procedure as normal, supplying newer versions of Windows Contrail containers. By default, deployment Ansible playbooks will pull the latest images.","title":"Upgrading existing deployment"},{"location":"Quick_start/upgrading/#upgrading-existing-deployment","text":"Deprecation warning : the need for cleanup phase will be removed in the future. This documentation section will be updated when it happens. Warning : this will remove all traces of Windows Contrail, along with any containers, networking and configuration. Redeployment procedure will configure all the services correctly. However, the containers and networking must be recreated manually by the user (according to usage ). Currently, upgrading Windows Contrail deployment consists of two steps: cleanup of Compute nodes, redeployment using Ansible playbooks. This will result in rolling out the newest containers from opencontrailnightly repository.","title":"Upgrading existing deployment"},{"location":"Quick_start/upgrading/#fixme-1-cleanup","text":"Run Clear-ComputeNode.ps1 script from tools repository for each Windows Compute node that needs upgrading. Note : to quickly have the ability to run this script on your Windows machine, you can use the following snippet: Invoke-WebRequest https://raw.githubusercontent.com/Juniper/contrail-windows-tools/master/Clear-ComputeNode.ps1 -OutFile Clear-ComputeNode.ps1 Consult the README on how to configure the script. Note : you can use Invoke-ScriptInRemoteSessions.ps1 script from tools repository to execute the cleanup script on multiple remote nodes at the same time.","title":"[FIXME] 1. Cleanup"},{"location":"Quick_start/upgrading/#2-redeploy","text":"Follow the deployment procedure as normal, supplying newer versions of Windows Contrail containers. By default, deployment Ansible playbooks will pull the latest images.","title":"2. Redeploy"},{"location":"Quick_start/usage/","text":"Usage The following section describes usage of Windows Contrail. Prerequisites Make sure that Windows Compute node was deployed successfuly. See deployment instructions . Creating a network In current workflow, virtual network (with IP pool) must be created manually in Contrail, e.g. using WebUI. Once it's created, run the following command on Windows Compute node to create a local Contrail network: docker network create --ipam-driver windows --driver Contrail --opt tenant=$tenant --opt network=$network --subnet $subnet $localnetwork where: $tenant is the name of your tenant in Contrail, $network is the name of your network in Contrail, $subnet is subnet CIDR (e.g. 10.0.0.0/24 ). This parameter must be specified, if you Contrail network has multiple subnets defined. $localnetwork is an arbitrary name that docker network should have on local compute node. Parameter --ipam-driver windows is specified to override docker's default IPAM driver with a noop one (named windows ). CNM plugin assigns IP address in a different way. Example: docker network create --ipam-driver windows --driver Contrail --opt tenant=admin --opt network=rednetwork --subnet 10.0.0.0/24 my_local_red_net Removing a network To remove a network, simply use docker network remove command. The network cannot have active endpoints for it to be removed. Note : this operation does not remove a virtual network in Contrail. To remove it as well, use Contrail WebUI. Creating a container When creating a container on Windows Compute, specify --net parameter: docker run -d --net $network microsoft/nanoserver ping -t localhost where: $network is a name of previously created local Contrail network. Unlike when creating a virtual network, manual creation of any resources for a container in Contrail is not required. Example: docker run -d --net my_local_red_net microsoft/nanoserver ping -t localhost Removing a container To remove a container, use docker rm command, as normal.","title":"Usage"},{"location":"Quick_start/usage/#usage","text":"The following section describes usage of Windows Contrail.","title":"Usage"},{"location":"Quick_start/usage/#prerequisites","text":"Make sure that Windows Compute node was deployed successfuly. See deployment instructions .","title":"Prerequisites"},{"location":"Quick_start/usage/#creating-a-network","text":"In current workflow, virtual network (with IP pool) must be created manually in Contrail, e.g. using WebUI. Once it's created, run the following command on Windows Compute node to create a local Contrail network: docker network create --ipam-driver windows --driver Contrail --opt tenant=$tenant --opt network=$network --subnet $subnet $localnetwork where: $tenant is the name of your tenant in Contrail, $network is the name of your network in Contrail, $subnet is subnet CIDR (e.g. 10.0.0.0/24 ). This parameter must be specified, if you Contrail network has multiple subnets defined. $localnetwork is an arbitrary name that docker network should have on local compute node. Parameter --ipam-driver windows is specified to override docker's default IPAM driver with a noop one (named windows ). CNM plugin assigns IP address in a different way. Example: docker network create --ipam-driver windows --driver Contrail --opt tenant=admin --opt network=rednetwork --subnet 10.0.0.0/24 my_local_red_net","title":"Creating a network"},{"location":"Quick_start/usage/#removing-a-network","text":"To remove a network, simply use docker network remove command. The network cannot have active endpoints for it to be removed. Note : this operation does not remove a virtual network in Contrail. To remove it as well, use Contrail WebUI.","title":"Removing a network"},{"location":"Quick_start/usage/#creating-a-container","text":"When creating a container on Windows Compute, specify --net parameter: docker run -d --net $network microsoft/nanoserver ping -t localhost where: $network is a name of previously created local Contrail network. Unlike when creating a virtual network, manual creation of any resources for a container in Contrail is not required. Example: docker run -d --net my_local_red_net microsoft/nanoserver ping -t localhost","title":"Creating a container"},{"location":"Quick_start/usage/#removing-a-container","text":"To remove a container, use docker rm command, as normal.","title":"Removing a container"}]}